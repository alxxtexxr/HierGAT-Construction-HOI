{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: `py39_torch271`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dacite import from_dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pyrutils.torch.train_utils import train, save_checkpoint\n",
    "from pyrutils.torch.multi_task import MultiTaskLossLearner\n",
    "from vhoi.data_loading import (\n",
    "    input_size_from_data_loader, \n",
    "    select_model_data_feeder, \n",
    "    select_model_data_fetcher,\n",
    ")\n",
    "from vhoi.data_loading_custom import (\n",
    "    create_data,\n",
    "    create_data_loader\n",
    ")\n",
    "from vhoi.losses_custom_v2 import (\n",
    "    select_loss, \n",
    "    decide_num_main_losses, \n",
    "    select_loss_types, \n",
    "    select_loss_learning_mask,\n",
    ")\n",
    "from vhoi.models import load_model_weights\n",
    "from vhoi.models_custom_v2 import TGGCN\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)   # Python的随机性\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "np.random.seed(seed)   # numpy的随机性\n",
    "torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True   # 选择确定性算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictMixin:\n",
    "    def get(self, key, default_value=None):\n",
    "        return getattr(self, key, default_value)\n",
    "\n",
    "    def as_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "@dataclass\n",
    "class Resources(DictMixin):\n",
    "    use_gpu: bool\n",
    "    num_threads: int\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata(DictMixin):\n",
    "    model_name: str\n",
    "    input_type: str\n",
    "\n",
    "@dataclass\n",
    "class ModelParameters(DictMixin):\n",
    "    add_segment_length: int\n",
    "    add_time_position: int\n",
    "    time_position_strategy: str\n",
    "    positional_encoding_style: str\n",
    "    attention_style: str\n",
    "    bias: bool\n",
    "    cat_level_states: int\n",
    "    discrete_networks_num_layers: int\n",
    "    discrete_optimization_strategy: str\n",
    "    filter_discrete_updates: bool\n",
    "    gcn_node: int\n",
    "    hidden_size: int\n",
    "    message_humans_to_human: bool\n",
    "    message_human_to_objects: bool\n",
    "    message_objects_to_human: bool\n",
    "    message_objects_to_object: bool\n",
    "    message_geometry_to_objects: bool\n",
    "    message_geometry_to_human: bool\n",
    "    message_segment: bool\n",
    "    message_type: str\n",
    "    message_granularity: str\n",
    "    message_aggregation: str\n",
    "    object_segment_update_strategy: str\n",
    "    share_level_mlps: int\n",
    "    update_segment_threshold: float\n",
    "\n",
    "@dataclass\n",
    "class ModelOptimization(DictMixin):\n",
    "    batch_size: int\n",
    "    clip_gradient_at: float\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    val_fraction: float\n",
    "\n",
    "@dataclass\n",
    "class BudgetLoss(DictMixin):\n",
    "    add: bool\n",
    "    human_weight: float\n",
    "    object_weight: float\n",
    "\n",
    "@dataclass\n",
    "class SegmentationLoss(DictMixin):\n",
    "    add: bool\n",
    "    pretrain: bool\n",
    "    sigma: float\n",
    "    weight: float\n",
    "\n",
    "@dataclass\n",
    "class ModelMisc(DictMixin):\n",
    "    anticipation_loss_weight: float\n",
    "    budget_loss: BudgetLoss\n",
    "    first_level_loss_weight: float\n",
    "    impose_segmentation_pattern: int\n",
    "    input_human_segmentation: bool\n",
    "    input_object_segmentation: bool\n",
    "    make_attention_distance_based: bool\n",
    "    multi_task_loss_learner: bool\n",
    "    pretrained: bool\n",
    "    pretrained_path: Optional[str]\n",
    "    segmentation_loss: SegmentationLoss\n",
    "\n",
    "@dataclass\n",
    "class ModelLogging(DictMixin):\n",
    "    root_log_dir: str\n",
    "    checkpoint_name: str\n",
    "    log_dir: str\n",
    "\n",
    "@dataclass\n",
    "class Models(DictMixin):\n",
    "    metadata: ModelMetadata\n",
    "    parameters: ModelParameters\n",
    "    optimization: ModelOptimization\n",
    "    misc: ModelMisc\n",
    "    logging: ModelLogging\n",
    "\n",
    "@dataclass\n",
    "class Data(DictMixin):\n",
    "    name: str\n",
    "    path: str\n",
    "    path_zarr: str\n",
    "    path_obb_zarr: str\n",
    "    path_hbb_zarr: str\n",
    "    path_hps_zarr: str\n",
    "    cross_validation_test_subject: str\n",
    "    scaling_strategy: Optional[str]\n",
    "    downsampling: int\n",
    "\n",
    "@dataclass\n",
    "class Config(DictMixin):\n",
    "    resources: Resources\n",
    "    models: Models\n",
    "    data: Data\n",
    "    \n",
    "metadata_dict = {\n",
    "    \"model_name\": \"2G-GCN\",\n",
    "    \"input_type\": \"multiple\"\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    \"add_segment_length\": 0,  # length of the segment to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"add_time_position\": 0,  # absolute time position to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"time_position_strategy\": \"s\",  # input time position to segment [s] or discrete update [u].\n",
    "    \"positional_encoding_style\": \"e\",  # e [embedding] or p [periodic].\n",
    "    \"attention_style\": \"v3\",  # v1 [concat], v2 [dot-product], v3 [scaled_dot-product], v4 [general]\n",
    "    \"bias\": True,\n",
    "    \"cat_level_states\": 0,  # concatenate first and second level hidden states for predictors MLPs.\n",
    "    \"discrete_networks_num_layers\": 1,  # depth of the state change detector MLP.\n",
    "    \"discrete_optimization_strategy\": \"gs\",  # straight-through [st] or gumbel-sigmoid [gs]\n",
    "    \"filter_discrete_updates\": False,  # maxima filter for soft output of state change detector.\n",
    "    \"gcn_node\": 25,  # custom, original: 19 for cad120, 30 for bimanual, 26 for mphoi\n",
    "    \"hidden_size\": 512,  # 512 for cad120 & mphoi; 64 for bimanual\n",
    "    \"message_humans_to_human\": False, # custom, original: True\n",
    "    \"message_human_to_objects\": True,\n",
    "    \"message_objects_to_human\": True,\n",
    "    \"message_objects_to_object\": False, # custom, original: True\n",
    "    \"message_geometry_to_objects\": True,\n",
    "    \"message_geometry_to_human\": True,  # custom, original: False\n",
    "    \"message_segment\": True,\n",
    "    \"message_type\": \"v2\",  # v1 [relational] or v2 [non-relational]\n",
    "    \"message_granularity\": \"v1\",  # v1 [generic] or v2 [specific]\n",
    "    \"message_aggregation\": \"att\",  # mean_pooling [mp] or attention [att]\n",
    "    \"object_segment_update_strategy\": \"ind\",  # same_as_human [sah], independent [ind], or conditional_on_human [coh]\n",
    "    \"share_level_mlps\": 0,  # whether to share [1] or not [0] the prediction MLPs of the levels.\n",
    "    \"update_segment_threshold\": 0.5  # [0.0, 1.0)\n",
    "}\n",
    "\n",
    "optimization_dict = {\n",
    "    \"batch_size\": 32,  # mphoi:8; cad120:16; bimanual: 32\n",
    "    \"clip_gradient_at\": 0.0,\n",
    "    \"epochs\": 10, # custom, original: cad120 & mphoi:40; bimanual: 60\n",
    "    \"learning_rate\": 1e-4,  # mphoi:1e-4; cad120 & bimanual:1e-3\n",
    "    \"val_fraction\": 0.1\n",
    "}\n",
    "\n",
    "data_dict = {\n",
    "    \"name\": \"mphoi\",\n",
    "    \"path\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_ground_truth_labels.json\",\n",
    "    \"path_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/faster_rcnn.zarr\",\n",
    "    \"path_obb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/object_bounding_boxes.zarr\",\n",
    "    \"path_hbb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_bounding_boxes.zarr\",\n",
    "    \"path_hps_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_pose.zarr\",\n",
    "    \"cross_validation_test_subject\": \"Subject14\",  # Subject45, Subject25, Subject14\n",
    "    \"scaling_strategy\": None,  # null or \"standard\"\n",
    "    \"downsampling\": 1 # custom, original: 3, 1 = full FPS, 2 = half FPS, ...\n",
    "}\n",
    "\n",
    "# root_log_dir = f\"{os.getcwd()}/outputs_hiergat/{data_dict['name']}/{metadata_dict['model_name']}\"\n",
    "root_log_dir = f\"{os.getcwd()}/outputs_hiergat/custom\"\n",
    "checkpoint_name = (\n",
    "    f\"hs{parameters_dict['hidden_size']}_e{optimization_dict['epochs']}_bs{optimization_dict['batch_size']}_\"\n",
    "    f\"lr{optimization_dict['learning_rate']}_{parameters_dict['update_segment_threshold']}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    ")\n",
    "log_dir = f\"{root_log_dir}/{checkpoint_name}\"\n",
    "print(\"Log directory:\", log_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "cfg_dict = {\n",
    "    \"resources\": {\n",
    "        \"use_gpu\": True,\n",
    "        \"num_threads\": 32\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"metadata\": metadata_dict,\n",
    "        \"parameters\": parameters_dict,\n",
    "        \"optimization\": optimization_dict,\n",
    "        \"misc\": {\n",
    "            \"anticipation_loss_weight\": 1.0,\n",
    "            \"budget_loss\": {\n",
    "                \"add\": False,\n",
    "                \"human_weight\": 1.0,\n",
    "                \"object_weight\": 1.0\n",
    "            },\n",
    "            \"first_level_loss_weight\": 0.0,  # if positive, first level does frame-level prediction\n",
    "            \"impose_segmentation_pattern\": 1,  # 0 [no pattern], 1 [all ones]\n",
    "            \"input_human_segmentation\": False,  # (was \"flase\" in YAML, corrected here)\n",
    "            \"input_object_segmentation\": False,\n",
    "            \"make_attention_distance_based\": True,  # only meaningful if message_aggregation is attention\n",
    "            \"multi_task_loss_learner\": False,\n",
    "            \"pretrained\": False,  # unfortunately need two entries for checkpoint name\n",
    "            \"pretrained_path\": None,  # specified parameters must match pre-trained model\n",
    "            \"segmentation_loss\": {\n",
    "                \"add\": False,\n",
    "                \"pretrain\": False,\n",
    "                \"sigma\": 0.0,  # Gaussian smoothing\n",
    "                \"weight\": 1.0\n",
    "            }\n",
    "        },\n",
    "        \"logging\": {\n",
    "            \"root_log_dir\": root_log_dir,\n",
    "            \"checkpoint_name\": checkpoint_name,\n",
    "            \"log_dir\": log_dir\n",
    "        },\n",
    "    },\n",
    "    \"data\": data_dict,\n",
    "}\n",
    "\n",
    "cfg = from_dict(data_class=Config, data=cfg_dict)\n",
    "\n",
    "torch.set_num_threads(cfg.resources.num_threads)\n",
    "model_name, model_input_type = cfg.models.metadata.model_name, cfg.models.metadata.input_type\n",
    "batch_size, val_fraction = cfg.models.optimization.batch_size, cfg.models.optimization.val_fraction\n",
    "misc_dict = cfg.get('misc', default_value={})\n",
    "sigma = misc_dict.get('segmentation_loss', {}).get('sigma', 0.0)\n",
    "scaling_strategy = cfg.data.scaling_strategy\n",
    "downsampling = cfg.data.downsampling\n",
    "\n",
    "action_classes = [\n",
    "    # human\n",
    "    'supervise',        # 0\n",
    "    'collaborate with', # 1\n",
    "    'assist',           # 2\n",
    "    'lead',             # 3\n",
    "    'coordinate with',  # 4\n",
    "    'listen to',        # 5\n",
    "\n",
    "    # rebar\n",
    "    'tie',              # 6\n",
    "    'erect',            # 7\n",
    "    'prepare_rebar',    # 8\n",
    "    'transport',        # 9\n",
    "\n",
    "    # formwork\n",
    "    'install',          # 10\n",
    "    'prepare_formwork', # 11\n",
    "\n",
    "    # concrete\n",
    "    'pour',             # 12\n",
    "    'finish',           # 13\n",
    "\n",
    "    # equipment         \n",
    "    'use',              # 14\n",
    "    'carry',            # 15\n",
    "\n",
    "    # all\n",
    "    'inspect',          # 16\n",
    "    'no interaction',   # 17\n",
    "]\n",
    "# new_action_classes = [\n",
    "#     # rebar\n",
    "#     'tie',              # 0\n",
    "#     'erect',            # 1\n",
    "#     'prepare_rebar',    # 2\n",
    "#     'transport',        # 3\n",
    "\n",
    "#     # equipment         \n",
    "#     'use',              # 4\n",
    "#     'carry',            # 5\n",
    "\n",
    "#     # all\n",
    "#     'inspect',          # 6\n",
    "#     'no interaction',   # 7\n",
    "# ]\n",
    "# new_action_classes = [\n",
    "#     # all\n",
    "#     'inspect',          # 0\n",
    "    \n",
    "#     # rebar\n",
    "#     'prepare_rebar',    # 1\n",
    "#     'erect',            # 2\n",
    "#     'tie',              # 3\n",
    "# ]\n",
    "# num_classes = len(new_action_classes)\n",
    "num_classes = len(action_classes)\n",
    "\n",
    "# Non-Windowed\n",
    "train_features_dirs = [\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'), \n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0090_full_MP4_anno_for_labelling_rizky_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0104_full_MP4_anno_for_labelling_full_temporal_3s'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0105_full_MP4_anno_for_labelling_full_temporal_3s'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0106_full_MP4_anno_for_labelling_full_temporal_3s'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0108_full_MP4_anno_for_labelling_fixed_full_temporal_3s'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0109_full_MP4_anno_for_labelling_full_temporal_3s'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0110_full_MP4_anno_for_labelling_full_temporal_3s'),\n",
    "]\n",
    "\n",
    "val_features_dirs = [\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0100_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'),\n",
    "]\n",
    "\n",
    "test_features_dirs = [\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0074_full_MP4_anno_for_labelling_done_ray_full_temporal_3s/features'),   \n",
    "]\n",
    "\n",
    "# Windowed\n",
    "# train_features_dirs = [\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features'),\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'), \n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'),\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_3s/features'),\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0090_full_MP4_anno_for_labelling_rizky_full_temporal_3s/features'),\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'),\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features'),\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features'),\n",
    "# ]\n",
    "\n",
    "# val_features_dirs = [\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0100_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'),\n",
    "# ]\n",
    "\n",
    "# test_features_dirs = [\n",
    "#     Path('/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0074_full_MP4_anno_for_labelling_done_ray_full_temporal_3s/features'),   \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def get_features_dirs_df(features_dirs):\n",
    "    feature_dirs_dict = []\n",
    "    for dirs in features_dirs:\n",
    "        for dir in dirs.iterdir():\n",
    "            action_label_str = str(dir).rsplit('_action_')[-1]\n",
    "            is_interpolated = 0\n",
    "            if '_interp' in action_label_str:\n",
    "                action_label_str = action_label_str.replace('_interp', '')\n",
    "                is_interpolated = 1\n",
    "            action_label = int(action_label_str)\n",
    "            action_class = action_classes[action_label]\n",
    "            if 'new_action_classes' in globals():\n",
    "                try:\n",
    "                    action_label = new_action_classes.index(action_class)\n",
    "                except ValueError:\n",
    "                    action_label = -1\n",
    "            feature_dirs_dict.append({\n",
    "                'base_dir': str(dirs),\n",
    "                'dir': str(dir),\n",
    "                'action_label': action_label,\n",
    "                # 'new_action_label': new_action_label,\n",
    "                'is_interpolated': is_interpolated,\n",
    "            })\n",
    "        \n",
    "    feature_dirs_df = pd.DataFrame(feature_dirs_dict)\n",
    "    feature_dirs_df = feature_dirs_df[feature_dirs_df['action_label'] != -1] # remove rows where action_label == -1\n",
    "    return feature_dirs_df\n",
    "\n",
    "train_feature_dirs_df = get_features_dirs_df(train_features_dirs)\n",
    "val_feature_dirs_df = get_features_dirs_df(val_features_dirs)\n",
    "test_feature_dirs_df = get_features_dirs_df(test_features_dirs)\n",
    "\n",
    "print(\"Training set action label counts:\")\n",
    "print(train_feature_dirs_df['action_label'].value_counts())\n",
    "print()\n",
    "print(\"Validation set action label counts:\")\n",
    "print(val_feature_dirs_df['action_label'].value_counts())\n",
    "print()\n",
    "print(\"Testing set action label counts:\")\n",
    "print(test_feature_dirs_df['action_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = new_action_classes if 'new_action_classes' in globals() else action_classes\n",
    "label_0 = classes.index('inspect')\n",
    "label_1 = classes.index('prepare_rebar')\n",
    "label_2 = classes.index('erect')\n",
    "label_3 = classes.index('tie')\n",
    "\n",
    "train_df_2 = train_feature_dirs_df[(train_feature_dirs_df['action_label'] == label_2)]\n",
    "train_df_0 = train_feature_dirs_df[(train_feature_dirs_df['action_label'] == label_0) & (train_feature_dirs_df['is_interpolated'] == 0)]\n",
    "\n",
    "# Sample a specified number of rows per group according to a size mapping\n",
    "def sample_by_group_size(df, groupby_col, size_map, seed=42):\n",
    "    return (\n",
    "        df\n",
    "        .groupby(groupby_col, group_keys=False)[df.columns]\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=size_map[x.name],\n",
    "            random_state=seed,\n",
    "        ))\n",
    "    )\n",
    "\n",
    "# Downsample training data for label 3\n",
    "train_df_3 = train_feature_dirs_df[(train_feature_dirs_df['action_label'] == label_3) & (train_feature_dirs_df['is_interpolated'] == 0)]\n",
    "# print(train_df_3.groupby('base_dir').size()) # Sanity Check\n",
    "train_size_map_3 = {\n",
    "    # Non-Windowed\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_3s/features'     : 65,   # 130 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'     : 65,   # 240 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features'     : 65,   # 200 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'     : 65,   # 205 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features'    : 2,    # 2   -> 2\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'     : 4,    # 4   -> 4\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features'   : 65,   # 147 -> 65 \n",
    "                                                                                                                                    # Total: 331\n",
    "    \n",
    "    # # Windowed\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_3s/features'    : 114,  # 297 -> 114\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'    : 115,  # 587 -> 115  \n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features'    : 115,  # 508 -> 115\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'    : 115,  # 415 -> 114\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features'   : 3,    # 3   -> 3 \n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'    : 7,    # 7   -> 7\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features'  : 114,  # 410 -> 114\n",
    "    #                                                                                                                                         # Total: 583\n",
    "}\n",
    "train_df_3_downsampled = sample_by_group_size(train_df_3, 'base_dir', train_size_map_3)\n",
    "\n",
    "# Downsample training data for label 1\n",
    "train_df_1 = train_feature_dirs_df[(train_feature_dirs_df['action_label'] == label_1) & (train_feature_dirs_df['is_interpolated'] == 0)]\n",
    "# print(train_df_1.groupby('base_dir').size()) # Sanity Check\n",
    "train_size_map_1 = {\n",
    "    # Non-Windowed\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features': 9,    # 56  -> 9\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features': 10,   # 181 -> 10\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features': 10,   # 274 -> 10\n",
    "                                                                                                                               # Total: 29\n",
    "    \n",
    "    # # Windowed\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features': 22,   # 135 -> 22\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features': 22,   # 491 -> 22\n",
    "    # '/root/vs-gats-plaster/deepsort/hiergat_data_windowed/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features': 22,   # 701 -> 22\n",
    "    #                                                                                                                                     # Total: 66\n",
    "}\n",
    "train_df_1_downsampled = sample_by_group_size(train_df_1, 'base_dir', train_size_map_1)\n",
    "\n",
    "# Concatenate downsampled and original label subsets to form the downsampled training set\n",
    "train_feature_dirs_df_downsampled = pd.concat((train_df_0, train_df_1_downsampled, train_df_2, train_df_3_downsampled))\n",
    "\n",
    "# Sanity Check\n",
    "print(\"Downsampled training set action label counts:\")\n",
    "print(train_feature_dirs_df_downsampled['action_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_dirs = train_feature_dirs_df_downsampled['dir'].tolist()\n",
    "val_feature_dirs   = val_feature_dirs_df['dir'].tolist()\n",
    "test_feature_dirs  = test_feature_dirs_df['dir'].tolist()\n",
    "\n",
    "train_data = create_data(train_feature_dirs, action_classes, new_action_classes if 'new_action_classes' in globals() else None)\n",
    "val_data = create_data(val_feature_dirs, action_classes, new_action_classes if 'new_action_classes' in globals() else None)\n",
    "test_data = create_data(test_feature_dirs, action_classes, new_action_classes if 'new_action_classes' in globals() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_human_features_list,\n",
    "    train_human_boxes_list,\n",
    "    train_human_poses_list,\n",
    "    train_object_features_list,\n",
    "    train_object_boxes_list,\n",
    "    train_gt_list,\n",
    "    train_xs_steps,\n",
    ") = train_data\n",
    "\n",
    "train_human_features_list = np.stack(train_human_features_list)[:, 0, :, :]     # (N, seq_len, 1024)\n",
    "train_human_boxes_list = np.stack(train_human_boxes_list)[:, 0, :, :]           # (N, seq_len, 4)\n",
    "train_human_poses_list = np.stack(train_human_poses_list)[:, 0, :, :]           # (N, seq_len, 17, 2)\n",
    "train_object_features_list = np.stack(train_object_features_list)[:, :, 0, :]   # (N, seq_len, 1024)\n",
    "train_object_boxes_list = np.stack(train_object_boxes_list)[:, :, 0, :]         # (N, seq_len, 4)\n",
    "train_gt_list = np.array([gt['Human1'][0] for gt in train_gt_list])             # (N,)\n",
    "# train_xs_steps                                                                # (N,)\n",
    "\n",
    "# Sanity Check\n",
    "print(\"train_human_features_list.shape:\", train_human_features_list.shape)\n",
    "print(\"train_human_boxes_list.shape:\", train_human_boxes_list.shape)\n",
    "print(\"train_human_poses_list.shape:\", train_human_poses_list.shape)\n",
    "print(\"train_object_features_list.shape:\", train_object_features_list.shape)\n",
    "print(\"train_object_boxes_list.shape:\", train_object_boxes_list.shape)\n",
    "print(\"train_gt_list.shape:\", train_gt_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_human_features_list = np.stack(train_human_features_list)[:, np.newaxis, :, :]          # (N, 1, seq_len, 1024)\n",
    "train_human_boxes_list = np.stack(train_human_boxes_list)[:, np.newaxis, :, :]                # (N, 1, seq_len, 4)\n",
    "train_human_poses_list = np.stack(train_human_poses_list)[:, np.newaxis, :, :]                # (N, 1, seq_len, 17, 2)\n",
    "train_object_features_list = np.stack(train_object_features_list)[:, :, np.newaxis, :]        # (N, seq_len, 1, 1024)\n",
    "train_object_boxes_list = np.stack(train_object_boxes_list)[:, :, np.newaxis, :]              # (N, seq_len, 1, 4)\n",
    "train_gt_list = [{'Human1': [gt]*train_human_features_list.shape[2]} for gt in train_gt_list] # N\n",
    "\n",
    "# Sanity Check\n",
    "print(\"train_human_features_list:\", train_human_features_list.shape)\n",
    "print(\"train_human_boxes_list:\", train_human_boxes_list.shape)\n",
    "print(\"train_human_poses_list:\", train_human_poses_list.shape)\n",
    "print(\"train_object_features_list:\", train_object_features_list.shape)\n",
    "print(\"train_object_boxes_list:\", train_object_boxes_list.shape)\n",
    "print(\"train_xs_steps:\", len(train_xs_steps))\n",
    "print(\"train_gt_list:\", len(train_gt_list))\n",
    "\n",
    "train_data = (\n",
    "    train_human_features_list, \n",
    "    train_human_boxes_list, \n",
    "    train_human_poses_list,\n",
    "    train_object_features_list, \n",
    "    train_object_boxes_list, \n",
    "    train_gt_list, \n",
    "    train_xs_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, scalers, _ = create_data_loader(\n",
    "    *train_data, \n",
    "    model_name, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    scaling_strategy=scaling_strategy, \n",
    "    sigma=sigma,\n",
    "    downsampling=downsampling,\n",
    ")\n",
    "val_loader, _, _ = create_data_loader(\n",
    "    *val_data, \n",
    "    model_name, \n",
    "    batch_size=len(val_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "test_loader, _, _ = create_data_loader(\n",
    "    *test_data, \n",
    "    model_name, \n",
    "    batch_size=len(test_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "input_size = input_size_from_data_loader(train_loader, model_name, model_input_type)\n",
    "data_info = {'input_size': input_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_creation_args = cfg.models.parameters\n",
    "model_creation_args = {**data_info, **model_creation_args.__dict__}\n",
    "dataset_name = cfg.data.name\n",
    "model_creation_args['num_classes'] = (num_classes, None)\n",
    "device = 'cuda' if torch.cuda.is_available() and cfg.resources.use_gpu else 'cpu'\n",
    "model = TGGCN(feat_dim=1024, **model_creation_args).to(device)\n",
    "if misc_dict.get('pretrained', False) and misc_dict.get('pretrained_path') is not None:\n",
    "    state_dict = load_model_weights(misc_dict['pretrained_path'])\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=cfg.models.optimization.learning_rate)\n",
    "criterion, loss_names = select_loss(model_name, model_input_type, dataset_name, cfg=cfg)\n",
    "mtll_model = None\n",
    "if misc_dict.get('multi_task_loss_learner', False):\n",
    "    loss_types = select_loss_types(model_name, dataset_name, cfg=cfg)\n",
    "    mask = select_loss_learning_mask(model_name, dataset_name, cfg=cfg)\n",
    "    mtll_model = MultiTaskLossLearner(loss_types=loss_types, mask=mask).to(device)\n",
    "    optimizer.add_param_group({'params': mtll_model.parameters()})\n",
    "# Some config + model training\n",
    "tensorboard_log_dir = cfg.models.logging.root_log_dir\n",
    "checkpoint_name = cfg.models.logging.checkpoint_name\n",
    "fetch_model_data = select_model_data_fetcher(model_name, model_input_type,\n",
    "                                             dataset_name=dataset_name, **{**misc_dict, **cfg.models.parameters.__dict__})\n",
    "feed_model_data = select_model_data_feeder(model_name, model_input_type, dataset_name=dataset_name, **misc_dict)\n",
    "num_main_losses = decide_num_main_losses(model_name, dataset_name, {**misc_dict, **cfg.models.parameters.__dict__})\n",
    "checkpoint = train(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    cfg.models.optimization.epochs, \n",
    "    device, \n",
    "    loss_names,\n",
    "    clip_gradient_at=cfg.models.optimization.clip_gradient_at,\n",
    "    fetch_model_data=fetch_model_data, feed_model_data=feed_model_data,\n",
    "    val_loader=val_loader, \n",
    "    mtll_model=mtll_model, \n",
    "    num_main_losses=num_main_losses,\n",
    "    tensorboard_log_dir=tensorboard_log_dir, \n",
    "    checkpoint_name=checkpoint_name,\n",
    ")\n",
    "# Logging\n",
    "if cfg.models.logging.log_dir is not None:\n",
    "    log_dir = cfg.models.logging.log_dir\n",
    "    checkpoint['scalers'] = scalers\n",
    "    save_checkpoint(log_dir, checkpoint, checkpoint_name=checkpoint_name, include_timestamp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from predict import match_shape, match_att_shape\n",
    "\n",
    "inspect_model = False\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets, attentions = [], [], []\n",
    "for i, dataset in enumerate(test_loader):\n",
    "    data, target = fetch_model_data(dataset, device=device)\n",
    "    with torch.no_grad():\n",
    "        output = feed_model_data(model, data)\n",
    "    if inspect_model:\n",
    "        output, attention_scores = output\n",
    "        attention_scores = [att_score[:, 0] for att_score in attention_scores]\n",
    "    if num_main_losses is not None:\n",
    "        output = output[-num_main_losses:]\n",
    "        target = target[-num_main_losses:]\n",
    "    if downsampling > 1:\n",
    "        for i, (out, tgt) in enumerate(zip(output, target)):\n",
    "            if out.ndim != 4:\n",
    "                raise RuntimeError(f'Number of dimensions for output is {out.ndim}')\n",
    "            out = torch.repeat_interleave(out, repeats=downsampling, dim=-2)\n",
    "            out = match_shape(out, tgt)\n",
    "            output[i] = out\n",
    "        if inspect_model:\n",
    "            a_target = target[0]\n",
    "            attention_scores = [torch.repeat_interleave(att_score, repeats=downsampling, dim=-2)\n",
    "                                for att_score in attention_scores]\n",
    "            attention_scores = [match_att_shape(att_score, a_target) for att_score in attention_scores]\n",
    "            attentions.append(attention_scores)\n",
    "    outputs += output\n",
    "    targets += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "# y_pred = torch.argmax(outputs[0], dim=1)\n",
    "# y_pred = y_pred.squeeze(-1).mode(dim=1).values.cpu().numpy()\n",
    "\n",
    "# CUSTOM\n",
    "y_pred = torch.argmax(outputs[0], dim=1).cpu().numpy()\n",
    "\n",
    "y_true = targets[0].squeeze(-1).mode(dim=1).values.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticklabels = action_classes\n",
    "if 'new_action_classes' in globals() and isinstance(new_action_classes, (list, tuple)):\n",
    "    y_true = [action_classes.index(new_action_classes[y]) for y in y_true]\n",
    "    y_pred = [action_classes.index(new_action_classes[y]) for y in y_pred]\n",
    "    \n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(ticklabels))))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=ticklabels, yticklabels=ticklabels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch271",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
