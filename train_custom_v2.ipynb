{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: `py39_torch271`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from dacite import from_dict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pyrutils.torch.train_utils import numpy_to_torch, train, save_checkpoint\n",
    "from pyrutils.torch.multi_task import MultiTaskLossLearner\n",
    "from vhoi.data_loading import (\n",
    "    segmentation_from_output_class, \n",
    "    compute_centroid, \n",
    "    ignore_last_step_end_flag_general, \n",
    "    smooth_segmentation, \n",
    "    maybe_scale_input_tensors, \n",
    "    input_size_from_data_loader, \n",
    "    select_model_data_feeder, \n",
    "    select_model_data_fetcher,\n",
    ")\n",
    "from vhoi.losses import (\n",
    "    select_loss, \n",
    "    decide_num_main_losses, \n",
    "    select_loss_types, \n",
    "    select_loss_learning_mask,\n",
    ")\n",
    "from vhoi.models import load_model_weights\n",
    "from vhoi.models_custom import TGGCN_Custom\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)   # Python的随机性\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "np.random.seed(seed)   # numpy的随机性\n",
    "torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True   # 选择确定性算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictMixin:\n",
    "    def get(self, key, default_value=None):\n",
    "        return getattr(self, key, default_value)\n",
    "\n",
    "    def as_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "@dataclass\n",
    "class Resources(DictMixin):\n",
    "    use_gpu: bool\n",
    "    num_threads: int\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata(DictMixin):\n",
    "    model_name: str\n",
    "    input_type: str\n",
    "\n",
    "@dataclass\n",
    "class ModelParameters(DictMixin):\n",
    "    add_segment_length: int\n",
    "    add_time_position: int\n",
    "    time_position_strategy: str\n",
    "    positional_encoding_style: str\n",
    "    attention_style: str\n",
    "    bias: bool\n",
    "    cat_level_states: int\n",
    "    discrete_networks_num_layers: int\n",
    "    discrete_optimization_strategy: str\n",
    "    filter_discrete_updates: bool\n",
    "    gcn_node: int\n",
    "    hidden_size: int\n",
    "    message_humans_to_human: bool\n",
    "    message_human_to_objects: bool\n",
    "    message_objects_to_human: bool\n",
    "    message_objects_to_object: bool\n",
    "    message_geometry_to_objects: bool\n",
    "    message_geometry_to_human: bool\n",
    "    message_segment: bool\n",
    "    message_type: str\n",
    "    message_granularity: str\n",
    "    message_aggregation: str\n",
    "    object_segment_update_strategy: str\n",
    "    share_level_mlps: int\n",
    "    update_segment_threshold: float\n",
    "\n",
    "@dataclass\n",
    "class ModelOptimization(DictMixin):\n",
    "    batch_size: int\n",
    "    clip_gradient_at: float\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    val_fraction: float\n",
    "\n",
    "@dataclass\n",
    "class BudgetLoss(DictMixin):\n",
    "    add: bool\n",
    "    human_weight: float\n",
    "    object_weight: float\n",
    "\n",
    "@dataclass\n",
    "class SegmentationLoss(DictMixin):\n",
    "    add: bool\n",
    "    pretrain: bool\n",
    "    sigma: float\n",
    "    weight: float\n",
    "\n",
    "@dataclass\n",
    "class ModelMisc(DictMixin):\n",
    "    anticipation_loss_weight: float\n",
    "    budget_loss: BudgetLoss\n",
    "    first_level_loss_weight: float\n",
    "    impose_segmentation_pattern: int\n",
    "    input_human_segmentation: bool\n",
    "    input_object_segmentation: bool\n",
    "    make_attention_distance_based: bool\n",
    "    multi_task_loss_learner: bool\n",
    "    pretrained: bool\n",
    "    pretrained_path: Optional[str]\n",
    "    segmentation_loss: SegmentationLoss\n",
    "\n",
    "@dataclass\n",
    "class ModelLogging(DictMixin):\n",
    "    root_log_dir: str\n",
    "    checkpoint_name: str\n",
    "    log_dir: str\n",
    "\n",
    "@dataclass\n",
    "class Models(DictMixin):\n",
    "    metadata: ModelMetadata\n",
    "    parameters: ModelParameters\n",
    "    optimization: ModelOptimization\n",
    "    misc: ModelMisc\n",
    "    logging: ModelLogging\n",
    "\n",
    "@dataclass\n",
    "class Data(DictMixin):\n",
    "    name: str\n",
    "    path: str\n",
    "    path_zarr: str\n",
    "    path_obb_zarr: str\n",
    "    path_hbb_zarr: str\n",
    "    path_hps_zarr: str\n",
    "    cross_validation_test_subject: str\n",
    "    scaling_strategy: Optional[str]\n",
    "    downsampling: int\n",
    "\n",
    "@dataclass\n",
    "class Config(DictMixin):\n",
    "    resources: Resources\n",
    "    models: Models\n",
    "    data: Data\n",
    "    \n",
    "metadata_dict = {\n",
    "    \"model_name\": \"2G-GCN\",\n",
    "    \"input_type\": \"multiple\"\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    \"add_segment_length\": 0,  # length of the segment to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"add_time_position\": 0,  # absolute time position to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"time_position_strategy\": \"s\",  # input time position to segment [s] or discrete update [u].\n",
    "    \"positional_encoding_style\": \"e\",  # e [embedding] or p [periodic].\n",
    "    \"attention_style\": \"v3\",  # v1 [concat], v2 [dot-product], v3 [scaled_dot-product], v4 [general]\n",
    "    \"bias\": True,\n",
    "    \"cat_level_states\": 0,  # concatenate first and second level hidden states for predictors MLPs.\n",
    "    \"discrete_networks_num_layers\": 1,  # depth of the state change detector MLP.\n",
    "    \"discrete_optimization_strategy\": \"gs\",  # straight-through [st] or gumbel-sigmoid [gs]\n",
    "    \"filter_discrete_updates\": False,  # maxima filter for soft output of state change detector.\n",
    "    \"gcn_node\": 25,  # custom, original: 19 for cad120, 30 for bimanual, 26 for mphoi\n",
    "    \"hidden_size\": 512,  # 512 for cad120 & mphoi; 64 for bimanual\n",
    "    \"message_humans_to_human\": False, # custom, original: True\n",
    "    \"message_human_to_objects\": True,\n",
    "    \"message_objects_to_human\": True,\n",
    "    \"message_objects_to_object\": False, # custom, original: True\n",
    "    \"message_geometry_to_objects\": True,\n",
    "    \"message_geometry_to_human\": True,  # custom, original: False\n",
    "    \"message_segment\": True,\n",
    "    \"message_type\": \"v2\",  # v1 [relational] or v2 [non-relational]\n",
    "    \"message_granularity\": \"v1\",  # v1 [generic] or v2 [specific]\n",
    "    \"message_aggregation\": \"att\",  # mean_pooling [mp] or attention [att]\n",
    "    \"object_segment_update_strategy\": \"ind\",  # same_as_human [sah], independent [ind], or conditional_on_human [coh]\n",
    "    \"share_level_mlps\": 0,  # whether to share [1] or not [0] the prediction MLPs of the levels.\n",
    "    \"update_segment_threshold\": 0.5  # [0.0, 1.0)\n",
    "}\n",
    "\n",
    "optimization_dict = {\n",
    "    \"batch_size\": 8,  # mphoi:8; cad120:16; bimanual: 32\n",
    "    \"clip_gradient_at\": 0.0,\n",
    "    \"epochs\": 10, # custom, original: cad120 & mphoi:40; bimanual: 60\n",
    "    \"learning_rate\": 1e-4,  # mphoi:1e-4; cad120 & bimanual:1e-3\n",
    "    \"val_fraction\": 0.1\n",
    "}\n",
    "\n",
    "data_dict = {\n",
    "    \"name\": \"mphoi\",\n",
    "    \"path\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_ground_truth_labels.json\",\n",
    "    \"path_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/faster_rcnn.zarr\",\n",
    "    \"path_obb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/object_bounding_boxes.zarr\",\n",
    "    \"path_hbb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_bounding_boxes.zarr\",\n",
    "    \"path_hps_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_pose.zarr\",\n",
    "    \"cross_validation_test_subject\": \"Subject14\",  # Subject45, Subject25, Subject14\n",
    "    \"scaling_strategy\": None,  # null or \"standard\"\n",
    "    \"downsampling\": 1 # custom, original: 3, 1 = full FPS, 2 = half FPS, ...\n",
    "}\n",
    "\n",
    "# root_log_dir = f\"{os.getcwd()}/outputs_hiergat/{data_dict['name']}/{metadata_dict['model_name']}\"\n",
    "root_log_dir = f\"{os.getcwd()}/outputs_hiergat/custom\"\n",
    "checkpoint_name = (\n",
    "    f\"hs{parameters_dict['hidden_size']}_e{optimization_dict['epochs']}_bs{optimization_dict['batch_size']}_\"\n",
    "    f\"lr{optimization_dict['learning_rate']}_{parameters_dict['update_segment_threshold']}_{data_dict['cross_validation_test_subject']}\"\n",
    ")\n",
    "log_dir = f\"{root_log_dir}/{checkpoint_name}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "cfg_dict = {\n",
    "    \"resources\": {\n",
    "        \"use_gpu\": True,\n",
    "        \"num_threads\": 32\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"metadata\": metadata_dict,\n",
    "        \"parameters\": parameters_dict,\n",
    "        \"optimization\": optimization_dict,\n",
    "        \"misc\": {\n",
    "            \"anticipation_loss_weight\": 1.0,\n",
    "            \"budget_loss\": {\n",
    "                \"add\": False,\n",
    "                \"human_weight\": 1.0,\n",
    "                \"object_weight\": 1.0\n",
    "            },\n",
    "            \"first_level_loss_weight\": 0.0,  # if positive, first level does frame-level prediction\n",
    "            \"impose_segmentation_pattern\": 1,  # 0 [no pattern], 1 [all ones]\n",
    "            \"input_human_segmentation\": False,  # (was \"flase\" in YAML, corrected here)\n",
    "            \"input_object_segmentation\": False,\n",
    "            \"make_attention_distance_based\": True,  # only meaningful if message_aggregation is attention\n",
    "            \"multi_task_loss_learner\": False,\n",
    "            \"pretrained\": False,  # unfortunately need two entries for checkpoint name\n",
    "            \"pretrained_path\": None,  # specified parameters must match pre-trained model\n",
    "            \"segmentation_loss\": {\n",
    "                \"add\": False,\n",
    "                \"pretrain\": False,\n",
    "                \"sigma\": 0.0,  # Gaussian smoothing\n",
    "                \"weight\": 1.0\n",
    "            }\n",
    "        },\n",
    "        \"logging\": {\n",
    "            \"root_log_dir\": root_log_dir,\n",
    "            \"checkpoint_name\": checkpoint_name,\n",
    "            \"log_dir\": log_dir\n",
    "        },\n",
    "    },\n",
    "    \"data\": data_dict,\n",
    "}\n",
    "\n",
    "cfg = from_dict(data_class=Config, data=cfg_dict)\n",
    "\n",
    "torch.set_num_threads(cfg.resources.num_threads)\n",
    "model_name, model_input_type = cfg.models.metadata.model_name, cfg.models.metadata.input_type\n",
    "batch_size, val_fraction = cfg.models.optimization.batch_size, cfg.models.optimization.val_fraction\n",
    "misc_dict = cfg.get('misc', default_value={})\n",
    "sigma = misc_dict.get('segmentation_loss', {}).get('sigma', 0.0)\n",
    "scaling_strategy = cfg.data.scaling_strategy\n",
    "downsampling = cfg.data.downsampling\n",
    "\n",
    "num_classes = 18\n",
    "features_dir = Path('/root/vs-gats-plaster/deepsort/outputs/anno_test_8/features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(feature_dirs, downsampling: int = 1):\n",
    "    human_features_list = []\n",
    "    human_boxes_list = []\n",
    "    human_poses_list = []\n",
    "    object_features_list = []\n",
    "    object_boxes_list = []\n",
    "    gt_list = []\n",
    "    xs_steps = []\n",
    "\n",
    "    for feature_dir in feature_dirs:\n",
    "        # Load and store human (subject) features\n",
    "        subject_visual_features = np.load(feature_dir / 'subject_visual_features.npy')\n",
    "        subject_boxes = np.load(feature_dir / 'subject_boxes.npy')\n",
    "        subject_poses = np.zeros((subject_visual_features.shape[0], 17, 2))\n",
    "        \n",
    "        human_features_list.append([subject_visual_features])\n",
    "        human_boxes_list.append([subject_boxes])\n",
    "        human_poses_list.append([subject_poses])\n",
    "        \n",
    "        # Load and store object features\n",
    "        object_visual_features = np.load(feature_dir / 'object_visual_features.npy')\n",
    "        object_boxes = np.load(feature_dir / 'object_boxes.npy')\n",
    "        \n",
    "        object_features_list.append(object_visual_features[:, np.newaxis, :])\n",
    "        object_boxes_list.append(object_boxes[:, np.newaxis, :])\n",
    "        \n",
    "        # Extract and store ground-truth action label\n",
    "        action_label = int(str(feature_dir).split('_action_')[-1])\n",
    "        seq_len = subject_visual_features.shape[0]\n",
    "        gt_list.append({\n",
    "            'Human1': [action_label] * seq_len,\n",
    "        })\n",
    "        \n",
    "        # Store number of steps\n",
    "        num_steps = len(subject_visual_features[downsampling - 1::downsampling])\n",
    "        xs_steps.append(num_steps)\n",
    "\n",
    "    xs_steps = np.array(xs_steps, dtype=np.float32)\n",
    "\n",
    "    return (\n",
    "        human_features_list,\n",
    "        human_boxes_list,\n",
    "        human_poses_list,\n",
    "        object_features_list,\n",
    "        object_boxes_list,\n",
    "        gt_list,\n",
    "        xs_steps,\n",
    "    )\n",
    "\n",
    "def split_list(lst, ratio=0.2):\n",
    "    n = len(lst)\n",
    "    split_idx = int(n * ratio) # number of items in first list\n",
    "    list1 = lst[split_idx:]\n",
    "    list2 = lst[:split_idx]\n",
    "    return list1, list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_frame_level_recurrent_human(\n",
    "    human_features_list, human_poses_list, object_boxes_list, gt_list,\n",
    "    downsampling: int = 1, \n",
    "    # test_data: bool = False, \n",
    "    max_no_objects: int = 4\n",
    "):\n",
    "    xs_h, xs_hp, x_obb = [], [], []\n",
    "    max_len, max_len_downsampled = 0, 0\n",
    "\n",
    "    if max_no_objects is None:\n",
    "        max_no_objects = max(\n",
    "            max(len(frame) for frame in video) for video in object_boxes_list\n",
    "        )\n",
    "\n",
    "    for humans, poses, objects_bounding_box in zip(human_features_list, human_poses_list, object_boxes_list):\n",
    "        num_humans = len(humans)\n",
    "        max_len = max(max_len, humans[0].shape[0])\n",
    "\n",
    "        humans_ds = [h[downsampling - 1::downsampling] for h in humans]\n",
    "        poses_ds  = [p[downsampling - 1::downsampling] / 1000 for p in poses]\n",
    "        max_len_downsampled = max(\n",
    "            max_len_downsampled,\n",
    "            max(h.shape[0] for h in humans_ds),\n",
    "            max(p.shape[0] for p in poses_ds),\n",
    "            objects_bounding_box[downsampling - 1::downsampling].shape[0],\n",
    "        )\n",
    "        xs_h.append(humans_ds)\n",
    "        xs_hp.append(poses_ds)\n",
    "\n",
    "        obb_ds = objects_bounding_box[downsampling - 1::downsampling] / 1000\n",
    "        x_obb.append(obb_ds)\n",
    "\n",
    "    xs_obb = []\n",
    "    for video in x_obb:\n",
    "        bb = []\n",
    "        for frame in video:\n",
    "            b = np.zeros((max_no_objects, 4))\n",
    "            n = min(len(frame), max_no_objects)\n",
    "            b[:n] = frame[:n]\n",
    "            b = b.reshape(max_no_objects * 2, 2)\n",
    "            bb.append(b)\n",
    "        xs_obb.append(bb)\n",
    "\n",
    "    # keypoints = [1, 2, 4, 6, 7, 11, 13, 14, 27]\n",
    "    keypoints = list(range(human_poses_list[0][0].shape[1]))\n",
    "    xs_h_with_context = []\n",
    "    for i, (humans_ds, poses_ds, obb_video) in enumerate(zip(xs_h, xs_hp, xs_obb)):\n",
    "        num_humans = len(humans_ds)\n",
    "        humans_context = [[] for _ in range(num_humans)]\n",
    "\n",
    "        for j in range(len(humans_ds[0])):\n",
    "            obb = obb_video[j]\n",
    "\n",
    "            if j + 1 < len(humans_ds[0]):\n",
    "                next_poses = [p[j+1][keypoints] for p in poses_ds]\n",
    "                pose_velos = [(next_pose - poses_ds[h][j][keypoints]) * 100 for h, next_pose in enumerate(next_poses)]\n",
    "                obb_velo = (obb_video[j+1] - obb) * 100\n",
    "            else:\n",
    "                pose_velos = [np.zeros((len(keypoints), 2)) for _ in poses_ds]\n",
    "                obb_velo = np.zeros((max_no_objects * 2, 2))\n",
    "\n",
    "            obbvelo = np.hstack((obb, obb_velo)).reshape(1, -1)\n",
    "\n",
    "            context = []\n",
    "            for h in range(num_humans):\n",
    "                pose = poses_ds[h][j][keypoints]\n",
    "                velo = pose_velos[h]\n",
    "                posevelo = np.hstack((pose, velo)).reshape(1, -1)\n",
    "                context.append(posevelo[0])\n",
    "\n",
    "            context = np.concatenate(context + [obbvelo[0]])\n",
    "\n",
    "            for h in range(num_humans):\n",
    "                h_con = np.concatenate((humans_ds[h][j], context))\n",
    "                humans_context[h].append(h_con)\n",
    "\n",
    "        xs_h_with_context.append([np.array(hc) for hc in humans_context])\n",
    "\n",
    "    feature_size = xs_h_with_context[0][0].shape[-1]\n",
    "    num_humans = len(xs_h_with_context[0])\n",
    "    x_hs = np.full([len(xs_h_with_context), max_len_downsampled, num_humans, feature_size],\n",
    "                   fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, humans in enumerate(xs_h_with_context):\n",
    "        for h, feats in enumerate(humans):\n",
    "            seq_len = min(len(feats), max_len_downsampled)\n",
    "            x_hs[m, :seq_len, h] = feats[:seq_len]\n",
    "\n",
    "    xs = [x_hs]\n",
    "\n",
    "    # ----------------------\n",
    "    # Outputs\n",
    "    # ----------------------\n",
    "    y_rec_hs = np.full([len(x_hs), max_len_downsampled, num_humans], fill_value=-1, dtype=np.int64)\n",
    "    y_pred_hs = np.full_like(y_rec_hs, fill_value=-1)\n",
    "    \n",
    "    for m, video_hands_ground_truth in enumerate(gt_list):\n",
    "        for h in range(num_humans):\n",
    "            human_key = f\"Human{h+1}\"\n",
    "            if human_key not in video_hands_ground_truth:\n",
    "                continue\n",
    "\n",
    "            y_h = video_hands_ground_truth[human_key]\n",
    "\n",
    "            # Ground truth (downsampled)\n",
    "            y_h_ds = y_h[downsampling - 1::downsampling]\n",
    "            seq_len = min(len(y_h_ds), y_rec_hs.shape[1])\n",
    "            y_rec_hs[m, :seq_len, h] = y_h_ds[:seq_len]\n",
    "\n",
    "            # Prediction: shift labels forward\n",
    "            y_h_p = np.roll(y_h, -1)\n",
    "            y_h_p[-1] = -1  # last frame has no \"next\"\n",
    "            y_h_p_ds = y_h_p[downsampling - 1::downsampling]\n",
    "            seq_len_p = min(len(y_h_p_ds), y_pred_hs.shape[1])\n",
    "            y_pred_hs[m, :seq_len_p, h] = y_h_p_ds[:seq_len_p]\n",
    "            # y_pred_hs[m, :seq_len, h] = y_h_ds[:seq_len]\n",
    "            \n",
    "    x_hs_segmentation = segmentation_from_output_class(y_rec_hs, segmentation_type=\"input\")\n",
    "    xs.append(x_hs_segmentation)\n",
    "\n",
    "    y_hs_segmentation = segmentation_from_output_class(y_rec_hs, segmentation_type=\"output\")\n",
    "    ys = [y_rec_hs, y_pred_hs, y_hs_segmentation]\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "# xs, ys = assemble_mphoi_frame_level_recurrent_human(human_features_list, human_poses_list, object_boxes_list, gt_list)\n",
    "\n",
    "# for ys_i, xs_i in zip(ys, xs):\n",
    "#     print(\"ys_i.shape:\", ys_i.shape)\n",
    "#     print(\"ys_i[0]:\", ys_i[0].flatten())\n",
    "#     print(\"ys_i[1]:\", ys_i[1].flatten())\n",
    "#     print(\"ys_i[2]:\", ys_i[2].flatten())\n",
    "#     print()\n",
    "#     print(\"xs_i:\", xs_i.shape)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_frame_level_recurrent_objects(object_features_list, downsampling: int = 1):\n",
    "    xs_objects = []\n",
    "    max_len, max_len_downsampled, max_num_objects = 0, 0, 0\n",
    "    for objects in object_features_list:\n",
    "        max_len = max(max_len, objects.shape[0])\n",
    "        max_num_objects = max(max_num_objects, objects.shape[1])\n",
    "        objects = objects[downsampling - 1::downsampling]\n",
    "        max_len_downsampled = max(max_len_downsampled, objects.shape[0])\n",
    "        xs_objects.append(objects)\n",
    "    feature_size = xs_objects[-1].shape[-1]\n",
    "    x_objects = np.full([len(xs_objects), max_len_downsampled, max_num_objects, feature_size],\n",
    "                        fill_value=np.nan, dtype=np.float32)\n",
    "    x_objects_mask = np.zeros([len(xs_objects), max_num_objects], dtype=np.float32)\n",
    "    for m, x_o in enumerate(xs_objects):\n",
    "        x_objects[m, :x_o.shape[0], :x_o.shape[1], :] = x_o\n",
    "        x_objects_mask[m, :x_o.shape[1]] = 1.0\n",
    "    xs = [x_objects, x_objects_mask]\n",
    "    return xs\n",
    "\n",
    "# xs_objects = assemble_mphoi_frame_level_recurrent_objects(object_features_list)\n",
    "# for xs_i in xs_objects:\n",
    "#     print(xs_i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_human_human_distances(human_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute pairwise human-human distances for multiple humans across videos.\n",
    "\n",
    "    Args:\n",
    "        human_boxes_list: list of list of human bounding boxes per video\n",
    "                          (outer list: videos, inner list: humans, array: frames x 4)\n",
    "        downsampling:     frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_hh_dists: tensor [num_videos, max_len, N, N] with pairwise distances\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_humans = 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for video_bbs in human_boxes_list:\n",
    "        num_humans = len(video_bbs)\n",
    "        max_num_humans = max(max_num_humans, num_humans)\n",
    "\n",
    "        # Downsample and compute centroids\n",
    "        centroids = []\n",
    "        for bb in video_bbs:\n",
    "            bb = bb[downsampling - 1::downsampling]\n",
    "            c = compute_centroid(bb) / mphoi_dims\n",
    "            centroids.append(c)\n",
    "\n",
    "        # Length of this video (frames)\n",
    "        max_len = max(max_len, centroids[0].shape[0])\n",
    "\n",
    "        # Compute pairwise distances (frames x N x N)\n",
    "        T = centroids[0].shape[0]\n",
    "        dists_matrix = np.zeros((T, num_humans, num_humans), dtype=np.float32)\n",
    "\n",
    "        for i, j in itertools.combinations(range(num_humans), 2):\n",
    "            d = np.linalg.norm(centroids[i] - centroids[j], ord=2, axis=-1)\n",
    "            dists_matrix[:, i, j] = d\n",
    "            dists_matrix[:, j, i] = d\n",
    "\n",
    "        all_dists.append(dists_matrix)\n",
    "\n",
    "    # Pad into a tensor [num_videos, max_len, max_num_humans, max_num_humans]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_humans, max_num_humans]\n",
    "    x_hh_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists_matrix in enumerate(all_dists):\n",
    "        T, N, _ = dists_matrix.shape\n",
    "        x_hh_dists[m, :T, :N, :N] = dists_matrix\n",
    "\n",
    "    return x_hh_dists\n",
    "\n",
    "# xs_hh_dists = assemble_mphoi_human_human_distances(human_boxes_list)\n",
    "# print(xs_hh_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_human_object_distances(human_boxes_list, object_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute human-object distances for multiple humans and objects across videos.\n",
    "\n",
    "    Args:\n",
    "        human_boxes_list:  list of list of human bounding boxes per video\n",
    "                           (outer list: videos, inner list: humans, array: frames x 4)\n",
    "        object_boxes_list: list of object bounding box arrays per video (frames x num_objects x 4)\n",
    "        downsampling:      frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_ho_dists: tensor [num_videos, max_len, max_num_humans, max_num_objects]\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_humans, max_num_objects = 0, 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for video_bbs, obj_bbs in zip(human_boxes_list, object_boxes_list):\n",
    "        num_humans = len(video_bbs)\n",
    "\n",
    "        # Downsample humans → centroids\n",
    "        human_centroids = []\n",
    "        for bb in video_bbs:\n",
    "            bb = bb[downsampling - 1::downsampling]\n",
    "            c = compute_centroid(bb) / mphoi_dims\n",
    "            human_centroids.append(c)\n",
    "\n",
    "        # Downsample objects → centroids\n",
    "        obj_bbs = obj_bbs[downsampling - 1::downsampling]\n",
    "        obj_centroids = compute_centroid(obj_bbs) / mphoi_dims\n",
    "\n",
    "        T = obj_centroids.shape[0]\n",
    "        max_len = max(max_len, T)\n",
    "        max_num_humans = max(max_num_humans, num_humans)\n",
    "        max_num_objects = max(max_num_objects, obj_centroids.shape[1])\n",
    "\n",
    "        # Compute distances [frames, num_humans, num_objects]\n",
    "        dists_matrix = np.zeros((T, num_humans, obj_centroids.shape[1]), dtype=np.float32)\n",
    "        for h, h_c in enumerate(human_centroids):\n",
    "            d = np.linalg.norm(obj_centroids - np.expand_dims(h_c, axis=1), ord=2, axis=-1)\n",
    "            dists_matrix[:, h, :] = d\n",
    "\n",
    "        all_dists.append(dists_matrix)\n",
    "\n",
    "    # Pad into a tensor [num_videos, max_len, max_num_humans, max_num_objects]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_humans, max_num_objects]\n",
    "    x_ho_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists_matrix in enumerate(all_dists):\n",
    "        T, H, O = dists_matrix.shape\n",
    "        x_ho_dists[m, :T, :H, :O] = dists_matrix\n",
    "\n",
    "    return x_ho_dists\n",
    "\n",
    "# xs_ho_dists = assemble_mphoi_human_object_distances(human_boxes_list, object_boxes_list)\n",
    "# print(xs_ho_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_object_object_distances(object_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute pairwise object-object distances across videos.\n",
    "\n",
    "    Args:\n",
    "        object_boxes_list: list of object bounding box arrays per video (frames x num_objects x 4)\n",
    "        downsampling:      frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_oo_dists: tensor [num_videos, max_len, max_num_objects, max_num_objects]\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_objects = 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for obj_bbs in object_boxes_list:\n",
    "        # Downsample and compute centroids\n",
    "        obj_bbs = obj_bbs[downsampling - 1::downsampling]\n",
    "        objs_centroid = compute_centroid(obj_bbs) / mphoi_dims   # (frames, num_objects, 2)\n",
    "        num_objects = objs_centroid.shape[1]\n",
    "\n",
    "        # Compute pairwise distances per frame\n",
    "        dists = []\n",
    "        for k in range(num_objects):\n",
    "            kth_object_centroid = objs_centroid[:, k:k+1]  # (frames, 1, 2)\n",
    "            kth_dist = np.linalg.norm(objs_centroid - kth_object_centroid, ord=2, axis=-1)  # (frames, num_objects)\n",
    "            dists.append(kth_dist)\n",
    "\n",
    "        dists = np.stack(dists, axis=1)  # (frames, num_objects, num_objects)\n",
    "        all_dists.append(dists)\n",
    "\n",
    "        max_len = max(max_len, obj_bbs.shape[0])\n",
    "        max_num_objects = max(max_num_objects, num_objects)\n",
    "\n",
    "    # Pad into tensor [num_videos, max_len, max_num_objects, max_num_objects]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_objects, max_num_objects]\n",
    "    x_oo_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists in enumerate(all_dists):\n",
    "        T, O1, O2 = dists.shape\n",
    "        x_oo_dists[m, :T, :O1, :O2] = dists\n",
    "\n",
    "    return x_oo_dists\n",
    "\n",
    "# xs_oo_dists = assemble_mphoi_object_object_distances(object_boxes_list)\n",
    "# print(xs_oo_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_tensors(\n",
    "    human_features_list,\n",
    "    human_boxes_list,\n",
    "    human_poses_list,\n",
    "    object_features_list,\n",
    "    object_boxes_list,\n",
    "    gt_list,\n",
    "    xs_steps,\n",
    "    model_name: str, \n",
    "    sigma: float = 0.0, \n",
    "    downsampling: int = 1,\n",
    "):\n",
    "    xs, ys = assemble_mphoi_frame_level_recurrent_human(human_features_list, human_poses_list, object_boxes_list, gt_list)\n",
    "    xs_objects = assemble_mphoi_frame_level_recurrent_objects(object_features_list, downsampling=downsampling)\n",
    "    if model_name == '2G-GCN':\n",
    "        if sigma:\n",
    "            ys[2] = ignore_last_step_end_flag_general(ys[2])\n",
    "        ys[2] = smooth_segmentation(ys[2], sigma)\n",
    "        ys_budget = ys[2]\n",
    "        xs_hh_dists = assemble_mphoi_human_human_distances(human_boxes_list, downsampling=downsampling)\n",
    "        xs_ho_dists = assemble_mphoi_human_object_distances(human_boxes_list, object_boxes_list, downsampling=downsampling)\n",
    "        xs_oo_dists = assemble_mphoi_object_object_distances(object_boxes_list, downsampling=downsampling)\n",
    "        xs = xs[:1] + xs_objects + xs[1:] + [xs_hh_dists, xs_ho_dists, xs_oo_dists, xs_steps]\n",
    "        ys = [ys_budget] + ys[2:] + ys[:2]\n",
    "        ys += ys[-2:]\n",
    "    else:\n",
    "        raise ValueError(f'MPHOI code not implemented for {model_name} yet.')\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(\n",
    "    human_features_list,\n",
    "    human_boxes_list,\n",
    "    human_poses_list,\n",
    "    object_features_list,\n",
    "    object_boxes_list,\n",
    "    gt_list,\n",
    "    xs_steps,\n",
    "    model_name: str, \n",
    "    batch_size: int, \n",
    "    shuffle: bool,\n",
    "    scaling_strategy: Optional[str] = None, \n",
    "    scalers: Optional[dict] = None, \n",
    "    sigma: float = 0.0,\n",
    "    downsampling: int = 1, \n",
    "):\n",
    "    x, y = assemble_mphoi_tensors(\n",
    "        human_features_list,\n",
    "        human_boxes_list,\n",
    "        human_poses_list,\n",
    "        object_features_list,\n",
    "        object_boxes_list,\n",
    "        gt_list,\n",
    "        xs_steps,\n",
    "        model_name=model_name, \n",
    "        sigma=sigma, \n",
    "        downsampling=downsampling, \n",
    "    )\n",
    "    \n",
    "    x, scalers = maybe_scale_input_tensors(x, model_name, scaling_strategy=scaling_strategy, scalers=scalers)\n",
    "    x = [np.nan_to_num(ix, copy=False, nan=0.0) for ix in x]\n",
    "    x, y = numpy_to_torch(*x), numpy_to_torch(*y)\n",
    "    dataset = TensorDataset(*(x + y))\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0,\n",
    "                             pin_memory=False, drop_last=False)\n",
    "    segmentations = None\n",
    "    return data_loader, scalers, segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "feature_dirs = [i for i in features_dir.iterdir()]\n",
    "train_feature_dirs, val_feature_dirs = split_list(feature_dirs, ratio=0.2)\n",
    "\n",
    "train_data = create_data(train_feature_dirs)\n",
    "val_data = create_data(val_feature_dirs)\n",
    "\n",
    "train_loader, scalers, _ = create_data_loader(\n",
    "    *train_data, \n",
    "    model_name, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    scaling_strategy=scaling_strategy, \n",
    "    sigma=sigma,\n",
    "    downsampling=downsampling,\n",
    ")\n",
    "val_loader, _, _ = create_data_loader(\n",
    "    *val_data, \n",
    "    model_name, \n",
    "    batch_size=len(val_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "input_size = input_size_from_data_loader(train_loader, model_name, model_input_type)\n",
    "data_info = {'input_size': input_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [   1/  10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   5.7214  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  2.8321  NLL_SAP:  2.8893\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   2.8841  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.4479  NLL_SAP:  1.4363\n",
      "     (Train) Loss:  3.3070   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.6399   NLL_SAP:  1.6671\n",
      "(Validation) Loss:  4.4901   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  2.2077   NLL_SAP:  2.2824\n",
      "\n",
      "Epoch: [   2/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   2.5169  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.2484  NLL_SAP:  1.2685\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   4.6308  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  2.3349  NLL_SAP:  2.2959\n",
      "     (Train) Loss:  3.1222   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.5560   NLL_SAP:  1.5662\n",
      "(Validation) Loss:  4.3519   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  2.1906   NLL_SAP:  2.1614\n",
      "\n",
      "Epoch: [   3/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   2.3178  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.1549  NLL_SAP:  1.1630\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   2.3181  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.1318  NLL_SAP:  1.1862\n",
      "     (Train) Loss:  3.0030   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.4934   NLL_SAP:  1.5096\n",
      "(Validation) Loss:  3.8022   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.9248   NLL_SAP:  1.8774\n",
      "\n",
      "Epoch: [   4/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   3.2175  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.5837  NLL_SAP:  1.6338\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   2.2532  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.1596  NLL_SAP:  1.0936\n",
      "     (Train) Loss:  2.8529   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.4265   NLL_SAP:  1.4264\n",
      "(Validation) Loss:  3.5909   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.7716   NLL_SAP:  1.8193\n",
      "\n",
      "Epoch: [   5/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   2.8112  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.3951  NLL_SAP:  1.4161\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   3.1309  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.5788  NLL_SAP:  1.5522\n",
      "     (Train) Loss:  2.8213   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.4115   NLL_SAP:  1.4098\n",
      "(Validation) Loss:  3.7175   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.8675   NLL_SAP:  1.8500\n",
      "\n",
      "Epoch: [   6/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   2.8882  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.4373  NLL_SAP:  1.4509\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   3.7099  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.8553  NLL_SAP:  1.8546\n",
      "     (Train) Loss:  2.8224   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.4129   NLL_SAP:  1.4095\n",
      "(Validation) Loss:  3.5809   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.7887   NLL_SAP:  1.7922\n",
      "\n",
      "Epoch: [   7/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   2.4597  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.2094  NLL_SAP:  1.2503\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   4.3095  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  2.1904  NLL_SAP:  2.1191\n",
      "     (Train) Loss:  2.7367   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.3719   NLL_SAP:  1.3647\n",
      "(Validation) Loss:  3.7544   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.8944   NLL_SAP:  1.8600\n",
      "\n",
      "Epoch: [   8/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   2.4934  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.2551  NLL_SAP:  1.2383\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   2.5352  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.2790  NLL_SAP:  1.2562\n",
      "     (Train) Loss:  2.6847   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.3426   NLL_SAP:  1.3421\n",
      "(Validation) Loss:  3.5303   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.7603   NLL_SAP:  1.7700\n",
      "\n",
      "Epoch: [   9/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   2.3586  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.1789  NLL_SAP:  1.1797\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   2.0483  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.0228  NLL_SAP:  1.0255\n",
      "     (Train) Loss:  2.5503   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.2755   NLL_SAP:  1.2748\n",
      "(Validation) Loss:  3.4521   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.7187   NLL_SAP:  1.7334\n",
      "\n",
      "Epoch: [  10/  10]\n",
      "(Train) Batch [     8/    69 ( 11%)]  Loss:   3.5212  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  1.7686  NLL_SAP:  1.7526\n",
      "(Train) Batch [    69/    69 (100%)]  Loss:   1.1211  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  0.5543  NLL_SAP:  0.5668\n",
      "     (Train) Loss:  2.3344   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.1688   NLL_SAP:  1.1656\n",
      "(Validation) Loss:  3.5220   B_HS:  0.0000   BCE_HS:  0.0000   NLL_SAR_F:  0.0000   NLL_SAP_F:  0.0000   NLL_SAR:  1.7586   NLL_SAP:  1.7634\n",
      "Lowest val_loss is 3.4520676136016846\n",
      "log files written to /root/workspace/HierGAT-2025/outputs_hiergat/custom/hs512_e10_bs8_lr0.0001_0.5_Subject14/hs512_e10_bs8_lr0.0001_0.5_Subject14.tar\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model_creation_args = cfg.models.parameters\n",
    "model_creation_args = {**data_info, **model_creation_args.__dict__}\n",
    "dataset_name = cfg.data.name\n",
    "model_creation_args['num_classes'] = (num_classes, None)\n",
    "device = 'cuda' if torch.cuda.is_available() and cfg.resources.use_gpu else 'cpu'\n",
    "model = TGGCN_Custom(feat_dim=1024, **model_creation_args).to(device)\n",
    "if misc_dict.get('pretrained', False) and misc_dict.get('pretrained_path') is not None:\n",
    "    state_dict = load_model_weights(misc_dict['pretrained_path'])\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=cfg.models.optimization.learning_rate)\n",
    "criterion, loss_names = select_loss(model_name, model_input_type, dataset_name, cfg=cfg)\n",
    "mtll_model = None\n",
    "if misc_dict.get('multi_task_loss_learner', False):\n",
    "    loss_types = select_loss_types(model_name, dataset_name, cfg=cfg)\n",
    "    mask = select_loss_learning_mask(model_name, dataset_name, cfg=cfg)\n",
    "    mtll_model = MultiTaskLossLearner(loss_types=loss_types, mask=mask).to(device)\n",
    "    optimizer.add_param_group({'params': mtll_model.parameters()})\n",
    "# Some config + model training\n",
    "tensorboard_log_dir = cfg.models.logging.root_log_dir\n",
    "checkpoint_name = cfg.models.logging.checkpoint_name\n",
    "fetch_model_data = select_model_data_fetcher(model_name, model_input_type,\n",
    "                                             dataset_name=dataset_name, **{**misc_dict, **cfg.models.parameters.__dict__})\n",
    "feed_model_data = select_model_data_feeder(model_name, model_input_type, dataset_name=dataset_name, **misc_dict)\n",
    "num_main_losses = decide_num_main_losses(model_name, dataset_name, {**misc_dict, **cfg.models.parameters.__dict__})\n",
    "checkpoint = train(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    cfg.models.optimization.epochs, \n",
    "    device, \n",
    "    loss_names,\n",
    "    clip_gradient_at=cfg.models.optimization.clip_gradient_at,\n",
    "    fetch_model_data=fetch_model_data, feed_model_data=feed_model_data,\n",
    "    val_loader=val_loader, \n",
    "    mtll_model=mtll_model, \n",
    "    num_main_losses=num_main_losses,\n",
    "    tensorboard_log_dir=tensorboard_log_dir, \n",
    "    checkpoint_name=checkpoint_name,\n",
    ")\n",
    "# Logging\n",
    "if cfg.models.logging.log_dir is not None:\n",
    "    log_dir = cfg.models.logging.log_dir\n",
    "    checkpoint['scalers'] = scalers\n",
    "    save_checkpoint(log_dir, checkpoint, checkpoint_name=checkpoint_name, include_timestamp=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch271",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
