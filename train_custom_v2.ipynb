{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: `py39_torch271`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "\n",
    "from pyrutils.torch.train_utils import train, save_checkpoint\n",
    "from pyrutils.torch.multi_task import MultiTaskLossLearner\n",
    "from vhoi.data_loading import load_training_data, select_model_data_feeder, select_model_data_fetcher\n",
    "from vhoi.data_loading import determine_num_classes\n",
    "from vhoi.losses import select_loss, decide_num_main_losses, select_loss_types, select_loss_learning_mask\n",
    "from vhoi.models import select_model, load_model_weights\n",
    "from vhoi.models_custom import TGGCN_Custom\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from dacite import from_dict\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)   # Python的随机性\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "np.random.seed(seed)   # numpy的随机性\n",
    "torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True   # 选择确定性算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "class DictMixin:\n",
    "    def get(self, key, default_value=None):\n",
    "        return getattr(self, key, default_value)\n",
    "\n",
    "    def as_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "@dataclass\n",
    "class Resources(DictMixin):\n",
    "    use_gpu: bool\n",
    "    num_threads: int\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata(DictMixin):\n",
    "    model_name: str\n",
    "    input_type: str\n",
    "\n",
    "@dataclass\n",
    "class ModelParameters(DictMixin):\n",
    "    add_segment_length: int\n",
    "    add_time_position: int\n",
    "    time_position_strategy: str\n",
    "    positional_encoding_style: str\n",
    "    attention_style: str\n",
    "    bias: bool\n",
    "    cat_level_states: int\n",
    "    discrete_networks_num_layers: int\n",
    "    discrete_optimization_strategy: str\n",
    "    filter_discrete_updates: bool\n",
    "    gcn_node: int\n",
    "    hidden_size: int\n",
    "    message_humans_to_human: bool\n",
    "    message_human_to_objects: bool\n",
    "    message_objects_to_human: bool\n",
    "    message_objects_to_object: bool\n",
    "    message_geometry_to_objects: bool\n",
    "    message_geometry_to_human: bool\n",
    "    message_segment: bool\n",
    "    message_type: str\n",
    "    message_granularity: str\n",
    "    message_aggregation: str\n",
    "    object_segment_update_strategy: str\n",
    "    share_level_mlps: int\n",
    "    update_segment_threshold: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOptimization(DictMixin):\n",
    "    batch_size: int\n",
    "    clip_gradient_at: float\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    val_fraction: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BudgetLoss(DictMixin):\n",
    "    add: bool\n",
    "    human_weight: float\n",
    "    object_weight: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegmentationLoss(DictMixin):\n",
    "    add: bool\n",
    "    pretrain: bool\n",
    "    sigma: float\n",
    "    weight: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelMisc(DictMixin):\n",
    "    anticipation_loss_weight: float\n",
    "    budget_loss: BudgetLoss\n",
    "    first_level_loss_weight: float\n",
    "    impose_segmentation_pattern: int\n",
    "    input_human_segmentation: bool\n",
    "    input_object_segmentation: bool\n",
    "    make_attention_distance_based: bool\n",
    "    multi_task_loss_learner: bool\n",
    "    pretrained: bool\n",
    "    pretrained_path: Optional[str]\n",
    "    segmentation_loss: SegmentationLoss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelLogging(DictMixin):\n",
    "    root_log_dir: str\n",
    "    checkpoint_name: str\n",
    "    log_dir: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Models(DictMixin):\n",
    "    metadata: ModelMetadata\n",
    "    parameters: ModelParameters\n",
    "    optimization: ModelOptimization\n",
    "    misc: ModelMisc\n",
    "    logging: ModelLogging\n",
    "\n",
    "@dataclass\n",
    "class Data(DictMixin):\n",
    "    name: str\n",
    "    path: str\n",
    "    path_zarr: str\n",
    "    path_obb_zarr: str\n",
    "    path_hbb_zarr: str\n",
    "    path_hps_zarr: str\n",
    "    cross_validation_test_subject: str\n",
    "    scaling_strategy: Optional[str]\n",
    "    downsampling: int\n",
    "\n",
    "@dataclass\n",
    "class Config(DictMixin):\n",
    "    resources: Resources\n",
    "    models: Models\n",
    "    data: Data\n",
    "\n",
    "cfg_dict = {\n",
    "    \"resources\": {\n",
    "        \"use_gpu\": True,\n",
    "        \"num_threads\": 32\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"metadata\": {\n",
    "            \"model_name\": \"2G-GCN\",\n",
    "            \"input_type\": \"multiple\"\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"add_segment_length\": 0,  # length of the segment to the segment-level rnn. 0 is off and 1 is on.\n",
    "            \"add_time_position\": 0,  # absolute time position to the segment-level rnn. 0 is off and 1 is on.\n",
    "            \"time_position_strategy\": \"s\",  # input time position to segment [s] or discrete update [u].\n",
    "            \"positional_encoding_style\": \"e\",  # e [embedding] or p [periodic].\n",
    "            \"attention_style\": \"v3\",  # v1 [concat], v2 [dot-product], v3 [scaled_dot-product], v4 [general]\n",
    "            \"bias\": True,\n",
    "            \"cat_level_states\": 0,  # concatenate first and second level hidden states for predictors MLPs.\n",
    "            \"discrete_networks_num_layers\": 1,  # depth of the state change detector MLP.\n",
    "            \"discrete_optimization_strategy\": \"gs\",  # straight-through [st] or gumbel-sigmoid [gs]\n",
    "            \"filter_discrete_updates\": False,  # maxima filter for soft output of state change detector.\n",
    "            \"gcn_node\": 26,  # 19 for cad120, 30 for bimanual, 26 for mphoi\n",
    "            \"hidden_size\": 512,  # 512 for cad120 & mphoi; 64 for bimanual\n",
    "            \"message_humans_to_human\": True,  # only meaningful for bimanual and mphoi\n",
    "            \"message_human_to_objects\": True,\n",
    "            \"message_objects_to_human\": True,\n",
    "            \"message_objects_to_object\": True,\n",
    "            \"message_geometry_to_objects\": True,\n",
    "            \"message_geometry_to_human\": False,  # false in original, note this!\n",
    "            \"message_segment\": True,\n",
    "            \"message_type\": \"v2\",  # v1 [relational] or v2 [non-relational]\n",
    "            \"message_granularity\": \"v1\",  # v1 [generic] or v2 [specific]\n",
    "            \"message_aggregation\": \"att\",  # mean_pooling [mp] or attention [att]\n",
    "            \"object_segment_update_strategy\": \"ind\",  # same_as_human [sah], independent [ind], or conditional_on_human [coh]\n",
    "            \"share_level_mlps\": 0,  # whether to share [1] or not [0] the prediction MLPs of the levels.\n",
    "            \"update_segment_threshold\": 0.5  # [0.0, 1.0)\n",
    "        },\n",
    "        \"optimization\": {\n",
    "        \"batch_size\": 8,  # mphoi:8; cad120:16; bimanual: 32\n",
    "        \"clip_gradient_at\": 0.0,\n",
    "        \"epochs\": 40,  # cad120 & mphoi:40; bimanual: 60\n",
    "        \"learning_rate\": 1e-4,  # mphoi:1e-4; cad120 & bimanual:1e-3\n",
    "        \"val_fraction\": 0.1\n",
    "    },\n",
    "    \"misc\": {\n",
    "        \"anticipation_loss_weight\": 1.0,\n",
    "        \"budget_loss\": {\n",
    "            \"add\": False,\n",
    "            \"human_weight\": 1.0,\n",
    "            \"object_weight\": 1.0\n",
    "        },\n",
    "        \"first_level_loss_weight\": 0.0,  # if positive, first level does frame-level prediction\n",
    "        \"impose_segmentation_pattern\": 1,  # 0 [no pattern], 1 [all ones]\n",
    "        \"input_human_segmentation\": False,  # (was \"flase\" in YAML, corrected here)\n",
    "        \"input_object_segmentation\": False,\n",
    "        \"make_attention_distance_based\": True,  # only meaningful if message_aggregation is attention\n",
    "        \"multi_task_loss_learner\": False,\n",
    "        \"pretrained\": False,  # unfortunately need two entries for checkpoint name\n",
    "        \"pretrained_path\": None,  # specified parameters must match pre-trained model\n",
    "        \"segmentation_loss\": {\n",
    "            \"add\": False,\n",
    "            \"pretrain\": False,\n",
    "            \"sigma\": 0.0,  # Gaussian smoothing\n",
    "            \"weight\": 1.0\n",
    "        }\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"root_log_dir\": \"${oc.env:PWD}/outputs_hiergat/${data.name}/${models.metadata.model_name}\",\n",
    "        \"checkpoint_name\": (\n",
    "            \"hs${models.parameters.hidden_size}_e${models.optimization.epochs}_bs${models.optimization.batch_size}_\"\n",
    "            \"lr${models.optimization.learning_rate}_${models.parameters.update_segment_threshold}_${data.cross_validation_test_subject}\"\n",
    "        ),\n",
    "        \"log_dir\": \"${models.logging.root_log_dir}/${models.logging.checkpoint_name}\"\n",
    "    },\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"name\": \"mphoi\",\n",
    "        \"path\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_ground_truth_labels.json\",\n",
    "        \"path_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/faster_rcnn.zarr\",\n",
    "        \"path_obb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/object_bounding_boxes.zarr\",\n",
    "        \"path_hbb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_bounding_boxes.zarr\",\n",
    "        \"path_hps_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_pose.zarr\",\n",
    "        \"cross_validation_test_subject\": \"Subject14\",  # Subject45, Subject25, Subject14\n",
    "        \"scaling_strategy\": None,  # null or \"standard\"\n",
    "        \"downsampling\": 3  # 1 = full FPS, 2 = half FPS, ...\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = from_dict(data_class=Config, data=cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.set_num_threads(cfg.resources.num_threads)\n",
    "# Data\n",
    "model_name, model_input_type = cfg.models.metadata.model_name, cfg.models.metadata.input_type\n",
    "batch_size, val_fraction = cfg.models.optimization.batch_size, cfg.models.optimization.val_fraction\n",
    "misc_dict = cfg.get('misc', default_value={})\n",
    "sigma = misc_dict.get('segmentation_loss', {}).get('sigma', 0.0)\n",
    "# train_loader, val_loader, data_info, scalers = load_training_data(cfg.data, model_name, model_input_type,\n",
    "#                                                                   batch_size=batch_size,\n",
    "#                                                                   val_fraction=val_fraction,\n",
    "#                                                                   seed=seed, debug=False, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "scaling_strategy = None\n",
    "scalers = None\n",
    "test_data = False\n",
    "downsampling = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_features_list[0][0]: (69, 1024)\n",
      "human_boxes_list[0][0]: (69, 4)\n",
      "human_poses_list[0][0]: (69, 17, 2)\n",
      "\n",
      "object_features_list: (69, 1, 1024)\n",
      "object_boxes_list: (69, 1, 4)\n",
      "\n",
      "gt_list: dict_keys(['Human1']) [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "xs_steps: [69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69.\n",
      " 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69.\n",
      " 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69.\n",
      " 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69.\n",
      " 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69. 69.]\n"
     ]
    }
   ],
   "source": [
    "features_dir = Path('/root/vs-gats-plaster/deepsort/outputs/anno_test_8/features')\n",
    "\n",
    "human_features_list = []\n",
    "human_boxes_list = []\n",
    "human_poses_list = []\n",
    "object_features_list = []\n",
    "object_boxes_list = []\n",
    "gt_list = []\n",
    "xs_steps = []\n",
    "\n",
    "for feature_dir_name in os.listdir(features_dir):\n",
    "    feature_dir = features_dir / feature_dir_name\n",
    "    \n",
    "    # Load and store human (subject) features\n",
    "    subject_visual_features = np.load(feature_dir / 'subject_visual_features.npy')\n",
    "    subject_boxes = np.load(feature_dir / 'subject_boxes.npy')\n",
    "    subject_poses = np.zeros((subject_visual_features.shape[0], 17, 2))\n",
    "    \n",
    "    human_features_list.append([subject_visual_features])\n",
    "    human_boxes_list.append([subject_boxes])\n",
    "    human_poses_list.append([subject_poses])\n",
    "    \n",
    "    # Load and store object features\n",
    "    object_visual_features = np.load(feature_dir / 'object_visual_features.npy')\n",
    "    object_boxes = np.load(feature_dir / 'object_boxes.npy')\n",
    "    \n",
    "    object_features_list.append(object_visual_features[:, np.newaxis, :])\n",
    "    object_boxes_list.append(object_boxes[:, np.newaxis, :])\n",
    "    \n",
    "    # Extract and store ground-truth action label\n",
    "    action_label = int(str(feature_dir).split('_action_')[-1])\n",
    "    seq_len = subject_visual_features.shape[0]\n",
    "    gt_list.append({\n",
    "        'Human1': [action_label] * seq_len,\n",
    "    })\n",
    "    \n",
    "    # Store number of steps\n",
    "    num_steps = len(subject_visual_features[downsampling - 1::downsampling])\n",
    "    xs_steps.append(num_steps)\n",
    "\n",
    "# gt_list = [{'Human1': gt_human1}]\n",
    "xs_steps = np.array(xs_steps, dtype=np.float32)\n",
    "\n",
    "print(\"human_features_list[0][0]:\", human_features_list[0][0].shape)\n",
    "print(\"human_boxes_list[0][0]:\", human_boxes_list[0][0].shape)\n",
    "print(\"human_poses_list[0][0]:\", human_poses_list[0][0].shape)\n",
    "print()\n",
    "\n",
    "print(\"object_features_list:\", object_features_list[0].shape)\n",
    "print(\"object_boxes_list:\", object_boxes_list[0].shape)\n",
    "print()\n",
    "\n",
    "print(\"gt_list:\", gt_list[0].keys(), gt_list[0]['Human1'])\n",
    "print()\n",
    "\n",
    "print(\"xs_steps:\", xs_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ys_i.shape: (86, 69, 1)\n",
      "ys_i[0]: [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "ys_i[1]: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "ys_i[2]: [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\n",
      "\n",
      "xs_i: (86, 69, 1, 1124)\n",
      "\n",
      "ys_i.shape: (86, 69, 1)\n",
      "ys_i[0]: [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "ys_i[1]: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "ys_i[2]: [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]\n",
      "\n",
      "xs_i: (86, 69, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyrutils.itertools import run_length_encoding\n",
    "from vhoi.data_loading import segmentation_from_output_class\n",
    "import numpy as np\n",
    "\n",
    "from pyrutils.itertools import run_length_encoding\n",
    "from vhoi.data_loading import segmentation_from_output_class\n",
    "import numpy as np\n",
    "\n",
    "def assemble_mphoi_frame_level_recurrent_human(\n",
    "    human_features_list, human_poses_list, object_boxes_list, gt_list,\n",
    "    downsampling: int = 1, test_data: bool = False, max_no_objects: int = 4\n",
    "):\n",
    "    xs_h, xs_hp, x_obb = [], [], []\n",
    "    max_len, max_len_downsampled = 0, 0\n",
    "\n",
    "    if max_no_objects is None:\n",
    "        max_no_objects = max(\n",
    "            max(len(frame) for frame in video) for video in object_boxes_list\n",
    "        )\n",
    "\n",
    "    for humans, poses, objects_bounding_box in zip(human_features_list, human_poses_list, object_boxes_list):\n",
    "        num_humans = len(humans)\n",
    "        max_len = max(max_len, humans[0].shape[0])\n",
    "\n",
    "        humans_ds = [h[downsampling - 1::downsampling] for h in humans]\n",
    "        poses_ds  = [p[downsampling - 1::downsampling] / 1000 for p in poses]\n",
    "        max_len_downsampled = max(\n",
    "            max_len_downsampled,\n",
    "            max(h.shape[0] for h in humans_ds),\n",
    "            max(p.shape[0] for p in poses_ds),\n",
    "            objects_bounding_box[downsampling - 1::downsampling].shape[0],\n",
    "        )\n",
    "        xs_h.append(humans_ds)\n",
    "        xs_hp.append(poses_ds)\n",
    "\n",
    "        obb_ds = objects_bounding_box[downsampling - 1::downsampling] / 1000\n",
    "        x_obb.append(obb_ds)\n",
    "\n",
    "    xs_obb = []\n",
    "    for video in x_obb:\n",
    "        bb = []\n",
    "        for frame in video:\n",
    "            b = np.zeros((max_no_objects, 4))\n",
    "            n = min(len(frame), max_no_objects)\n",
    "            b[:n] = frame[:n]\n",
    "            b = b.reshape(max_no_objects * 2, 2)\n",
    "            bb.append(b)\n",
    "        xs_obb.append(bb)\n",
    "\n",
    "    # keypoints = [1, 2, 4, 6, 7, 11, 13, 14, 27]\n",
    "    keypoints = list(range(human_poses_list[0][0].shape[1]))\n",
    "    xs_h_with_context = []\n",
    "    for i, (humans_ds, poses_ds, obb_video) in enumerate(zip(xs_h, xs_hp, xs_obb)):\n",
    "        num_humans = len(humans_ds)\n",
    "        humans_context = [[] for _ in range(num_humans)]\n",
    "\n",
    "        for j in range(len(humans_ds[0])):\n",
    "            obb = obb_video[j]\n",
    "\n",
    "            if j + 1 < len(humans_ds[0]):\n",
    "                next_poses = [p[j+1][keypoints] for p in poses_ds]\n",
    "                pose_velos = [(next_pose - poses_ds[h][j][keypoints]) * 100 for h, next_pose in enumerate(next_poses)]\n",
    "                obb_velo = (obb_video[j+1] - obb) * 100\n",
    "            else:\n",
    "                pose_velos = [np.zeros((len(keypoints), 2)) for _ in poses_ds]\n",
    "                obb_velo = np.zeros((max_no_objects * 2, 2))\n",
    "\n",
    "            obbvelo = np.hstack((obb, obb_velo)).reshape(1, -1)\n",
    "\n",
    "            context = []\n",
    "            for h in range(num_humans):\n",
    "                pose = poses_ds[h][j][keypoints]\n",
    "                velo = pose_velos[h]\n",
    "                posevelo = np.hstack((pose, velo)).reshape(1, -1)\n",
    "                context.append(posevelo[0])\n",
    "\n",
    "            context = np.concatenate(context + [obbvelo[0]])\n",
    "\n",
    "            for h in range(num_humans):\n",
    "                h_con = np.concatenate((humans_ds[h][j], context))\n",
    "                humans_context[h].append(h_con)\n",
    "\n",
    "        xs_h_with_context.append([np.array(hc) for hc in humans_context])\n",
    "\n",
    "    feature_size = xs_h_with_context[0][0].shape[-1]\n",
    "    num_humans = len(xs_h_with_context[0])\n",
    "    x_hs = np.full([len(xs_h_with_context), max_len_downsampled, num_humans, feature_size],\n",
    "                   fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, humans in enumerate(xs_h_with_context):\n",
    "        for h, feats in enumerate(humans):\n",
    "            seq_len = min(len(feats), max_len_downsampled)\n",
    "            x_hs[m, :seq_len, h] = feats[:seq_len]\n",
    "\n",
    "    xs = [x_hs]\n",
    "\n",
    "    # ----------------------\n",
    "    # Outputs\n",
    "    # ----------------------\n",
    "    y_rec_hs = np.full([len(x_hs), max_len_downsampled, num_humans], fill_value=-1, dtype=np.int64)\n",
    "    y_pred_hs = np.full_like(y_rec_hs, fill_value=-1)\n",
    "    \n",
    "    for m, video_hands_ground_truth in enumerate(gt_list):\n",
    "        for h in range(num_humans):\n",
    "            human_key = f\"Human{h+1}\"\n",
    "            if human_key not in video_hands_ground_truth:\n",
    "                continue\n",
    "\n",
    "            y_h = video_hands_ground_truth[human_key]\n",
    "\n",
    "            # Ground truth (downsampled)\n",
    "            y_h_ds = y_h[downsampling - 1::downsampling]\n",
    "            seq_len = min(len(y_h_ds), y_rec_hs.shape[1])\n",
    "            y_rec_hs[m, :seq_len, h] = y_h_ds[:seq_len]\n",
    "\n",
    "            # Prediction: shift labels forward\n",
    "            # y_h_p = np.roll(y_h, -1)\n",
    "            # y_h_p[-1] = -1  # last frame has no \"next\"\n",
    "            # y_h_p_ds = y_h_p[downsampling - 1::downsampling]\n",
    "            # seq_len_p = min(len(y_h_p_ds), y_pred_hs.shape[1])\n",
    "            # y_pred_hs[m, :seq_len_p, h] = y_h_p_ds[:seq_len_p]\n",
    "            y_pred_hs[m, :seq_len, h] = y_h_ds[:seq_len]\n",
    "            \n",
    "    x_hs_segmentation = segmentation_from_output_class(y_rec_hs, segmentation_type=\"input\")\n",
    "    xs.append(x_hs_segmentation)\n",
    "\n",
    "    y_hs_segmentation = segmentation_from_output_class(y_rec_hs, segmentation_type=\"output\")\n",
    "    ys = [y_rec_hs, y_pred_hs, y_hs_segmentation]\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = assemble_mphoi_frame_level_recurrent_human(human_features_list, human_poses_list, object_boxes_list, gt_list)\n",
    "\n",
    "for ys_i, xs_i in zip(ys, xs):\n",
    "    print(\"ys_i.shape:\", ys_i.shape)\n",
    "    print(\"ys_i[0]:\", ys_i[0].flatten())\n",
    "    print(\"ys_i[1]:\", ys_i[1].flatten())\n",
    "    print(\"ys_i[2]:\", ys_i[2].flatten())\n",
    "    print()\n",
    "    print(\"xs_i:\", xs_i.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_frame_level_recurrent_objects(object_features_list, downsampling: int = 1):\n",
    "    xs_objects = []\n",
    "    max_len, max_len_downsampled, max_num_objects = 0, 0, 0\n",
    "    for objects in object_features_list:\n",
    "        max_len = max(max_len, objects.shape[0])\n",
    "        max_num_objects = max(max_num_objects, objects.shape[1])\n",
    "        objects = objects[downsampling - 1::downsampling]\n",
    "        max_len_downsampled = max(max_len_downsampled, objects.shape[0])\n",
    "        xs_objects.append(objects)\n",
    "    feature_size = xs_objects[-1].shape[-1]\n",
    "    x_objects = np.full([len(xs_objects), max_len_downsampled, max_num_objects, feature_size],\n",
    "                        fill_value=np.nan, dtype=np.float32)\n",
    "    x_objects_mask = np.zeros([len(xs_objects), max_num_objects], dtype=np.float32)\n",
    "    for m, x_o in enumerate(xs_objects):\n",
    "        x_objects[m, :x_o.shape[0], :x_o.shape[1], :] = x_o\n",
    "        x_objects_mask[m, :x_o.shape[1]] = 1.0\n",
    "    xs = [x_objects, x_objects_mask]\n",
    "    return xs\n",
    "\n",
    "xs_objects = assemble_mphoi_frame_level_recurrent_objects(object_features_list)\n",
    "for xs_i in xs_objects:\n",
    "    print(xs_i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from vhoi.data_loading import compute_centroid\n",
    "\n",
    "def assemble_mphoi_human_human_distances(human_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute pairwise human-human distances for multiple humans across videos.\n",
    "\n",
    "    Args:\n",
    "        human_boxes_list: list of list of human bounding boxes per video\n",
    "                          (outer list: videos, inner list: humans, array: frames x 4)\n",
    "        downsampling:     frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_hh_dists: tensor [num_videos, max_len, N, N] with pairwise distances\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_humans = 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for video_bbs in human_boxes_list:\n",
    "        num_humans = len(video_bbs)\n",
    "        max_num_humans = max(max_num_humans, num_humans)\n",
    "\n",
    "        # Downsample and compute centroids\n",
    "        centroids = []\n",
    "        for bb in video_bbs:\n",
    "            bb = bb[downsampling - 1::downsampling]\n",
    "            c = compute_centroid(bb) / mphoi_dims\n",
    "            centroids.append(c)\n",
    "\n",
    "        # Length of this video (frames)\n",
    "        max_len = max(max_len, centroids[0].shape[0])\n",
    "\n",
    "        # Compute pairwise distances (frames x N x N)\n",
    "        T = centroids[0].shape[0]\n",
    "        dists_matrix = np.zeros((T, num_humans, num_humans), dtype=np.float32)\n",
    "\n",
    "        for i, j in itertools.combinations(range(num_humans), 2):\n",
    "            d = np.linalg.norm(centroids[i] - centroids[j], ord=2, axis=-1)\n",
    "            dists_matrix[:, i, j] = d\n",
    "            dists_matrix[:, j, i] = d\n",
    "\n",
    "        all_dists.append(dists_matrix)\n",
    "\n",
    "    # Pad into a tensor [num_videos, max_len, max_num_humans, max_num_humans]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_humans, max_num_humans]\n",
    "    x_hh_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists_matrix in enumerate(all_dists):\n",
    "        T, N, _ = dists_matrix.shape\n",
    "        x_hh_dists[m, :T, :N, :N] = dists_matrix\n",
    "\n",
    "    return x_hh_dists\n",
    "\n",
    "\n",
    "xs_hh_dists = assemble_mphoi_human_human_distances(human_boxes_list)\n",
    "print(xs_hh_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_mphoi_human_object_distances(human_boxes_list, object_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute human-object distances for multiple humans and objects across videos.\n",
    "\n",
    "    Args:\n",
    "        human_boxes_list:  list of list of human bounding boxes per video\n",
    "                           (outer list: videos, inner list: humans, array: frames x 4)\n",
    "        object_boxes_list: list of object bounding box arrays per video (frames x num_objects x 4)\n",
    "        downsampling:      frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_ho_dists: tensor [num_videos, max_len, max_num_humans, max_num_objects]\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_humans, max_num_objects = 0, 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for video_bbs, obj_bbs in zip(human_boxes_list, object_boxes_list):\n",
    "        num_humans = len(video_bbs)\n",
    "\n",
    "        # Downsample humans → centroids\n",
    "        human_centroids = []\n",
    "        for bb in video_bbs:\n",
    "            bb = bb[downsampling - 1::downsampling]\n",
    "            c = compute_centroid(bb) / mphoi_dims\n",
    "            human_centroids.append(c)\n",
    "\n",
    "        # Downsample objects → centroids\n",
    "        obj_bbs = obj_bbs[downsampling - 1::downsampling]\n",
    "        obj_centroids = compute_centroid(obj_bbs) / mphoi_dims\n",
    "\n",
    "        T = obj_centroids.shape[0]\n",
    "        max_len = max(max_len, T)\n",
    "        max_num_humans = max(max_num_humans, num_humans)\n",
    "        max_num_objects = max(max_num_objects, obj_centroids.shape[1])\n",
    "\n",
    "        # Compute distances [frames, num_humans, num_objects]\n",
    "        dists_matrix = np.zeros((T, num_humans, obj_centroids.shape[1]), dtype=np.float32)\n",
    "        for h, h_c in enumerate(human_centroids):\n",
    "            d = np.linalg.norm(obj_centroids - np.expand_dims(h_c, axis=1), ord=2, axis=-1)\n",
    "            dists_matrix[:, h, :] = d\n",
    "\n",
    "        all_dists.append(dists_matrix)\n",
    "\n",
    "    # Pad into a tensor [num_videos, max_len, max_num_humans, max_num_objects]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_humans, max_num_objects]\n",
    "    x_ho_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists_matrix in enumerate(all_dists):\n",
    "        T, H, O = dists_matrix.shape\n",
    "        x_ho_dists[m, :T, :H, :O] = dists_matrix\n",
    "\n",
    "    return x_ho_dists\n",
    "\n",
    "xs_ho_dists = assemble_mphoi_human_object_distances(human_boxes_list, object_boxes_list)\n",
    "print(xs_ho_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def assemble_mphoi_object_object_distances(object_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute pairwise object-object distances across videos.\n",
    "\n",
    "    Args:\n",
    "        object_boxes_list: list of object bounding box arrays per video (frames x num_objects x 4)\n",
    "        downsampling:      frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_oo_dists: tensor [num_videos, max_len, max_num_objects, max_num_objects]\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_objects = 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for obj_bbs in object_boxes_list:\n",
    "        # Downsample and compute centroids\n",
    "        obj_bbs = obj_bbs[downsampling - 1::downsampling]\n",
    "        objs_centroid = compute_centroid(obj_bbs) / mphoi_dims   # (frames, num_objects, 2)\n",
    "        num_objects = objs_centroid.shape[1]\n",
    "\n",
    "        # Compute pairwise distances per frame\n",
    "        dists = []\n",
    "        for k in range(num_objects):\n",
    "            kth_object_centroid = objs_centroid[:, k:k+1]  # (frames, 1, 2)\n",
    "            kth_dist = np.linalg.norm(objs_centroid - kth_object_centroid, ord=2, axis=-1)  # (frames, num_objects)\n",
    "            dists.append(kth_dist)\n",
    "\n",
    "        dists = np.stack(dists, axis=1)  # (frames, num_objects, num_objects)\n",
    "        all_dists.append(dists)\n",
    "\n",
    "        max_len = max(max_len, obj_bbs.shape[0])\n",
    "        max_num_objects = max(max_num_objects, num_objects)\n",
    "\n",
    "    # Pad into tensor [num_videos, max_len, max_num_objects, max_num_objects]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_objects, max_num_objects]\n",
    "    x_oo_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists in enumerate(all_dists):\n",
    "        T, O1, O2 = dists.shape\n",
    "        x_oo_dists[m, :T, :O1, :O2] = dists\n",
    "\n",
    "    return x_oo_dists\n",
    "\n",
    "xs_oo_dists = assemble_mphoi_object_object_distances(object_boxes_list)\n",
    "print(xs_oo_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create_data_loader(...)\n",
    "from vhoi.data_loading import (\n",
    "    # assemble_mphoi_tensors, \n",
    "    assemble_mphoi_frame_level_recurrent_objects, \n",
    "    ignore_last_step_end_flag_general,\n",
    "    smooth_segmentation,\n",
    "    assemble_mphoi_human_human_distances,\n",
    "    assemble_mphoi_human_object_distances,\n",
    "    assemble_mphoi_object_object_distances,\n",
    "    assemble_num_steps,\n",
    ")\n",
    "\n",
    "# data = training_data\n",
    "\n",
    "# if dataset_name.lower() == 'cad120':\n",
    "#     x, y = assemble_tensors(data, model_name, model_input_type, sigma=sigma, downsampling=downsampling,\n",
    "#                             test_data=test_data)\n",
    "# elif dataset_name.lower() == 'mphoi':\n",
    "# x, y = assemble_mphoi_tensors(data=training_data, model_name=model_name, sigma=sigma, downsampling=downsampling, test_data=test_data)\n",
    "\n",
    "### assemble_mphoi_tensors(...)\n",
    "# from vhoi.data_loading import assemble_mphoi_frame_level_recurrent_human\n",
    "# xs, ys = assemble_mphoi_frame_level_recurrent_human(data, downsampling=downsampling, test_data=test_data)\n",
    "\n",
    "# xs (assemble_mphoi_frame_level_recurrent_human) info:\n",
    "# 0: x_human            => [8, 154, 2, 2152]\n",
    "# 1: human_segmentation => [8, 154, 2]\n",
    "\n",
    "# xs_objects = assemble_mphoi_frame_level_recurrent_objects(data, downsampling=downsampling)\n",
    "# if model_name == '2G-GCN':\n",
    "if sigma:\n",
    "    ys[2] = ignore_last_step_end_flag_general(ys[2])\n",
    "ys[2] = smooth_segmentation(ys[2], sigma)\n",
    "ys_budget = ys[2]\n",
    "# xs_hh_dists = assemble_mphoi_human_human_distances(data, downsampling=downsampling)\n",
    "# xs_ho_dists = assemble_mphoi_human_object_distances(data, downsampling=downsampling)\n",
    "# xs_oo_dists = assemble_mphoi_object_object_distances(data, downsampling=downsampling)\n",
    "# xs_steps = assemble_num_steps(data, downsampling=downsampling)\n",
    "\n",
    "# print(\"xs:\")\n",
    "# for xsi in xs:\n",
    "#     print(xsi.shape)\n",
    "# print()\n",
    "\n",
    "# xs info:\n",
    "# 0: x_human                    => [8, 154, 2, 2152]\n",
    "# 1: x_objects                  => [8, 154, 4, 2048]\n",
    "# 2: objects_mask               => [8, 4]\n",
    "# 3: human_segmentation         => [8, 154, 2]\n",
    "# 4: human_human_distances      => [8, 154, 2, 2]\n",
    "# 5: human_object_distances     => [8, 154, 2, 4]\n",
    "# 6: object_object_distances    => [8, 154, 4, 4]\n",
    "# 7: steps_per_example          => [8]\n",
    "# see ./vhoi/models.py, line 612, for more info\n",
    "\n",
    "#    0,       1, 2,        3,        4,           5,           6,           7\n",
    "xs = xs[:1] + xs_objects + xs[1:] + [xs_hh_dists, xs_ho_dists, xs_oo_dists, xs_steps]\n",
    "ys = [ys_budget] + ys[2:] + ys[:2]\n",
    "ys += ys[-2:]\n",
    "x, y = xs, ys\n",
    "\n",
    "# print(\"x:\")\n",
    "# for xi in x:\n",
    "#     print(xi.shape)\n",
    "# print()\n",
    "\n",
    "# else:\n",
    "#     x, y = assemble_bimanual_tensors(data, model_name, sigma=sigma, downsampling=downsampling, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from vhoi.data_loading import maybe_scale_input_tensors\n",
    "from pyrutils.torch.train_utils import numpy_to_torch\n",
    "\n",
    "x, y = xs, ys\n",
    "\n",
    "x, scalers = maybe_scale_input_tensors(x, model_name, scaling_strategy=scaling_strategy, scalers=scalers)\n",
    "x = [np.nan_to_num(ix, copy=False, nan=0.0) for ix in x]\n",
    "x, y = numpy_to_torch(*x), numpy_to_torch(*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_i, y_i in zip(x, y):\n",
    "    print(\"x_i.shape:\", x_i.shape, x_i.dtype, x_i.max().item(), x_i.min().item())\n",
    "    print(\"y_i.shape:\", y_i.shape, y_i.dtype, y_i.max().item(), y_i.min().item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(*(x + y))\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0,\n",
    "                            pin_memory=False, drop_last=False)\n",
    "# segmentations = assemble_segmentations(data, model_name, dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in data_loader:\n",
    "    for idx, i in enumerate(b):\n",
    "        if idx > 7: \n",
    "            print(i.shape, i.dtype, i.max().item(), i.min().item())\n",
    "            print(i[0].flatten())\n",
    "            print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vhoi.data_loading import input_size_from_data_loader\n",
    "\n",
    "train_loader = data_loader\n",
    "\n",
    "input_size = input_size_from_data_loader(train_loader, model_name, model_input_type)\n",
    "data_info = {'input_size': input_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "### Model = select_model(model_name)\n",
    "model_creation_args = cfg.models.parameters\n",
    "### model_creation_args = {**data_info, **model_creation_args}\n",
    "model_creation_args = {**data_info, **model_creation_args.__dict__}\n",
    "dataset_name = cfg.data.name\n",
    "# num_classes = determine_num_classes(model_name, model_input_type, dataset_name)\n",
    "device = 'cuda' if torch.cuda.is_available() and cfg.resources.use_gpu else 'cpu'\n",
    "\n",
    "model_creation_args['num_classes'] = (18, None)\n",
    "\n",
    "# Disable geometry features, as they do not support more than 2 humans\n",
    "model_creation_args['message_geometry_to_objects'] = True\n",
    "model_creation_args['message_geometry_to_human'] = True\n",
    "\n",
    "# Model configuration if object is only one\n",
    "model_creation_args['message_objects_to_object'] = False\n",
    "model_creation_args['gcn_node'] = 25\n",
    "\n",
    "# Model configuration if human is only one\n",
    "model_creation_args['message_humans_to_human'] = False\n",
    "\n",
    "model = TGGCN_Custom(feat_dim=1024, **model_creation_args).to(device)\n",
    "if misc_dict.get('pretrained', False) and misc_dict.get('pretrained_path') is not None:\n",
    "    state_dict = load_model_weights(misc_dict['pretrained_path'])\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=cfg.models.optimization.learning_rate)\n",
    "criterion, loss_names = select_loss(model_name, model_input_type, dataset_name, cfg=cfg)\n",
    "mtll_model = None\n",
    "if misc_dict.get('multi_task_loss_learner', False):\n",
    "    loss_types = select_loss_types(model_name, dataset_name, cfg=cfg)\n",
    "    mask = select_loss_learning_mask(model_name, dataset_name, cfg=cfg)\n",
    "    mtll_model = MultiTaskLossLearner(loss_types=loss_types, mask=mask).to(device)\n",
    "    optimizer.add_param_group({'params': mtll_model.parameters()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = cfg.models.logging.root_log_dir\n",
    "checkpoint_name = cfg.models.logging.checkpoint_name\n",
    "fetch_model_data = select_model_data_fetcher(model_name, model_input_type,\n",
    "                                             dataset_name=dataset_name, **{**misc_dict, **cfg.models.parameters.__dict__})\n",
    "feed_model_data = select_model_data_feeder(model_name, model_input_type, dataset_name=dataset_name, **misc_dict)\n",
    "num_main_losses = decide_num_main_losses(model_name, dataset_name, {**misc_dict, **cfg.models.parameters.__dict__})\n",
    "# checkpoint = train(model, train_loader, optimizer, criterion, cfg.optimization.epochs, device, loss_names,\n",
    "#                    clip_gradient_at=cfg.optimization.clip_gradient_at,\n",
    "#                    fetch_model_data=fetch_model_data, feed_model_data=feed_model_data,\n",
    "#                    val_loader=val_loader, mtll_model=mtll_model, num_main_losses=num_main_losses,\n",
    "#                    tensorboard_log_dir=tensorboard_log_dir, checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_name = kwargs.get('checkpoint_name', None)\n",
    "# tensorboard_log_dir = kwargs.get('tensorboard_log_dir', None)\n",
    "# writer = None\n",
    "# if tensorboard_log_dir is not None and checkpoint_name is not None:\n",
    "#     writer = SummaryWriter(os.path.join(tensorboard_log_dir, 'runs', checkpoint_name))\n",
    "checkpoint = {}\n",
    "train_losses, val_losses, train_raw_losses, val_raw_losses = [], [], [], []\n",
    "val_loss = float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [   1/  40]\n",
      "torch.Size([8, 69, 1, 1124])\n",
      "torch.Size([8, 69, 1, 1024])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8, 69, 1])\n",
      "torch.Size([8, 69, 1, 1])\n",
      "torch.Size([8, 69, 1, 1])\n",
      "torch.Size([8, 69, 1, 1])\n",
      "torch.Size([8])\n",
      "\n",
      "torch.Size([8, 69, 1])\n",
      "torch.Size([8, 69, 1])\n",
      "torch.Size([8, 69, 1])\n",
      "torch.Size([8, 69, 1])\n",
      "torch.Size([8, 69, 1])\n",
      "torch.Size([8, 69, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Train) Batch [     8/    86 (  9%)]  Loss:   4.3514  B_HS:  0.0000  BCE_HS:  0.0000  NLL_SAR_F:  0.0000  NLL_SAP_F:  0.0000  NLL_SAR:  2.1457  NLL_SAP:  2.2057\n"
     ]
    }
   ],
   "source": [
    "data_loader = train_loader\n",
    "clip_gradient_at=False\n",
    "log_interval=25\n",
    "\n",
    "initial_epoch = 1\n",
    "epochs = 40\n",
    "\n",
    "for epoch in range(initial_epoch, epochs + initial_epoch):\n",
    "    print(f'\\nEpoch: [{epoch:4d}/{epochs + initial_epoch - 1:4d}]')\n",
    "\n",
    "    # train_single_epoch()\n",
    "    model.train()\n",
    "    if mtll_model is not None:\n",
    "        mtll_model.train()\n",
    "    num_examples = len(data_loader.dataset)\n",
    "    for batch_idx, dataset in enumerate(data_loader):\n",
    "        data, target = fetch_model_data(dataset, device=device)\n",
    "        \n",
    "        # data\n",
    "        # torch.Size([8, 69, 1, 1124])  -> subject visual features\n",
    "        # torch.Size([8, 69, 1, 1024])  -> object visual features\n",
    "        # torch.Size([8, 1])            -> \n",
    "        # torch.Size([8, 69, 1])\n",
    "        # torch.Size([8, 69, 1, 1])\n",
    "        # torch.Size([8, 69, 1, 1])\n",
    "        # torch.Size([8, 69, 1, 1])\n",
    "        # torch.Size([8])\n",
    "        \n",
    "        \n",
    "        for d in data:\n",
    "            print(d.shape)\n",
    "        print()\n",
    "        for t in target:\n",
    "            print(t.shape)\n",
    "        \n",
    "        skip = False\n",
    "        for t in target:\n",
    "            if t.max().item() == t.min().item():\n",
    "                # print(\"Skipped.\")\n",
    "                # DEBUG\n",
    "                # print(t.shape, t.dtype, t.max().item(), t.min().item())\n",
    "                \n",
    "                skip = True\n",
    "                break\n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = feed_model_data(model, data)\n",
    "        \n",
    "        # # DEBUG\n",
    "        # for o in output:\n",
    "        #     print(o.shape, o.dtype, o.max().item(), o.min().item())\n",
    "        # print()\n",
    "        \n",
    "        losses = criterion(output, target, reduction='mean')\n",
    "        \n",
    "        # # DEBUG\n",
    "        # for l in losses:\n",
    "        #     print(l)\n",
    "        # print()\n",
    "        \n",
    "        if mtll_model is not None:\n",
    "            losses = mtll_model(losses)\n",
    "        loss = sum(losses)\n",
    "        loss.backward()\n",
    "        if clip_gradient_at:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_gradient_at)\n",
    "        optimizer.step()\n",
    "        log_now, is_last_batch = (batch_idx % log_interval) == 0, batch_idx == (len(data_loader) - 1)\n",
    "        if log_now or is_last_batch:\n",
    "            num_main_losses = num_main_losses if num_main_losses is not None else len(losses)\n",
    "            loss = sum(losses[-num_main_losses:])\n",
    "            batch_initial_example_idx = min((batch_idx + 1) * data_loader.batch_size, num_examples)\n",
    "            epoch_progress = 100 * (batch_idx + 1) / len(data_loader)\n",
    "            print(f'(Train) Batch [{batch_initial_example_idx:6d}/{num_examples:6d} ({epoch_progress:3.0f}%)] ',\n",
    "                    f'Loss: {loss.item(): 8.4f}', end='')\n",
    "            for loss_name, single_loss in zip(loss_names, losses):\n",
    "                print(f'  {loss_name}: {single_loss: 6.4f}', end='')\n",
    "            print()\n",
    "            # print()\n",
    "        \n",
    "        # Test for only single batch\n",
    "        break\n",
    "    \n",
    "    # Test for only single epoch\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch271",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
