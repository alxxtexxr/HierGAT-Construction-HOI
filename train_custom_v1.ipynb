{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: `py39_torch271`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "\n",
    "from pyrutils.torch.train_utils import train, save_checkpoint\n",
    "from pyrutils.torch.multi_task import MultiTaskLossLearner\n",
    "from vhoi.data_loading import load_training_data, select_model_data_feeder, select_model_data_fetcher\n",
    "from vhoi.data_loading import determine_num_classes\n",
    "from vhoi.losses import select_loss, decide_num_main_losses, select_loss_types, select_loss_learning_mask\n",
    "from vhoi.models import select_model, load_model_weights\n",
    "from vhoi.models_custom import TGGCN_Custom\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from dacite import from_dict\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)   # Python的随机性\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "np.random.seed(seed)   # numpy的随机性\n",
    "torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True   # 选择确定性算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "class DictMixin:\n",
    "    def get(self, key, default_value=None):\n",
    "        return getattr(self, key, default_value)\n",
    "\n",
    "    def as_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "@dataclass\n",
    "class Resources(DictMixin):\n",
    "    use_gpu: bool\n",
    "    num_threads: int\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata(DictMixin):\n",
    "    model_name: str\n",
    "    input_type: str\n",
    "\n",
    "@dataclass\n",
    "class ModelParameters(DictMixin):\n",
    "    add_segment_length: int\n",
    "    add_time_position: int\n",
    "    time_position_strategy: str\n",
    "    positional_encoding_style: str\n",
    "    attention_style: str\n",
    "    bias: bool\n",
    "    cat_level_states: int\n",
    "    discrete_networks_num_layers: int\n",
    "    discrete_optimization_strategy: str\n",
    "    filter_discrete_updates: bool\n",
    "    gcn_node: int\n",
    "    hidden_size: int\n",
    "    message_humans_to_human: bool\n",
    "    message_human_to_objects: bool\n",
    "    message_objects_to_human: bool\n",
    "    message_objects_to_object: bool\n",
    "    message_geometry_to_objects: bool\n",
    "    message_geometry_to_human: bool\n",
    "    message_segment: bool\n",
    "    message_type: str\n",
    "    message_granularity: str\n",
    "    message_aggregation: str\n",
    "    object_segment_update_strategy: str\n",
    "    share_level_mlps: int\n",
    "    update_segment_threshold: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOptimization(DictMixin):\n",
    "    batch_size: int\n",
    "    clip_gradient_at: float\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    val_fraction: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BudgetLoss(DictMixin):\n",
    "    add: bool\n",
    "    human_weight: float\n",
    "    object_weight: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegmentationLoss(DictMixin):\n",
    "    add: bool\n",
    "    pretrain: bool\n",
    "    sigma: float\n",
    "    weight: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelMisc(DictMixin):\n",
    "    anticipation_loss_weight: float\n",
    "    budget_loss: BudgetLoss\n",
    "    first_level_loss_weight: float\n",
    "    impose_segmentation_pattern: int\n",
    "    input_human_segmentation: bool\n",
    "    input_object_segmentation: bool\n",
    "    make_attention_distance_based: bool\n",
    "    multi_task_loss_learner: bool\n",
    "    pretrained: bool\n",
    "    pretrained_path: Optional[str]\n",
    "    segmentation_loss: SegmentationLoss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelLogging(DictMixin):\n",
    "    root_log_dir: str\n",
    "    checkpoint_name: str\n",
    "    log_dir: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Models(DictMixin):\n",
    "    metadata: ModelMetadata\n",
    "    parameters: ModelParameters\n",
    "    optimization: ModelOptimization\n",
    "    misc: ModelMisc\n",
    "    logging: ModelLogging\n",
    "\n",
    "@dataclass\n",
    "class Data(DictMixin):\n",
    "    name: str\n",
    "    path: str\n",
    "    path_zarr: str\n",
    "    path_obb_zarr: str\n",
    "    path_hbb_zarr: str\n",
    "    path_hps_zarr: str\n",
    "    cross_validation_test_subject: str\n",
    "    scaling_strategy: Optional[str]\n",
    "    downsampling: int\n",
    "\n",
    "@dataclass\n",
    "class Config(DictMixin):\n",
    "    resources: Resources\n",
    "    models: Models\n",
    "    data: Data\n",
    "\n",
    "cfg_dict = {\n",
    "    \"resources\": {\n",
    "        \"use_gpu\": True,\n",
    "        \"num_threads\": 32\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"metadata\": {\n",
    "            \"model_name\": \"2G-GCN\",\n",
    "            \"input_type\": \"multiple\"\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"add_segment_length\": 0,  # length of the segment to the segment-level rnn. 0 is off and 1 is on.\n",
    "            \"add_time_position\": 0,  # absolute time position to the segment-level rnn. 0 is off and 1 is on.\n",
    "            \"time_position_strategy\": \"s\",  # input time position to segment [s] or discrete update [u].\n",
    "            \"positional_encoding_style\": \"e\",  # e [embedding] or p [periodic].\n",
    "            \"attention_style\": \"v3\",  # v1 [concat], v2 [dot-product], v3 [scaled_dot-product], v4 [general]\n",
    "            \"bias\": True,\n",
    "            \"cat_level_states\": 0,  # concatenate first and second level hidden states for predictors MLPs.\n",
    "            \"discrete_networks_num_layers\": 1,  # depth of the state change detector MLP.\n",
    "            \"discrete_optimization_strategy\": \"gs\",  # straight-through [st] or gumbel-sigmoid [gs]\n",
    "            \"filter_discrete_updates\": False,  # maxima filter for soft output of state change detector.\n",
    "            \"gcn_node\": 26,  # 19 for cad120, 30 for bimanual, 26 for mphoi\n",
    "            \"hidden_size\": 512,  # 512 for cad120 & mphoi; 64 for bimanual\n",
    "            \"message_humans_to_human\": True,  # only meaningful for bimanual and mphoi\n",
    "            \"message_human_to_objects\": True,\n",
    "            \"message_objects_to_human\": True,\n",
    "            \"message_objects_to_object\": True,\n",
    "            \"message_geometry_to_objects\": True,\n",
    "            \"message_geometry_to_human\": False,  # false in original, note this!\n",
    "            \"message_segment\": True,\n",
    "            \"message_type\": \"v2\",  # v1 [relational] or v2 [non-relational]\n",
    "            \"message_granularity\": \"v1\",  # v1 [generic] or v2 [specific]\n",
    "            \"message_aggregation\": \"att\",  # mean_pooling [mp] or attention [att]\n",
    "            \"object_segment_update_strategy\": \"ind\",  # same_as_human [sah], independent [ind], or conditional_on_human [coh]\n",
    "            \"share_level_mlps\": 0,  # whether to share [1] or not [0] the prediction MLPs of the levels.\n",
    "            \"update_segment_threshold\": 0.5  # [0.0, 1.0)\n",
    "        },\n",
    "        \"optimization\": {\n",
    "        \"batch_size\": 8,  # mphoi:8; cad120:16; bimanual: 32\n",
    "        \"clip_gradient_at\": 0.0,\n",
    "        \"epochs\": 40,  # cad120 & mphoi:40; bimanual: 60\n",
    "        \"learning_rate\": 1e-4,  # mphoi:1e-4; cad120 & bimanual:1e-3\n",
    "        \"val_fraction\": 0.1\n",
    "    },\n",
    "    \"misc\": {\n",
    "        \"anticipation_loss_weight\": 1.0,\n",
    "        \"budget_loss\": {\n",
    "            \"add\": False,\n",
    "            \"human_weight\": 1.0,\n",
    "            \"object_weight\": 1.0\n",
    "        },\n",
    "        \"first_level_loss_weight\": 0.0,  # if positive, first level does frame-level prediction\n",
    "        \"impose_segmentation_pattern\": 1,  # 0 [no pattern], 1 [all ones]\n",
    "        \"input_human_segmentation\": False,  # (was \"flase\" in YAML, corrected here)\n",
    "        \"input_object_segmentation\": False,\n",
    "        \"make_attention_distance_based\": True,  # only meaningful if message_aggregation is attention\n",
    "        \"multi_task_loss_learner\": False,\n",
    "        \"pretrained\": False,  # unfortunately need two entries for checkpoint name\n",
    "        \"pretrained_path\": None,  # specified parameters must match pre-trained model\n",
    "        \"segmentation_loss\": {\n",
    "            \"add\": False,\n",
    "            \"pretrain\": False,\n",
    "            \"sigma\": 0.0,  # Gaussian smoothing\n",
    "            \"weight\": 1.0\n",
    "        }\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"root_log_dir\": \"${oc.env:PWD}/outputs_hiergat/${data.name}/${models.metadata.model_name}\",\n",
    "        \"checkpoint_name\": (\n",
    "            \"hs${models.parameters.hidden_size}_e${models.optimization.epochs}_bs${models.optimization.batch_size}_\"\n",
    "            \"lr${models.optimization.learning_rate}_${models.parameters.update_segment_threshold}_${data.cross_validation_test_subject}\"\n",
    "        ),\n",
    "        \"log_dir\": \"${models.logging.root_log_dir}/${models.logging.checkpoint_name}\"\n",
    "    },\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"name\": \"mphoi\",\n",
    "        \"path\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_ground_truth_labels.json\",\n",
    "        \"path_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/faster_rcnn.zarr\",\n",
    "        \"path_obb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/object_bounding_boxes.zarr\",\n",
    "        \"path_hbb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_bounding_boxes.zarr\",\n",
    "        \"path_hps_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_pose.zarr\",\n",
    "        \"cross_validation_test_subject\": \"Subject14\",  # Subject45, Subject25, Subject14\n",
    "        \"scaling_strategy\": None,  # null or \"standard\"\n",
    "        \"downsampling\": 3  # 1 = full FPS, 2 = half FPS, ...\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = from_dict(data_class=Config, data=cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.set_num_threads(cfg.resources.num_threads)\n",
    "# Data\n",
    "model_name, model_input_type = cfg.models.metadata.model_name, cfg.models.metadata.input_type\n",
    "batch_size, val_fraction = cfg.models.optimization.batch_size, cfg.models.optimization.val_fraction\n",
    "misc_dict = cfg.get('misc', default_value={})\n",
    "sigma = misc_dict.get('segmentation_loss', {}).get('sigma', 0.0)\n",
    "# train_loader, val_loader, data_info, scalers = load_training_data(cfg.data, model_name, model_input_type,\n",
    "#                                                                   batch_size=batch_size,\n",
    "#                                                                   val_fraction=val_fraction,\n",
    "#                                                                   seed=seed, debug=False, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_mphoi_training_data(...)\n",
    "import json\n",
    "\n",
    "import zarr\n",
    "\n",
    "from vhoi.data_loading import create_data_loader\n",
    "\n",
    "test_subject_id = cfg.data.cross_validation_test_subject\n",
    "\n",
    "with open(cfg.data.path, mode='rb') as f:\n",
    "        data = json.load(f)\n",
    "root = zarr.open(cfg.data.path_zarr, mode='r')\n",
    "root_obbs = zarr.open(cfg.data.path_obb_zarr, mode='r')\n",
    "root_hbbs = zarr.open(cfg.data.path_hbb_zarr, mode='r')\n",
    "root_hps = zarr.open(cfg.data.path_hps_zarr, mode='r')\n",
    "training_data = []\n",
    "for video_id, human_ground_truth in data.items():\n",
    "    subject_id, task, take = video_id.split(sep='-')\n",
    "    first_sub, second_sub = int(subject_id[-2]), int(subject_id[-1])\n",
    "    first_test_sub, second_test_sub = int(test_subject_id[-2]), int(test_subject_id[-1])\n",
    "    if (first_sub-first_test_sub)*(second_sub-second_test_sub)*(first_sub-second_test_sub)*(second_sub-first_test_sub) == 0:\n",
    "        continue\n",
    "    Human1_features = root[video_id]['Human1'][:]   # 0: human 1 features       => [391, 2048]\n",
    "    Human2_features = root[video_id]['Human2'][:]   # 1: human 2 features       => [391, 2048]\n",
    "    object_features = root[video_id]['objects'][:]  # 2: object features        => [391, 4, 2048]\n",
    "                                                    # 3: human ground truth     => {'Human1': list[391], 'Human2': list[391]}\n",
    "    Human1_bbs = root_hbbs[video_id]['Human1'][:]   # 4: human 1 bounding boxes => [391, 4]\n",
    "    Human2_bbs = root_hbbs[video_id]['Human2'][:]   # 5: human 2 bounding boxes => [391, 4]\n",
    "    objects_bbs = root_obbs[video_id]['objects'][:] # 6: object bounding boxes  => [391, 4, 4]\n",
    "    Human1_hps = root_hps[video_id]['Human1'][:]    # 7: human 1 poses          => [391, 32, 2]\n",
    "    Human2_hps = root_hps[video_id]['Human2'][:]    # 8: human 2 poses          => [391, 32, 2]\n",
    "    training_data.append([Human1_features, Human2_features, object_features, human_ground_truth,\n",
    "                            Human1_bbs, Human2_bbs, objects_bbs, Human1_hps, Human2_hps])\n",
    "# train_loader, scalers, _ = create_data_loader(training_data, model_name, model_input_type, 'mphoi',\n",
    "#                                                   batch_size=batch_size, shuffle=True,\n",
    "#                                                   scaling_strategy=None, sigma=sigma,\n",
    "#                                                   downsampling=1, test_data=False)\n",
    "# val_loader, _, _ = create_data_loader(val_data, model_name, model_input_type, 'mphoi', batch_size=len(val_data),\n",
    "#                                         shuffle=False, scalers=scalers, sigma=sigma, downsampling=downsampling,\n",
    "#                                         test_data=False)\n",
    "\n",
    "# training_data info:\n",
    "# 0: human 1 features       => [391, 2048]\n",
    "# 1: human 2 features       => [391, 2048]\n",
    "# 2: object features        => [391, 4, 2048]\n",
    "# 3: human ground truth     => {'Human1': list[391], 'Human2': list[391]}\n",
    "# 4: human 1 bounding boxes => [391, 4]\n",
    "# 5: human 2 bounding boxes => [391, 4]\n",
    "# 6: object bounding boxes  => [391, 4, 4]\n",
    "# 7: human 1 poses          => [391, 32, 2]\n",
    "# 8: human 2 poses          => [391, 32, 2]\n",
    "\n",
    "# print(\"training_data:\")\n",
    "# for d in training_data:\n",
    "#     for di in d:\n",
    "#         if isinstance(di, np.ndarray):\n",
    "#             print(di.shape)\n",
    "#         elif isinstance(di, dict):\n",
    "#             for diik, diiv in di.items():\n",
    "#                 print(diik, \"=>\", len(diiv))\n",
    "#         else:\n",
    "#             print(di)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "scaling_strategy = None\n",
    "scalers = None\n",
    "test_data = False\n",
    "downsampling = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_features_list[0][0]: (391, 2048)\n",
      "human_boxes_list[0][0]: (391, 4)\n",
      "human_poses_list[0][0]: (391, 32, 2)\n",
      "\n",
      "object_features_list: (391, 1, 2048)\n",
      "object_boxes_list: (391, 1, 4)\n",
      "\n",
      "gt_list: dict_keys(['Human1']) [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "\n",
      "xs_steps: [391. 321. 368. 450. 442. 464. 385. 385. 441. 289. 238. 253. 369. 303.\n",
      " 341. 284. 267. 227. 307. 295. 314. 306. 349. 361. 245. 276. 281.]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================================================================\n",
    "# CUSTOM\n",
    "# ================================================================================================================================\n",
    "human_features_list = []\n",
    "human_boxes_list = []\n",
    "human_poses_list = []\n",
    "\n",
    "object_features_list = []\n",
    "object_boxes_list = []\n",
    "\n",
    "gt_list = []\n",
    "xs_steps = []\n",
    "\n",
    "for data_i in training_data:\n",
    "    # ================================================================================================================================\n",
    "    # HUMANS\n",
    "    # ================================================================================================================================\n",
    "    human_features_list.append([\n",
    "        # ==== ORIGINAL ====\n",
    "        data_i[0], \n",
    "        # data_i[1], \n",
    "        \n",
    "        # ==== CUSTOM ====\n",
    "        # data_i[0], data_i[1],\n",
    "    ])\n",
    "    human_boxes_list.append([\n",
    "        # ==== ORIGINAL ====\n",
    "        data_i[4], \n",
    "        # data_i[5],\n",
    "        \n",
    "        # ==== CUSOTM ====\n",
    "        # data_i[4], data_i[5], \n",
    "    ])\n",
    "    human_poses_list.append([\n",
    "        # ==== ORIGINAL ====\n",
    "        data_i[7], \n",
    "        # data_i[8], \n",
    "        \n",
    "        # ==== CUSTOM ====\n",
    "        # data_i[7], data_i[8],\n",
    "    ])\n",
    "    \n",
    "    # ================================================================================================================================\n",
    "    # OBJECTS\n",
    "    # ================================================================================================================================\n",
    "    object_features = data_i[2]\n",
    "    # object_features_rand = np.random.randn(object_features.shape[0], 3, object_features.shape[2])\n",
    "    # object_features = np.concatenate((object_features, object_features_rand), axis=1) \n",
    "    object_features = object_features[:, 0:1, :]\n",
    "    \n",
    "    object_boxes = data_i[6]\n",
    "    # object_boxes_rand = np.random.randn(object_boxes.shape[0], 3, object_boxes.shape[2])\n",
    "    # object_boxes = np.concatenate((object_boxes, object_boxes_rand), axis=1)\n",
    "    object_boxes = object_boxes[:, 0:1, :]\n",
    "    \n",
    "    assert object_features.shape[:-1] == object_boxes.shape[:-1], f\"object_features.shape[:-1]: {object_features.shape[:-1]} != object_boxes.shape[:-1]:{object_boxes.shape[:-1]}\"\n",
    "    \n",
    "    object_features_list.append(object_features)\n",
    "    object_boxes_list.append(object_boxes)\n",
    "    \n",
    "    # ================================================================================================================================\n",
    "    # MISC.\n",
    "    # ================================================================================================================================\n",
    "    gt_list.append(\n",
    "        {\n",
    "            # ==== ORIGINAL ====\n",
    "            'Human1': data_i[3]['Human1'],\n",
    "            # 'Human2': data_i[3]['Human2'],\n",
    "            \n",
    "            # ==== CUSTOM ====\n",
    "            # 'Human3': data_i[3]['Human1'],\n",
    "            # 'Human4': data_i[3]['Human2'],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ### assemble_num_steps(...)\n",
    "    x = data_i[0]\n",
    "    # print(x.shape)\n",
    "    num_steps = len(x[downsampling - 1::downsampling])\n",
    "    xs_steps.append(num_steps)\n",
    "    \n",
    "xs_steps = np.array(xs_steps, dtype=np.float32)\n",
    "\n",
    "print(\"human_features_list[0][0]:\", human_features_list[0][0].shape)\n",
    "print(\"human_boxes_list[0][0]:\", human_boxes_list[0][0].shape)\n",
    "print(\"human_poses_list[0][0]:\", human_poses_list[0][0].shape)\n",
    "print()\n",
    "\n",
    "print(\"object_features_list:\", object_features_list[0].shape)\n",
    "print(\"object_boxes_list:\", object_boxes_list[0].shape)\n",
    "print()\n",
    "\n",
    "print(\"gt_list:\", gt_list[0].keys(), gt_list[0]['Human1'])\n",
    "print()\n",
    "\n",
    "print(\"xs_steps:\", xs_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs_i.shape: (27, 464, 1, 2116)\n",
      "ys_i.shape: (27, 464, 1) int64 12 -1\n",
      "\n",
      "xs_i.shape: (27, 464, 1)\n",
      "ys_i.shape: (27, 464, 1) int64 12 -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyrutils.itertools import run_length_encoding\n",
    "from vhoi.data_loading import segmentation_from_output_class\n",
    "\n",
    "def assemble_mphoi_frame_level_recurrent_human(\n",
    "    human_features_list, human_poses_list, object_boxes_list, gt_list,\n",
    "    downsampling: int = 1, test_data: bool = False, max_no_objects: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Assemble recurrent human features for multiple humans in frame-level videos.\n",
    "\n",
    "    Args:\n",
    "        human_features_list:  list of list of human feature arrays per video\n",
    "        human_poses_list:     list of list of human pose arrays per video\n",
    "        object_boxes_list:    list of object bounding box arrays per video\n",
    "        gt_list: list of dicts {\"Human1\": labels, \"Human2\": ..., }\n",
    "        downsampling:         frame downsampling factor\n",
    "        test_data:            if True, skip label slicing\n",
    "        max_no_objects:       maximum number of objects in bounding boxes\n",
    "\n",
    "    Returns:\n",
    "        xs: [x_hs, x_hs_segmentation]\n",
    "        ys: [y_rec_hs, y_pred_hs, y_hs_segmentation]\n",
    "    \"\"\"\n",
    "    xs_h, xs_hp, x_obb = [], [], []\n",
    "    max_len, max_len_downsampled = 0, 0\n",
    "\n",
    "    # If not provided, infer maximum number of objects across all videos\n",
    "    if max_no_objects is None:\n",
    "        max_no_objects = max(\n",
    "            max(len(frame) for frame in video) for video in object_boxes_list\n",
    "        )\n",
    "\n",
    "    # Loop over videos\n",
    "    for humans, poses, objects_bounding_box in zip(human_features_list, human_poses_list, object_boxes_list):\n",
    "        num_humans = len(humans)\n",
    "        max_len = max(max_len, humans[0].shape[0])\n",
    "\n",
    "        # Downsample humans/poses\n",
    "        humans_ds = [h[downsampling - 1::downsampling] for h in humans]\n",
    "        poses_ds  = [p[downsampling - 1::downsampling] / 1000 for p in poses]\n",
    "        max_len_downsampled = max(max_len_downsampled, humans_ds[0].shape[0])\n",
    "        xs_h.append(humans_ds)\n",
    "        xs_hp.append(poses_ds)\n",
    "\n",
    "        # Downsample objects\n",
    "        obb_ds = objects_bounding_box[downsampling - 1::downsampling] / 1000\n",
    "        x_obb.append(obb_ds)\n",
    "\n",
    "    # Reshape object bounding boxes\n",
    "    xs_obb = []\n",
    "    for video in x_obb:\n",
    "        bb = []\n",
    "        for frame in video:\n",
    "            b = np.zeros((max_no_objects, 4))\n",
    "            n = min(len(frame), max_no_objects)\n",
    "            b[:n] = frame[:n]     # pad if fewer, truncate if more\n",
    "            b = b.reshape(max_no_objects * 2, 2)\n",
    "            bb.append(b)\n",
    "        xs_obb.append(bb)\n",
    "\n",
    "    # Add context features to each human\n",
    "    keypoints = [1, 2, 4, 6, 7, 11, 13, 14, 27]  # upper body keypoints\n",
    "    xs_h_with_context = []\n",
    "    for i, (humans_ds, poses_ds, obb_video) in enumerate(zip(xs_h, xs_hp, xs_obb)):\n",
    "        num_humans = len(humans_ds)\n",
    "        humans_context = [[] for _ in range(num_humans)]\n",
    "\n",
    "        for j in range(len(humans_ds[0])):  # loop frames\n",
    "            obb = obb_video[j]\n",
    "\n",
    "            # Compute velocities\n",
    "            if j + 1 < len(humans_ds[0]):\n",
    "                next_poses = [p[j+1][keypoints] for p in poses_ds]\n",
    "                pose_velos = [(next_pose - poses_ds[h][j][keypoints]) * 100 for h, next_pose in enumerate(next_poses)]\n",
    "                obb_velo = (obb_video[j+1] - obb) * 100\n",
    "            else:\n",
    "                pose_velos = [np.zeros((len(keypoints), 2)) for _ in poses_ds]\n",
    "                obb_velo = np.zeros((max_no_objects * 2, 2))\n",
    "\n",
    "            obbvelo = np.hstack((obb, obb_velo)).reshape(1, -1)\n",
    "\n",
    "            # Context per human\n",
    "            context = []\n",
    "            for h in range(num_humans):\n",
    "                pose = poses_ds[h][j][keypoints]\n",
    "                velo = pose_velos[h]\n",
    "                posevelo = np.hstack((pose, velo)).reshape(1, -1)\n",
    "                context.append(posevelo[0])\n",
    "            # Flatten [h1, h2, ..., obb] into context vector\n",
    "            context = np.concatenate(context + [obbvelo[0]])\n",
    "\n",
    "            # Concatenate context to each human’s own frame features\n",
    "            for h in range(num_humans):\n",
    "                h_con = np.concatenate((humans_ds[h][j], context))\n",
    "                humans_context[h].append(h_con)\n",
    "\n",
    "        xs_h_with_context.append([np.array(hc) for hc in humans_context])\n",
    "\n",
    "    # Stack humans into one array\n",
    "    feature_size = xs_h_with_context[0][0].shape[-1]\n",
    "    num_humans = len(xs_h_with_context[0])\n",
    "    x_hs = np.full([len(xs_h_with_context), max_len_downsampled, num_humans, feature_size],\n",
    "                   fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, humans in enumerate(xs_h_with_context):\n",
    "        for h, feats in enumerate(humans):\n",
    "            x_hs[m, :len(feats), h] = feats\n",
    "\n",
    "    xs = [x_hs]\n",
    "\n",
    "    # Output labels\n",
    "    y_rec_hs = np.full([len(x_hs), max_len, num_humans], fill_value=-1, dtype=np.int64)\n",
    "    y_pred_hs = np.full_like(y_rec_hs, fill_value=-1)\n",
    "\n",
    "    for m, video_hands_ground_truth in enumerate(gt_list):\n",
    "        for h in range(num_humans):\n",
    "            human_key = f\"Human{h+1}\"\n",
    "            if human_key not in video_hands_ground_truth:\n",
    "                continue\n",
    "            y_h = video_hands_ground_truth[human_key]\n",
    "            y_rec_hs[m, :len(y_h), h] = y_h\n",
    "            rle = list(run_length_encoding(y_h))\n",
    "            y_h_p = []\n",
    "            for (_, prev_len), (next_label, _) in zip(rle[:-1], rle[1:]):\n",
    "                y_h_p += [next_label] * prev_len\n",
    "            y_pred_hs[m, :len(y_h_p), h] = y_h_p\n",
    "\n",
    "    x_hs_segmentation = segmentation_from_output_class(\n",
    "        y_rec_hs[:, downsampling - 1::downsampling],\n",
    "        segmentation_type=\"input\"\n",
    "    )\n",
    "    xs.append(x_hs_segmentation)\n",
    "\n",
    "    if not test_data:\n",
    "        y_rec_hs = y_rec_hs[:, downsampling - 1::downsampling]\n",
    "        y_pred_hs = y_pred_hs[:, downsampling - 1::downsampling]\n",
    "\n",
    "    y_hs_segmentation = segmentation_from_output_class(y_rec_hs, segmentation_type=\"output\")\n",
    "    ys = [y_rec_hs, y_pred_hs, y_hs_segmentation]\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = assemble_mphoi_frame_level_recurrent_human(human_features_list, human_poses_list, object_boxes_list, gt_list)\n",
    "\n",
    "for xs_i, ys_i in zip(xs, ys):\n",
    "    print(\"xs_i.shape:\", xs_i.shape)\n",
    "    print(\"ys_i.shape:\", ys_i.shape, ys_i.dtype, ys_i.max().item(), ys_i.min().item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 464, 1, 2048) <class 'numpy.ndarray'>\n",
      "(27, 1) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def assemble_mphoi_frame_level_recurrent_objects(object_features_list, downsampling: int = 1):\n",
    "    xs_objects = []\n",
    "    max_len, max_len_downsampled, max_num_objects = 0, 0, 0\n",
    "    for objects in object_features_list:\n",
    "        max_len = max(max_len, objects.shape[0])\n",
    "        max_num_objects = max(max_num_objects, objects.shape[1])\n",
    "        objects = objects[downsampling - 1::downsampling]\n",
    "        max_len_downsampled = max(max_len_downsampled, objects.shape[0])\n",
    "        xs_objects.append(objects)\n",
    "    feature_size = xs_objects[-1].shape[-1]\n",
    "    x_objects = np.full([len(xs_objects), max_len_downsampled, max_num_objects, feature_size],\n",
    "                        fill_value=np.nan, dtype=np.float32)\n",
    "    x_objects_mask = np.zeros([len(xs_objects), max_num_objects], dtype=np.float32)\n",
    "    for m, x_o in enumerate(xs_objects):\n",
    "        x_objects[m, :x_o.shape[0], :x_o.shape[1], :] = x_o\n",
    "        x_objects_mask[m, :x_o.shape[1]] = 1.0\n",
    "    xs = [x_objects, x_objects_mask]\n",
    "    return xs\n",
    "\n",
    "xs_objects = assemble_mphoi_frame_level_recurrent_objects(object_features_list)\n",
    "for xs_i in xs_objects:\n",
    "    print(xs_i.shape, type(xs_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 464, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "from vhoi.data_loading import compute_centroid\n",
    "\n",
    "def assemble_mphoi_human_human_distances(human_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute pairwise human-human distances for multiple humans across videos.\n",
    "\n",
    "    Args:\n",
    "        human_boxes_list: list of list of human bounding boxes per video\n",
    "                          (outer list: videos, inner list: humans, array: frames x 4)\n",
    "        downsampling:     frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_hh_dists: tensor [num_videos, max_len, N, N] with pairwise distances\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_humans = 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for video_bbs in human_boxes_list:\n",
    "        num_humans = len(video_bbs)\n",
    "        max_num_humans = max(max_num_humans, num_humans)\n",
    "\n",
    "        # Downsample and compute centroids\n",
    "        centroids = []\n",
    "        for bb in video_bbs:\n",
    "            bb = bb[downsampling - 1::downsampling]\n",
    "            c = compute_centroid(bb) / mphoi_dims\n",
    "            centroids.append(c)\n",
    "\n",
    "        # Length of this video (frames)\n",
    "        max_len = max(max_len, centroids[0].shape[0])\n",
    "\n",
    "        # Compute pairwise distances (frames x N x N)\n",
    "        T = centroids[0].shape[0]\n",
    "        dists_matrix = np.zeros((T, num_humans, num_humans), dtype=np.float32)\n",
    "\n",
    "        for i, j in itertools.combinations(range(num_humans), 2):\n",
    "            d = np.linalg.norm(centroids[i] - centroids[j], ord=2, axis=-1)\n",
    "            dists_matrix[:, i, j] = d\n",
    "            dists_matrix[:, j, i] = d\n",
    "\n",
    "        all_dists.append(dists_matrix)\n",
    "\n",
    "    # Pad into a tensor [num_videos, max_len, max_num_humans, max_num_humans]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_humans, max_num_humans]\n",
    "    x_hh_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists_matrix in enumerate(all_dists):\n",
    "        T, N, _ = dists_matrix.shape\n",
    "        x_hh_dists[m, :T, :N, :N] = dists_matrix\n",
    "\n",
    "    return x_hh_dists\n",
    "\n",
    "\n",
    "xs_hh_dists = assemble_mphoi_human_human_distances(human_boxes_list)\n",
    "print(xs_hh_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 464, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "def assemble_mphoi_human_object_distances(human_boxes_list, object_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute human-object distances for multiple humans and objects across videos.\n",
    "\n",
    "    Args:\n",
    "        human_boxes_list:  list of list of human bounding boxes per video\n",
    "                           (outer list: videos, inner list: humans, array: frames x 4)\n",
    "        object_boxes_list: list of object bounding box arrays per video (frames x num_objects x 4)\n",
    "        downsampling:      frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_ho_dists: tensor [num_videos, max_len, max_num_humans, max_num_objects]\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_humans, max_num_objects = 0, 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for video_bbs, obj_bbs in zip(human_boxes_list, object_boxes_list):\n",
    "        num_humans = len(video_bbs)\n",
    "\n",
    "        # Downsample humans → centroids\n",
    "        human_centroids = []\n",
    "        for bb in video_bbs:\n",
    "            bb = bb[downsampling - 1::downsampling]\n",
    "            c = compute_centroid(bb) / mphoi_dims\n",
    "            human_centroids.append(c)\n",
    "\n",
    "        # Downsample objects → centroids\n",
    "        obj_bbs = obj_bbs[downsampling - 1::downsampling]\n",
    "        obj_centroids = compute_centroid(obj_bbs) / mphoi_dims\n",
    "\n",
    "        T = obj_centroids.shape[0]\n",
    "        max_len = max(max_len, T)\n",
    "        max_num_humans = max(max_num_humans, num_humans)\n",
    "        max_num_objects = max(max_num_objects, obj_centroids.shape[1])\n",
    "\n",
    "        # Compute distances [frames, num_humans, num_objects]\n",
    "        dists_matrix = np.zeros((T, num_humans, obj_centroids.shape[1]), dtype=np.float32)\n",
    "        for h, h_c in enumerate(human_centroids):\n",
    "            d = np.linalg.norm(obj_centroids - np.expand_dims(h_c, axis=1), ord=2, axis=-1)\n",
    "            dists_matrix[:, h, :] = d\n",
    "\n",
    "        all_dists.append(dists_matrix)\n",
    "\n",
    "    # Pad into a tensor [num_videos, max_len, max_num_humans, max_num_objects]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_humans, max_num_objects]\n",
    "    x_ho_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists_matrix in enumerate(all_dists):\n",
    "        T, H, O = dists_matrix.shape\n",
    "        x_ho_dists[m, :T, :H, :O] = dists_matrix\n",
    "\n",
    "    return x_ho_dists\n",
    "\n",
    "xs_ho_dists = assemble_mphoi_human_object_distances(human_boxes_list, object_boxes_list)\n",
    "print(xs_ho_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 464, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def assemble_mphoi_object_object_distances(object_boxes_list, downsampling: int = 1):\n",
    "    \"\"\"\n",
    "    Compute pairwise object-object distances across videos.\n",
    "\n",
    "    Args:\n",
    "        object_boxes_list: list of object bounding box arrays per video (frames x num_objects x 4)\n",
    "        downsampling:      frame downsampling factor\n",
    "\n",
    "    Returns:\n",
    "        x_oo_dists: tensor [num_videos, max_len, max_num_objects, max_num_objects]\n",
    "    \"\"\"\n",
    "    mphoi_dims = np.array([3840, 2160], dtype=np.float32)\n",
    "    max_len, max_num_objects = 0, 0\n",
    "    all_dists = []\n",
    "\n",
    "    for obj_bbs in object_boxes_list:\n",
    "        # Downsample and compute centroids\n",
    "        obj_bbs = obj_bbs[downsampling - 1::downsampling]\n",
    "        objs_centroid = compute_centroid(obj_bbs) / mphoi_dims   # (frames, num_objects, 2)\n",
    "        num_objects = objs_centroid.shape[1]\n",
    "\n",
    "        # Compute pairwise distances per frame\n",
    "        dists = []\n",
    "        for k in range(num_objects):\n",
    "            kth_object_centroid = objs_centroid[:, k:k+1]  # (frames, 1, 2)\n",
    "            kth_dist = np.linalg.norm(objs_centroid - kth_object_centroid, ord=2, axis=-1)  # (frames, num_objects)\n",
    "            dists.append(kth_dist)\n",
    "\n",
    "        dists = np.stack(dists, axis=1)  # (frames, num_objects, num_objects)\n",
    "        all_dists.append(dists)\n",
    "\n",
    "        max_len = max(max_len, obj_bbs.shape[0])\n",
    "        max_num_objects = max(max_num_objects, num_objects)\n",
    "\n",
    "    # Pad into tensor [num_videos, max_len, max_num_objects, max_num_objects]\n",
    "    tensor_shape = [len(all_dists), max_len, max_num_objects, max_num_objects]\n",
    "    x_oo_dists = np.full(tensor_shape, fill_value=np.nan, dtype=np.float32)\n",
    "\n",
    "    for m, dists in enumerate(all_dists):\n",
    "        T, O1, O2 = dists.shape\n",
    "        x_oo_dists[m, :T, :O1, :O2] = dists\n",
    "\n",
    "    return x_oo_dists\n",
    "\n",
    "xs_oo_dists = assemble_mphoi_object_object_distances(object_boxes_list)\n",
    "print(xs_oo_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs info:\n",
    "# 0: x_human                    => [8, 154, 2, 2152]\n",
    "# 1: x_objects                  => [8, 154, 4, 2048]\n",
    "# 2: objects_mask               => [8, 4]\n",
    "# 3: human_segmentation         => [8, 154, 2]\n",
    "# 4: human_human_distances      => [8, 154, 2, 2]\n",
    "# 5: human_object_distances     => [8, 154, 2, 4]\n",
    "# 6: object_object_distances    => [8, 154, 4, 4]\n",
    "# 7: steps_per_example          => [8]\n",
    "# see ./vhoi/models.py, line 612, for more info\n",
    "\n",
    "#    0,       1, 2,        3,        4,           5,           6,           7\n",
    "# xs = xs[:1] + xs_objects + xs[1:] + [xs_hh_dists, xs_ho_dists, xs_oo_dists, xs_steps]\n",
    "# for xsi in xs:\n",
    "#     print(xsi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "(27, 464, 1, 2116)\n",
      "(27, 464, 1, 2048)\n",
      "(27, 1)\n",
      "(27, 464, 1)\n",
      "(27, 464, 1, 1)\n",
      "(27, 464, 1, 1)\n",
      "(27, 464, 1, 1)\n",
      "(27,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### create_data_loader(...)\n",
    "from vhoi.data_loading import (\n",
    "    # assemble_mphoi_tensors, \n",
    "    assemble_mphoi_frame_level_recurrent_objects, \n",
    "    ignore_last_step_end_flag_general,\n",
    "    smooth_segmentation,\n",
    "    assemble_mphoi_human_human_distances,\n",
    "    assemble_mphoi_human_object_distances,\n",
    "    assemble_mphoi_object_object_distances,\n",
    "    assemble_num_steps,\n",
    ")\n",
    "\n",
    "data = training_data\n",
    "\n",
    "# if dataset_name.lower() == 'cad120':\n",
    "#     x, y = assemble_tensors(data, model_name, model_input_type, sigma=sigma, downsampling=downsampling,\n",
    "#                             test_data=test_data)\n",
    "# elif dataset_name.lower() == 'mphoi':\n",
    "# x, y = assemble_mphoi_tensors(data=training_data, model_name=model_name, sigma=sigma, downsampling=downsampling, test_data=test_data)\n",
    "\n",
    "### assemble_mphoi_tensors(...)\n",
    "# from vhoi.data_loading import assemble_mphoi_frame_level_recurrent_human\n",
    "# xs, ys = assemble_mphoi_frame_level_recurrent_human(data, downsampling=downsampling, test_data=test_data)\n",
    "\n",
    "# xs (assemble_mphoi_frame_level_recurrent_human) info:\n",
    "# 0: x_human            => [8, 154, 2, 2152]\n",
    "# 1: human_segmentation => [8, 154, 2]\n",
    "\n",
    "# xs_objects = assemble_mphoi_frame_level_recurrent_objects(data, downsampling=downsampling)\n",
    "# if model_name == '2G-GCN':\n",
    "if sigma:\n",
    "    ys[2] = ignore_last_step_end_flag_general(ys[2])\n",
    "ys[2] = smooth_segmentation(ys[2], sigma)\n",
    "ys_budget = ys[2]\n",
    "# xs_hh_dists = assemble_mphoi_human_human_distances(data, downsampling=downsampling)\n",
    "# xs_ho_dists = assemble_mphoi_human_object_distances(data, downsampling=downsampling)\n",
    "# xs_oo_dists = assemble_mphoi_object_object_distances(data, downsampling=downsampling)\n",
    "# xs_steps = assemble_num_steps(data, downsampling=downsampling)\n",
    "\n",
    "# print(\"xs:\")\n",
    "# for xsi in xs:\n",
    "#     print(xsi.shape)\n",
    "# print()\n",
    "\n",
    "# xs info:\n",
    "# 0: x_human                    => [8, 154, 2, 2152]\n",
    "# 1: x_objects                  => [8, 154, 4, 2048]\n",
    "# 2: objects_mask               => [8, 4]\n",
    "# 3: human_segmentation         => [8, 154, 2]\n",
    "# 4: human_human_distances      => [8, 154, 2, 2]\n",
    "# 5: human_object_distances     => [8, 154, 2, 4]\n",
    "# 6: object_object_distances    => [8, 154, 4, 4]\n",
    "# 7: steps_per_example          => [8]\n",
    "# see ./vhoi/models.py, line 612, for more info\n",
    "\n",
    "#    0,       1, 2,        3,        4,           5,           6,           7\n",
    "xs = xs[:1] + xs_objects + xs[1:] + [xs_hh_dists, xs_ho_dists, xs_oo_dists, xs_steps]\n",
    "ys = [ys_budget] + ys[2:] + ys[:2]\n",
    "ys += ys[-2:]\n",
    "x, y = xs, ys\n",
    "\n",
    "print(\"x:\")\n",
    "for xi in x:\n",
    "    print(xi.shape)\n",
    "print()\n",
    "\n",
    "# else:\n",
    "#     x, y = assemble_bimanual_tensors(data, model_name, sigma=sigma, downsampling=downsampling, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      "(27, 464, 1)\n",
      "(27, 464, 1)\n",
      "(27, 464, 1)\n",
      "(27, 464, 1)\n",
      "(27, 464, 1)\n",
      "(27, 464, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"y:\")\n",
    "for y_i in y:\n",
    "    print(y_i.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "torch.Size([27, 464, 1, 2116])\n",
      "torch.Size([27, 464, 1, 2048])\n",
      "torch.Size([27, 1])\n",
      "torch.Size([27, 464, 1])\n",
      "torch.Size([27, 464, 1, 1])\n",
      "torch.Size([27, 464, 1, 1])\n",
      "torch.Size([27, 464, 1, 1])\n",
      "torch.Size([27])\n",
      "torch.Size([27, 464, 1])\n",
      "torch.Size([27, 464, 1])\n",
      "torch.Size([27, 464, 1])\n",
      "torch.Size([27, 464, 1])\n",
      "torch.Size([27, 464, 1])\n",
      "torch.Size([27, 464, 1])\n",
      "\n",
      "data_loader\n",
      "torch.Size([8, 464, 1, 2116]) torch.float32\n",
      "torch.Size([8, 464, 1, 2048]) torch.float32\n",
      "torch.Size([8, 1]) torch.float32\n",
      "torch.Size([8, 464, 1]) torch.float32\n",
      "torch.Size([8, 464, 1, 1]) torch.float32\n",
      "torch.Size([8, 464, 1, 1]) torch.float32\n",
      "torch.Size([8, 464, 1, 1]) torch.float32\n",
      "torch.Size([8]) torch.float32\n",
      "torch.Size([8, 464, 1]) torch.float32\n",
      "torch.Size([8, 464, 1]) torch.float32\n",
      "torch.Size([8, 464, 1]) torch.int64\n",
      "torch.Size([8, 464, 1]) torch.int64\n",
      "torch.Size([8, 464, 1]) torch.int64\n",
      "torch.Size([8, 464, 1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from vhoi.data_loading import maybe_scale_input_tensors\n",
    "from pyrutils.torch.train_utils import numpy_to_torch\n",
    "\n",
    "x, y = xs, ys\n",
    "\n",
    "x, scalers = maybe_scale_input_tensors(x, model_name, scaling_strategy=scaling_strategy, scalers=scalers)\n",
    "x = [np.nan_to_num(ix, copy=False, nan=0.0) for ix in x]\n",
    "x, y = numpy_to_torch(*x), numpy_to_torch(*y)\n",
    "\n",
    "# print(\"x (numpy_to_torch):\")\n",
    "# for xi in x:\n",
    "#     print(xi.shape)\n",
    "# print()\n",
    "\n",
    "dataset = TensorDataset(*(x + y))\n",
    "\n",
    "print(\"dataset:\")\n",
    "for d in dataset.tensors:\n",
    "    print(d.shape)\n",
    "print()\n",
    "    \n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0,\n",
    "                            pin_memory=False, drop_last=False)\n",
    "# segmentations = assemble_segmentations(data, model_name, dataset_name=dataset_name)\n",
    "\n",
    "print(\"data_loader\")\n",
    "for idx, i in enumerate(next(iter(data_loader))):\n",
    "    print(i.shape, i.dtype)\n",
    "    # if idx == 6:\n",
    "    #     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vhoi.data_loading import input_size_from_data_loader\n",
    "\n",
    "train_loader = data_loader\n",
    "\n",
    "input_size = input_size_from_data_loader(train_loader, model_name, model_input_type)\n",
    "data_info = {'input_size': input_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "### Model = select_model(model_name)\n",
    "model_creation_args = cfg.models.parameters\n",
    "### model_creation_args = {**data_info, **model_creation_args}\n",
    "model_creation_args = {**data_info, **model_creation_args.__dict__}\n",
    "dataset_name = cfg.data.name\n",
    "num_classes = determine_num_classes(model_name, model_input_type, dataset_name)\n",
    "model_creation_args['num_classes'] = num_classes\n",
    "device = 'cuda' if torch.cuda.is_available() and cfg.resources.use_gpu else 'cpu'\n",
    "\n",
    "# Disable geometry features, as they do not support more than 2 humans\n",
    "model_creation_args['message_geometry_to_objects'] = True\n",
    "model_creation_args['message_geometry_to_human'] = True\n",
    "\n",
    "# Model configuration if object is only one\n",
    "model_creation_args['message_objects_to_object'] = False\n",
    "model_creation_args['gcn_node'] = 17\n",
    "\n",
    "# Model configuration if human is only one\n",
    "model_creation_args['message_humans_to_human'] = False\n",
    "\n",
    "model = TGGCN_Custom(**model_creation_args).to(device)\n",
    "if misc_dict.get('pretrained', False) and misc_dict.get('pretrained_path') is not None:\n",
    "    state_dict = load_model_weights(misc_dict['pretrained_path'])\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=cfg.models.optimization.learning_rate)\n",
    "criterion, loss_names = select_loss(model_name, model_input_type, dataset_name, cfg=cfg)\n",
    "mtll_model = None\n",
    "if misc_dict.get('multi_task_loss_learner', False):\n",
    "    loss_types = select_loss_types(model_name, dataset_name, cfg=cfg)\n",
    "    mask = select_loss_learning_mask(model_name, dataset_name, cfg=cfg)\n",
    "    mtll_model = MultiTaskLossLearner(loss_types=loss_types, mask=mask).to(device)\n",
    "    optimizer.add_param_group({'params': mtll_model.parameters()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = cfg.models.logging.root_log_dir\n",
    "checkpoint_name = cfg.models.logging.checkpoint_name\n",
    "fetch_model_data = select_model_data_fetcher(model_name, model_input_type,\n",
    "                                             dataset_name=dataset_name, **{**misc_dict, **cfg.models.parameters.__dict__})\n",
    "feed_model_data = select_model_data_feeder(model_name, model_input_type, dataset_name=dataset_name, **misc_dict)\n",
    "num_main_losses = decide_num_main_losses(model_name, dataset_name, {**misc_dict, **cfg.models.parameters.__dict__})\n",
    "# checkpoint = train(model, train_loader, optimizer, criterion, cfg.optimization.epochs, device, loss_names,\n",
    "#                    clip_gradient_at=cfg.optimization.clip_gradient_at,\n",
    "#                    fetch_model_data=fetch_model_data, feed_model_data=feed_model_data,\n",
    "#                    val_loader=val_loader, mtll_model=mtll_model, num_main_losses=num_main_losses,\n",
    "#                    tensorboard_log_dir=tensorboard_log_dir, checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_name = kwargs.get('checkpoint_name', None)\n",
    "# tensorboard_log_dir = kwargs.get('tensorboard_log_dir', None)\n",
    "# writer = None\n",
    "# if tensorboard_log_dir is not None and checkpoint_name is not None:\n",
    "#     writer = SummaryWriter(os.path.join(tensorboard_log_dir, 'runs', checkpoint_name))\n",
    "checkpoint = {}\n",
    "train_losses, val_losses, train_raw_losses, val_raw_losses = [], [], [], []\n",
    "val_loss = float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([8, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 464, 1]) torch.float32 1.0 0.0\n",
      "torch.Size([8, 464, 1]) torch.float32 0.9998103976249695 5.2600487833842635e-05\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.1928343772888184 -2.9663479328155518\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.199812173843384 -3.0870087146759033\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.434875726699829 -2.7114181518554688\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.4137790203094482 -2.694533348083496\n",
      "\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.5714, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.5326, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "(Train) Batch [     8/    27 ( 25%)]  Loss:   5.1040  B_HS:  0.0000\n",
      "  BCE_HS:  0.0000\n",
      "  NLL_SAR_F:  0.0000\n",
      "  NLL_SAP_F:  0.0000\n",
      "  NLL_SAR:  2.5714\n",
      "  NLL_SAP:  2.5326\n",
      "\n",
      "torch.Size([8, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([8, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "\n",
      "torch.Size([8, 464, 1]) torch.float32 1.0 0.0\n",
      "torch.Size([8, 464, 1]) torch.float32 0.9996055960655212 0.0001595731737324968\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.134519577026367 -3.159463405609131\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.1893343925476074 -3.0143778324127197\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.302581548690796 -2.773771047592163\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.1775927543640137 -2.8284215927124023\n",
      "\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.5201, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.4938, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "torch.Size([8, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([8, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([8, 464, 1]) torch.int64 12 -1\n",
      "\n",
      "torch.Size([8, 464, 1]) torch.float32 1.0 0.0\n",
      "torch.Size([8, 464, 1]) torch.float32 0.9994454979896545 0.00016831266111694276\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.160889148712158 -2.986966371536255\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.152660369873047 -3.0430209636688232\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -2.190765142440796 -2.866318464279175\n",
      "torch.Size([8, 13, 464, 1]) torch.float32 -1.9388961791992188 -2.99282169342041\n",
      "\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.4936, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.3257, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "torch.Size([3, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([3, 464, 1]) torch.float32 1.0 -1.0\n",
      "torch.Size([3, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([3, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([3, 464, 1]) torch.int64 12 -1\n",
      "torch.Size([3, 464, 1]) torch.int64 12 -1\n",
      "\n",
      "torch.Size([3, 464, 1]) torch.float32 1.0 0.0\n",
      "torch.Size([3, 464, 1]) torch.float32 0.9996640682220459 0.0016239054966717958\n",
      "torch.Size([3, 13, 464, 1]) torch.float32 -2.1982240676879883 -3.025599956512451\n",
      "torch.Size([3, 13, 464, 1]) torch.float32 -2.1285486221313477 -3.101890802383423\n",
      "torch.Size([3, 13, 464, 1]) torch.float32 -2.0727007389068604 -3.0677452087402344\n",
      "torch.Size([3, 13, 464, 1]) torch.float32 -1.6746482849121094 -3.2040746212005615\n",
      "\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.4361, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2403, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "(Train) Batch [    27/    27 (100%)]  Loss:   4.6764  B_HS:  0.0000\n",
      "  BCE_HS:  0.0000\n",
      "  NLL_SAR_F:  0.0000\n",
      "  NLL_SAP_F:  0.0000\n",
      "  NLL_SAR:  2.4361\n",
      "  NLL_SAP:  2.2403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader = train_loader\n",
    "clip_gradient_at=0.0\n",
    "log_interval=25\n",
    "\n",
    "model.train()\n",
    "if mtll_model is not None:\n",
    "    mtll_model.train()\n",
    "num_examples = len(data_loader.dataset)\n",
    "for batch_idx, dataset in enumerate(data_loader):\n",
    "    data, target = fetch_model_data(dataset, device=device)\n",
    "    for t in target:\n",
    "        print(t.shape, t.dtype, t.max().item(), t.min().item())\n",
    "    print()\n",
    "    optimizer.zero_grad()\n",
    "    output = feed_model_data(model, data)\n",
    "    for o in output:\n",
    "        print(o.shape, o.dtype, o.max().item(), o.min().item())\n",
    "    print()\n",
    "    losses = criterion(output, target, reduction='mean')\n",
    "    for l in losses:\n",
    "        print(l)\n",
    "    print()\n",
    "    if mtll_model is not None:\n",
    "        losses = mtll_model(losses)\n",
    "    loss = sum(losses)\n",
    "    loss.backward()\n",
    "    if clip_gradient_at:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_gradient_at)\n",
    "    optimizer.step()\n",
    "    log_now, is_last_batch = (batch_idx % log_interval) == 0, batch_idx == (len(data_loader) - 1)\n",
    "    if log_now or is_last_batch:\n",
    "        num_main_losses = num_main_losses if num_main_losses is not None else len(losses)\n",
    "        loss = sum(losses[-num_main_losses:])\n",
    "        batch_initial_example_idx = min((batch_idx + 1) * data_loader.batch_size, num_examples)\n",
    "        epoch_progress = 100 * (batch_idx + 1) / len(data_loader)\n",
    "        print(f'(Train) Batch [{batch_initial_example_idx:6d}/{num_examples:6d} ({epoch_progress:3.0f}%)] ',\n",
    "                f'Loss: {loss.item(): 8.4f}', end='')\n",
    "        for loss_name, single_loss in zip(loss_names, losses):\n",
    "            print(f'  {loss_name}: {single_loss: 6.4f}', end='')\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "    # Test for only single batch\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch271",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
