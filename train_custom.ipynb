{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: `py39_torch271`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dacite import from_dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pyrutils.torch.train_utils import train, save_checkpoint\n",
    "from pyrutils.torch.multi_task import MultiTaskLossLearner\n",
    "from vhoi.data_loading import (\n",
    "    input_size_from_data_loader, \n",
    "    select_model_data_feeder, \n",
    "    select_model_data_fetcher,\n",
    ")\n",
    "from vhoi.data_loading_custom import (\n",
    "    create_data,\n",
    "    create_data_loader\n",
    ")\n",
    "from vhoi.losses import (\n",
    "    select_loss, \n",
    "    decide_num_main_losses, \n",
    "    select_loss_types, \n",
    "    select_loss_learning_mask,\n",
    ")\n",
    "from vhoi.models import load_model_weights\n",
    "from vhoi.models_custom import TGGCN\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)   # Python的随机性\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "np.random.seed(seed)   # numpy的随机性\n",
    "torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True   # 选择确定性算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictMixin:\n",
    "    def get(self, key, default_value=None):\n",
    "        return getattr(self, key, default_value)\n",
    "\n",
    "    def as_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "@dataclass\n",
    "class Resources(DictMixin):\n",
    "    use_gpu: bool\n",
    "    num_threads: int\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata(DictMixin):\n",
    "    model_name: str\n",
    "    input_type: str\n",
    "\n",
    "@dataclass\n",
    "class ModelParameters(DictMixin):\n",
    "    add_segment_length: int\n",
    "    add_time_position: int\n",
    "    time_position_strategy: str\n",
    "    positional_encoding_style: str\n",
    "    attention_style: str\n",
    "    bias: bool\n",
    "    cat_level_states: int\n",
    "    discrete_networks_num_layers: int\n",
    "    discrete_optimization_strategy: str\n",
    "    filter_discrete_updates: bool\n",
    "    gcn_node: int\n",
    "    hidden_size: int\n",
    "    message_humans_to_human: bool\n",
    "    message_human_to_objects: bool\n",
    "    message_objects_to_human: bool\n",
    "    message_objects_to_object: bool\n",
    "    message_geometry_to_objects: bool\n",
    "    message_geometry_to_human: bool\n",
    "    message_segment: bool\n",
    "    message_type: str\n",
    "    message_granularity: str\n",
    "    message_aggregation: str\n",
    "    object_segment_update_strategy: str\n",
    "    share_level_mlps: int\n",
    "    update_segment_threshold: float\n",
    "\n",
    "@dataclass\n",
    "class ModelOptimization(DictMixin):\n",
    "    batch_size: int\n",
    "    clip_gradient_at: float\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    val_fraction: float\n",
    "\n",
    "@dataclass\n",
    "class BudgetLoss(DictMixin):\n",
    "    add: bool\n",
    "    human_weight: float\n",
    "    object_weight: float\n",
    "\n",
    "@dataclass\n",
    "class SegmentationLoss(DictMixin):\n",
    "    add: bool\n",
    "    pretrain: bool\n",
    "    sigma: float\n",
    "    weight: float\n",
    "\n",
    "@dataclass\n",
    "class ModelMisc(DictMixin):\n",
    "    anticipation_loss_weight: float\n",
    "    budget_loss: BudgetLoss\n",
    "    first_level_loss_weight: float\n",
    "    impose_segmentation_pattern: int\n",
    "    input_human_segmentation: bool\n",
    "    input_object_segmentation: bool\n",
    "    make_attention_distance_based: bool\n",
    "    multi_task_loss_learner: bool\n",
    "    pretrained: bool\n",
    "    pretrained_path: Optional[str]\n",
    "    segmentation_loss: SegmentationLoss\n",
    "\n",
    "@dataclass\n",
    "class ModelLogging(DictMixin):\n",
    "    root_log_dir: str\n",
    "    checkpoint_name: str\n",
    "    log_dir: str\n",
    "\n",
    "@dataclass\n",
    "class Models(DictMixin):\n",
    "    metadata: ModelMetadata\n",
    "    parameters: ModelParameters\n",
    "    optimization: ModelOptimization\n",
    "    misc: ModelMisc\n",
    "    logging: ModelLogging\n",
    "\n",
    "@dataclass\n",
    "class Data(DictMixin):\n",
    "    name: str\n",
    "    path: str\n",
    "    path_zarr: str\n",
    "    path_obb_zarr: str\n",
    "    path_hbb_zarr: str\n",
    "    path_hps_zarr: str\n",
    "    cross_validation_test_subject: str\n",
    "    scaling_strategy: Optional[str]\n",
    "    downsampling: int\n",
    "\n",
    "@dataclass\n",
    "class Config(DictMixin):\n",
    "    resources: Resources\n",
    "    models: Models\n",
    "    data: Data\n",
    "    \n",
    "metadata_dict = {\n",
    "    \"model_name\": \"2G-GCN\",\n",
    "    \"input_type\": \"multiple\"\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    \"add_segment_length\": 0,  # length of the segment to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"add_time_position\": 0,  # absolute time position to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"time_position_strategy\": \"s\",  # input time position to segment [s] or discrete update [u].\n",
    "    \"positional_encoding_style\": \"e\",  # e [embedding] or p [periodic].\n",
    "    \"attention_style\": \"v3\",  # v1 [concat], v2 [dot-product], v3 [scaled_dot-product], v4 [general]\n",
    "    \"bias\": True,\n",
    "    \"cat_level_states\": 0,  # concatenate first and second level hidden states for predictors MLPs.\n",
    "    \"discrete_networks_num_layers\": 1,  # depth of the state change detector MLP.\n",
    "    \"discrete_optimization_strategy\": \"gs\",  # straight-through [st] or gumbel-sigmoid [gs]\n",
    "    \"filter_discrete_updates\": False,  # maxima filter for soft output of state change detector.\n",
    "    \"gcn_node\": 25,  # custom, original: 19 for cad120, 30 for bimanual, 26 for mphoi\n",
    "    \"hidden_size\": 512,  # 512 for cad120 & mphoi; 64 for bimanual\n",
    "    \"message_humans_to_human\": False, # custom, original: True\n",
    "    \"message_human_to_objects\": True,\n",
    "    \"message_objects_to_human\": True,\n",
    "    \"message_objects_to_object\": False, # custom, original: True\n",
    "    \"message_geometry_to_objects\": True,\n",
    "    \"message_geometry_to_human\": True,  # custom, original: False\n",
    "    \"message_segment\": True,\n",
    "    \"message_type\": \"v2\",  # v1 [relational] or v2 [non-relational]\n",
    "    \"message_granularity\": \"v1\",  # v1 [generic] or v2 [specific]\n",
    "    \"message_aggregation\": \"att\",  # mean_pooling [mp] or attention [att]\n",
    "    \"object_segment_update_strategy\": \"ind\",  # same_as_human [sah], independent [ind], or conditional_on_human [coh]\n",
    "    \"share_level_mlps\": 0,  # whether to share [1] or not [0] the prediction MLPs of the levels.\n",
    "    \"update_segment_threshold\": 0.5  # [0.0, 1.0)\n",
    "}\n",
    "\n",
    "optimization_dict = {\n",
    "    \"batch_size\": 64,  # mphoi:8; cad120:16; bimanual: 32\n",
    "    \"clip_gradient_at\": 0.0,\n",
    "    \"epochs\": 10, # custom, original: cad120 & mphoi:40; bimanual: 60\n",
    "    \"learning_rate\": 1e-4,  # mphoi:1e-4; cad120 & bimanual:1e-3\n",
    "    \"val_fraction\": 0.1\n",
    "}\n",
    "\n",
    "data_dict = {\n",
    "    \"name\": \"mphoi\",\n",
    "    \"path\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_ground_truth_labels.json\",\n",
    "    \"path_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/faster_rcnn.zarr\",\n",
    "    \"path_obb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/object_bounding_boxes.zarr\",\n",
    "    \"path_hbb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_bounding_boxes.zarr\",\n",
    "    \"path_hps_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_pose.zarr\",\n",
    "    \"cross_validation_test_subject\": \"Subject14\",  # Subject45, Subject25, Subject14\n",
    "    \"scaling_strategy\": None,  # null or \"standard\"\n",
    "    \"downsampling\": 1 # custom, original: 3, 1 = full FPS, 2 = half FPS, ...\n",
    "}\n",
    "\n",
    "# root_log_dir = f\"{os.getcwd()}/outputs_hiergat/{data_dict['name']}/{metadata_dict['model_name']}\"\n",
    "root_log_dir = f\"{os.getcwd()}/outputs_hiergat/custom\"\n",
    "checkpoint_name = (\n",
    "    f\"hs{parameters_dict['hidden_size']}_e{optimization_dict['epochs']}_bs{optimization_dict['batch_size']}_\"\n",
    "    f\"lr{optimization_dict['learning_rate']}_{parameters_dict['update_segment_threshold']}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    ")\n",
    "log_dir = f\"{root_log_dir}/{checkpoint_name}\"\n",
    "print(\"Log directory:\", log_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "cfg_dict = {\n",
    "    \"resources\": {\n",
    "        \"use_gpu\": True,\n",
    "        \"num_threads\": 32\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"metadata\": metadata_dict,\n",
    "        \"parameters\": parameters_dict,\n",
    "        \"optimization\": optimization_dict,\n",
    "        \"misc\": {\n",
    "            \"anticipation_loss_weight\": 1.0,\n",
    "            \"budget_loss\": {\n",
    "                \"add\": False,\n",
    "                \"human_weight\": 1.0,\n",
    "                \"object_weight\": 1.0\n",
    "            },\n",
    "            \"first_level_loss_weight\": 0.0,  # if positive, first level does frame-level prediction\n",
    "            \"impose_segmentation_pattern\": 1,  # 0 [no pattern], 1 [all ones]\n",
    "            \"input_human_segmentation\": False,  # (was \"flase\" in YAML, corrected here)\n",
    "            \"input_object_segmentation\": False,\n",
    "            \"make_attention_distance_based\": True,  # only meaningful if message_aggregation is attention\n",
    "            \"multi_task_loss_learner\": False,\n",
    "            \"pretrained\": False,  # unfortunately need two entries for checkpoint name\n",
    "            \"pretrained_path\": None,  # specified parameters must match pre-trained model\n",
    "            \"segmentation_loss\": {\n",
    "                \"add\": False,\n",
    "                \"pretrain\": False,\n",
    "                \"sigma\": 0.0,  # Gaussian smoothing\n",
    "                \"weight\": 1.0\n",
    "            }\n",
    "        },\n",
    "        \"logging\": {\n",
    "            \"root_log_dir\": root_log_dir,\n",
    "            \"checkpoint_name\": checkpoint_name,\n",
    "            \"log_dir\": log_dir\n",
    "        },\n",
    "    },\n",
    "    \"data\": data_dict,\n",
    "}\n",
    "\n",
    "cfg = from_dict(data_class=Config, data=cfg_dict)\n",
    "\n",
    "torch.set_num_threads(cfg.resources.num_threads)\n",
    "model_name, model_input_type = cfg.models.metadata.model_name, cfg.models.metadata.input_type\n",
    "batch_size, val_fraction = cfg.models.optimization.batch_size, cfg.models.optimization.val_fraction\n",
    "misc_dict = cfg.get('misc', default_value={})\n",
    "sigma = misc_dict.get('segmentation_loss', {}).get('sigma', 0.0)\n",
    "scaling_strategy = cfg.data.scaling_strategy\n",
    "downsampling = cfg.data.downsampling\n",
    "\n",
    "action_classes = [\n",
    "    # human\n",
    "    'supervise',        # 0\n",
    "    'collaborate with', # 1\n",
    "    'assist',           # 2\n",
    "    'lead',             # 3\n",
    "    'coordinate with',  # 4\n",
    "    'listen to',        # 5\n",
    "\n",
    "    # rebar\n",
    "    'tie',              # 6\n",
    "    'erect',            # 7\n",
    "    'prepare_rebar',    # 8\n",
    "    'transport',        # 9\n",
    "\n",
    "    # formwork\n",
    "    'install',          # 10\n",
    "    'prepare_formwork', # 11\n",
    "\n",
    "    # concrete\n",
    "    'pour',             # 12\n",
    "    'finish',           # 13\n",
    "\n",
    "    # equipment         \n",
    "    'use',              # 14\n",
    "    'carry',            # 15\n",
    "\n",
    "    # all\n",
    "    'inspect',          # 16\n",
    "    'no interaction',   # 17\n",
    "]\n",
    "# new_action_classes = [\n",
    "#     # rebar\n",
    "#     'tie',              # 0\n",
    "#     'erect',            # 1\n",
    "#     'prepare_rebar',    # 2\n",
    "#     'transport',        # 3\n",
    "\n",
    "#     # equipment         \n",
    "#     'use',              # 4\n",
    "#     'carry',            # 5\n",
    "\n",
    "#     # all\n",
    "#     'inspect',          # 6\n",
    "#     'no interaction',   # 7\n",
    "# ]\n",
    "new_action_classes = [\n",
    "    # all\n",
    "    'inspect',          # 0\n",
    "    \n",
    "    # rebar\n",
    "    'prepare_rebar',    # 1\n",
    "    'erect',            # 2\n",
    "    'tie',              # 3\n",
    "]\n",
    "num_classes = len(new_action_classes)\n",
    "train_features_dirs = [\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'), \n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0090_full_MP4_anno_for_labelling_rizky_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features'),\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features'),\n",
    "    # Path('/root/vs-gats-plaster/deepsort/hiergat_data_3/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features'),\n",
    "]\n",
    "\n",
    "val_features_dirs = [\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0100_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'),\n",
    "]\n",
    "\n",
    "test_features_dirs = [\n",
    "    Path('/root/vs-gats-plaster/deepsort/hiergat_data/C0074_full_MP4_anno_for_labelling_done_ray_full_temporal_3s/features'),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def get_features_dirs_df(features_dirs):\n",
    "    feature_dirs_dict = []\n",
    "    for dirs in features_dirs:\n",
    "        for dir in dirs.iterdir():\n",
    "            action_label_str = str(dir).rsplit('_action_')[-1]\n",
    "            is_interpolated = 0\n",
    "            if '_interp' in action_label_str:\n",
    "                action_label_str = action_label_str.replace('_interp', '')\n",
    "                is_interpolated = 1\n",
    "            action_label = int(action_label_str)\n",
    "            action_class = action_classes[action_label]\n",
    "            try:\n",
    "                new_action_label = new_action_classes.index(action_class)\n",
    "            except ValueError:\n",
    "                new_action_label = -1\n",
    "            feature_dirs_dict.append({\n",
    "                'base_dir': str(dirs),\n",
    "                'dir': str(dir),\n",
    "                'action_label': action_label,\n",
    "                'new_action_label': new_action_label,\n",
    "                'is_interpolated': is_interpolated,\n",
    "            })\n",
    "        \n",
    "    feature_dirs_df = pd.DataFrame(feature_dirs_dict)\n",
    "    feature_dirs_df = feature_dirs_df[feature_dirs_df['new_action_label'] != -1] # remove rows where action_label == -1\n",
    "    return feature_dirs_df\n",
    "\n",
    "train_feature_dirs_df = get_features_dirs_df(train_features_dirs)\n",
    "val_feature_dirs_df = get_features_dirs_df(val_features_dirs)\n",
    "test_feature_dirs_df = get_features_dirs_df(test_features_dirs)\n",
    "\n",
    "print(\"Training set action label counts:\")\n",
    "print(train_feature_dirs_df['new_action_label'].value_counts())\n",
    "print()\n",
    "print(\"Validation set action label counts:\")\n",
    "print(val_feature_dirs_df['new_action_label'].value_counts())\n",
    "print()\n",
    "print(\"Testing set action label counts:\")\n",
    "print(test_feature_dirs_df['new_action_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = train_feature_dirs_df[(train_feature_dirs_df['new_action_label'] == 2)]\n",
    "train_df_0 = train_feature_dirs_df[(train_feature_dirs_df['new_action_label'] == 0) & (train_feature_dirs_df['is_interpolated'] == 0)]\n",
    "\n",
    "# Sample a specified number of rows per group according to a size mapping\n",
    "def sample_by_group_size(df, groupby_col, size_map, seed=42):\n",
    "    return (\n",
    "        df\n",
    "        .groupby(groupby_col, group_keys=False)[df.columns]\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=size_map[x.name],\n",
    "            random_state=seed,\n",
    "        ))\n",
    "    )\n",
    "\n",
    "# Downsample training data for label 3\n",
    "train_df_3 = train_feature_dirs_df[(train_feature_dirs_df['new_action_label'] == 3) & (train_feature_dirs_df['is_interpolated'] == 0)]\n",
    "# print(train_df_3.groupby('base_dir').size()) # Sanity Check\n",
    "train_size_map_3 = {\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_3s/features'     : 65,   # 130 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'     : 65,   # 240 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features'     : 65,   # 200 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'     : 65,   # 205 -> 65\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features'    : 2,    # 2   -> 2\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'     : 4,    # 4   -> 4\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features'   : 65,   # 147 -> 65 \n",
    "}                                                                                                                                   # Total: 331\n",
    "train_df_3_downsampled = sample_by_group_size(train_df_3, 'base_dir', train_size_map_3)\n",
    "\n",
    "# Downsample training data for label 1\n",
    "train_df_1 = train_feature_dirs_df[(train_feature_dirs_df['new_action_label'] == 1) & (train_feature_dirs_df['is_interpolated'] == 0)]\n",
    "# print(train_df_1.groupby('base_dir').size()) # Sanity Check\n",
    "train_size_map_1 = {\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features'     : 9,    # 56  -> 9\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features'     : 10,   # 181 -> 10\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features'     : 10,   # 274 -> 10\n",
    "                                                                                                                                    # Total: 29\n",
    "}\n",
    "train_df_1_downsampled = sample_by_group_size(train_df_1, 'base_dir', train_size_map_1)\n",
    "\n",
    "# Concatenate downsampled and original label subsets to form the downsampled training set\n",
    "train_feature_dirs_df_downsampled = pd.concat((train_df_0, train_df_1_downsampled, train_df_2, train_df_3_downsampled))\n",
    "\n",
    "# Sanity Check\n",
    "print(\"Downsampled training set action label counts:\")\n",
    "print(train_feature_dirs_df_downsampled['new_action_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_dirs = train_feature_dirs_df_downsampled['dir'].tolist()\n",
    "val_feature_dirs   = val_feature_dirs_df['dir'].tolist()\n",
    "test_feature_dirs  = test_feature_dirs_df['dir'].tolist()\n",
    "\n",
    "train_data = create_data(train_feature_dirs, action_classes, new_action_classes)\n",
    "val_data = create_data(val_feature_dirs, action_classes, new_action_classes)\n",
    "test_data = create_data(test_feature_dirs, action_classes, new_action_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_human_features_list,\n",
    "    train_human_boxes_list,\n",
    "    train_human_poses_list,\n",
    "    train_object_features_list,\n",
    "    train_object_boxes_list,\n",
    "    train_gt_list,\n",
    "    train_xs_steps,\n",
    ") = train_data\n",
    "\n",
    "train_human_features_list = np.stack(train_human_features_list)[:, 0, :, :]     # (N, seq_len, 1024)\n",
    "train_human_boxes_list = np.stack(train_human_boxes_list)[:, 0, :, :]           # (N, seq_len, 4)\n",
    "train_human_poses_list = np.stack(train_human_poses_list)[:, 0, :, :]           # (N, seq_len, 17, 2)\n",
    "train_object_features_list = np.stack(train_object_features_list)[:, :, 0, :]   # (N, seq_len, 1024)\n",
    "train_object_boxes_list = np.stack(train_object_boxes_list)[:, :, 0, :]         # (N, seq_len, 4)\n",
    "train_gt_list = np.array([gt['Human1'][0] for gt in train_gt_list])             # (N,)\n",
    "# train_xs_steps                                                                # (N,)\n",
    "\n",
    "# Sanity Check\n",
    "print(\"train_human_features_list.shape:\", train_human_features_list.shape)\n",
    "print(\"train_human_boxes_list.shape:\", train_human_boxes_list.shape)\n",
    "print(\"train_human_poses_list.shape:\", train_human_poses_list.shape)\n",
    "print(\"train_object_features_list.shape:\", train_object_features_list.shape)\n",
    "print(\"train_object_boxes_list.shape:\", train_object_boxes_list.shape)\n",
    "print(\"train_gt_list.shape:\", train_gt_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_or_crop(seq, target_len):\n",
    "#     seq_len = seq.shape[0]\n",
    "#     if seq_len == target_len:\n",
    "#         return seq\n",
    "#     elif seq_len > target_len:\n",
    "#         return seq[:target_len]\n",
    "#     else:\n",
    "#         pad_shape = (target_len - seq_len,) + seq.shape[1:]\n",
    "#         pad = np.zeros(pad_shape, dtype=seq.dtype)\n",
    "#         return np.concatenate([seq, pad], axis=0)\n",
    "\n",
    "# def to_float(x): return x.astype(np.float32, copy=False)\n",
    "# def add_noise(x, scale=0.01): return to_float(x) + np.random.normal(0, scale, x.shape).astype(np.float32)\n",
    "\n",
    "# def translate_jitter(boxes, poses, max_shift=0.05):\n",
    "#     boxes, poses = to_float(boxes), to_float(poses)\n",
    "#     shift = np.random.uniform(-max_shift, max_shift, size=(1, 2)).astype(np.float32)\n",
    "#     boxes[..., :2] += shift\n",
    "#     poses += shift\n",
    "#     return boxes, poses\n",
    "\n",
    "# def scale_zoom(boxes, poses, scale_range=(0.9, 1.1)):\n",
    "#     boxes, poses = to_float(boxes), to_float(poses)\n",
    "#     scale = np.random.uniform(*scale_range)\n",
    "#     boxes[..., :2] *= scale\n",
    "#     boxes[..., 2:] *= scale\n",
    "#     poses *= scale\n",
    "#     return boxes, poses\n",
    "\n",
    "# def horizontal_flip(boxes, poses, img_width=1.0):\n",
    "#     boxes, poses = to_float(boxes), to_float(poses)\n",
    "#     boxes[..., 0] = img_width - boxes[..., 0] - boxes[..., 2]\n",
    "#     poses[..., 0] = img_width - poses[..., 0]\n",
    "#     return boxes, poses\n",
    "\n",
    "# def time_warp_stretch(seq, stretch_factor_range=(1.1, 1.3)):\n",
    "#     seq = to_float(seq)\n",
    "#     factor = np.random.uniform(*stretch_factor_range)\n",
    "#     new_len = int(seq.shape[0] * factor)\n",
    "#     new_idx = np.linspace(0, seq.shape[0]-1, new_len).astype(np.int32)\n",
    "#     return seq[new_idx, ...]\n",
    "\n",
    "# def time_warp_compress(seq, compress_factor_range=(0.7, 0.9)):\n",
    "#     seq = to_float(seq)\n",
    "#     factor = np.random.uniform(*compress_factor_range)\n",
    "#     new_len = int(seq.shape[0] * factor)\n",
    "#     new_idx = np.linspace(0, seq.shape[0]-1, new_len).astype(np.int32)\n",
    "#     return seq[new_idx, ...]\n",
    "\n",
    "# from scipy.interpolate import interp1d\n",
    "# def interpolate_seq(seq, factor_range=(0.7, 1.3)):\n",
    "#     \"\"\"\n",
    "#     Interpolates the sequence to a new length using linear interpolation.\n",
    "#     Unlike stretch/compress which samples by index, this does real interpolation.\n",
    "#     \"\"\"\n",
    "#     seq = to_float(seq)\n",
    "#     old_len = seq.shape[0]\n",
    "#     factor = np.random.uniform(*factor_range)\n",
    "#     new_len = int(old_len * factor)\n",
    "#     old_time = np.linspace(0, 1, old_len)\n",
    "#     new_time = np.linspace(0, 1, new_len)\n",
    "#     # interp1d supports multi-d interpolation along axis 0\n",
    "#     f = interp1d(old_time, seq, axis=0, kind='linear')\n",
    "#     new_seq = f(new_time)\n",
    "#     return new_seq\n",
    "\n",
    "# def mixup(x1, x2, alpha=0.2):\n",
    "#     x1, x2 = to_float(x1), to_float(x2)\n",
    "#     lam = np.random.beta(alpha, alpha)\n",
    "#     return lam * x1 + (1 - lam) * x2\n",
    "\n",
    "# def augment_by_label(\n",
    "#     label,\n",
    "#     train_human_features_list,\n",
    "#     train_human_boxes_list,\n",
    "#     train_human_poses_list,\n",
    "#     train_object_features_list,\n",
    "#     train_object_boxes_list,\n",
    "#     train_gt_list,\n",
    "#     train_xs_steps,\n",
    "#     seed=42,\n",
    "#     aug_samples_per_func=None\n",
    "# ):\n",
    "#     np.random.seed(seed)\n",
    "#     default_aug = 5\n",
    "#     if aug_samples_per_func is None:\n",
    "#         aug_samples_per_func = {}\n",
    "\n",
    "#     aug_samples_per_func = {k: aug_samples_per_func.get(k, default_aug) for k in [\n",
    "#         'noise_spatial', 'translate', 'scale', 'flip',\n",
    "#         'noise_temporal', 'stretch', 'compress', 'interpolate',\n",
    "#         'noise_feature', 'mixup'\n",
    "#     ]}\n",
    "\n",
    "#     print(f\"Augmenting for label {label}...\")\n",
    "\n",
    "#     target_indices = np.where(train_gt_list == label)[0]\n",
    "#     if len(target_indices) == 0:\n",
    "#         print(f\"No samples found for label {label}. Skipping.\")\n",
    "#         return (\n",
    "#             train_human_features_list,\n",
    "#             train_human_boxes_list,\n",
    "#             train_human_poses_list,\n",
    "#             train_object_features_list,\n",
    "#             train_object_boxes_list,\n",
    "#             train_gt_list,\n",
    "#             train_xs_steps\n",
    "#         )\n",
    "\n",
    "#     human_features = train_human_features_list[target_indices]\n",
    "#     human_boxes = train_human_boxes_list[target_indices]\n",
    "#     human_poses = train_human_poses_list[target_indices]\n",
    "#     object_features = train_object_features_list[target_indices]\n",
    "#     object_boxes = train_object_boxes_list[target_indices]\n",
    "#     xs_steps = train_xs_steps[target_indices]\n",
    "#     gts = train_gt_list[target_indices]\n",
    "\n",
    "#     original_seq_len = human_features.shape[1]\n",
    "\n",
    "#     augmented = dict(\n",
    "#         human_features=[], human_boxes=[], human_poses=[],\n",
    "#         object_features=[], object_boxes=[], gts=[], xs_steps=[]\n",
    "#     )\n",
    "\n",
    "#     def append_augmented(hf, hb, hp, of, ob, gt, step):\n",
    "#         augmented['human_features'].append(hf)\n",
    "#         augmented['human_boxes'].append(hb)\n",
    "#         augmented['human_poses'].append(hp)\n",
    "#         augmented['object_features'].append(of)\n",
    "#         augmented['object_boxes'].append(ob)\n",
    "#         augmented['gts'].append(gt)\n",
    "#         augmented['xs_steps'].append(step)\n",
    "\n",
    "#     def sample_and_apply(func, n_times):\n",
    "#         for _ in range(n_times):\n",
    "#             i = np.random.randint(0, len(gts))\n",
    "#             hf, hb, hp, of, ob, gt, step = map(deepcopy,\n",
    "#                 [human_features[i], human_boxes[i], human_poses[i],\n",
    "#                  object_features[i], object_boxes[i], gts[i], xs_steps[i]])\n",
    "\n",
    "#             if func == 'noise_spatial':\n",
    "#                 hb, hp = add_noise(hb), add_noise(hp)\n",
    "#             elif func == 'translate':\n",
    "#                 hb, hp = translate_jitter(hb, hp)\n",
    "#             elif func == 'scale':\n",
    "#                 hb, hp = scale_zoom(hb, hp)\n",
    "#             elif func == 'flip':\n",
    "#                 hb, hp = horizontal_flip(hb, hp)\n",
    "#             elif func == 'noise_temporal':\n",
    "#                 hf, of = add_noise(hf), add_noise(of)\n",
    "#             elif func == 'stretch':\n",
    "#                 hf = time_warp_stretch(hf)\n",
    "#                 of = time_warp_stretch(of)\n",
    "#             elif func == 'compress':\n",
    "#                 hf = time_warp_compress(hf)\n",
    "#                 of = time_warp_compress(of)\n",
    "#             elif func == 'interpolate':\n",
    "#                 hf = interpolate_seq(hf)\n",
    "#                 of = interpolate_seq(of)\n",
    "#                 hb = interpolate_seq(hb)\n",
    "#                 hp = interpolate_seq(hp)\n",
    "#                 ob = interpolate_seq(ob)\n",
    "#             elif func == 'noise_feature':\n",
    "#                 hf, of = add_noise(hf), add_noise(of)\n",
    "#             elif func == 'mixup':\n",
    "#                 j = np.random.randint(0, len(gts))\n",
    "#                 hf = mixup(hf, human_features[j])\n",
    "#                 of = mixup(of, object_features[j])\n",
    "\n",
    "#             hf = pad_or_crop(hf, original_seq_len)\n",
    "#             of = pad_or_crop(of, original_seq_len)\n",
    "#             hb = pad_or_crop(hb, original_seq_len)\n",
    "#             hp = pad_or_crop(hp, original_seq_len)\n",
    "#             ob = pad_or_crop(ob, original_seq_len)\n",
    "\n",
    "#             append_augmented(hf, hb, hp, of, ob, gt, step)\n",
    "\n",
    "#     for func_name, n_samples in aug_samples_per_func.items():\n",
    "#         sample_and_apply(func_name, n_samples)\n",
    "\n",
    "#     aug_hf = np.stack(augmented['human_features'])\n",
    "#     aug_hb = np.stack(augmented['human_boxes'])\n",
    "#     aug_hp = np.stack(augmented['human_poses'])\n",
    "#     aug_of = np.stack(augmented['object_features'])\n",
    "#     aug_ob = np.stack(augmented['object_boxes'])\n",
    "#     aug_gt = np.array(augmented['gts'])\n",
    "#     aug_xs = np.array(augmented['xs_steps'])\n",
    "\n",
    "#     train_human_features_list = np.concatenate([train_human_features_list, aug_hf], axis=0)\n",
    "#     train_human_boxes_list = np.concatenate([train_human_boxes_list, aug_hb], axis=0)\n",
    "#     train_human_poses_list = np.concatenate([train_human_poses_list, aug_hp], axis=0)\n",
    "#     train_object_features_list = np.concatenate([train_object_features_list, aug_of], axis=0)\n",
    "#     train_object_boxes_list = np.concatenate([train_object_boxes_list, aug_ob], axis=0)\n",
    "#     train_gt_list = np.concatenate([train_gt_list, aug_gt], axis=0)\n",
    "#     train_xs_steps = np.concatenate([train_xs_steps, aug_xs], axis=0)\n",
    "\n",
    "#     print(f\"Done! Total: {np.sum(train_gt_list == label)}\")\n",
    "\n",
    "#     return (\n",
    "#         train_human_features_list,\n",
    "#         train_human_boxes_list,\n",
    "#         train_human_poses_list,\n",
    "#         train_object_features_list,\n",
    "#         train_object_boxes_list,\n",
    "#         train_gt_list,\n",
    "#         train_xs_steps\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Augment training data for label 2 (Total: 52)\n",
    "# train_human_features_list, train_human_boxes_list, train_human_poses_list, \\\n",
    "# train_object_features_list, train_object_boxes_list, train_gt_list, train_xs_steps = augment_by_label(\n",
    "#     label=2,\n",
    "#     train_human_features_list=train_human_features_list,\n",
    "#     train_human_boxes_list=train_human_boxes_list,\n",
    "#     train_human_poses_list=train_human_poses_list,\n",
    "#     train_object_features_list=train_object_features_list,\n",
    "#     train_object_boxes_list=train_object_boxes_list,\n",
    "#     train_gt_list=train_gt_list,\n",
    "#     train_xs_steps=train_xs_steps,\n",
    "#     aug_samples_per_func={\n",
    "#         # Spatial Augmentation\n",
    "#         'noise_spatial': 0,\n",
    "#         'translate': 0,\n",
    "#         'scale': 0,\n",
    "#         'flip': 0,\n",
    "        \n",
    "#         # Temporal Augmentation\n",
    "#         'noise_temporal': 0,\n",
    "#         'stretch': 0,\n",
    "#         'compress': 0,\n",
    "        \n",
    "#         # Feature-Space Augmentation\n",
    "#         'noise_feature': 0,\n",
    "#         'mixup': 0,\n",
    "        \n",
    "#         # Misc.\n",
    "#         'interpolate': 52,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # Sanity Check\n",
    "# print(pd.DataFrame(train_gt_list).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_human_features_list = np.stack(train_human_features_list)[:, np.newaxis, :, :]          # (N, 1, seq_len, 1024)\n",
    "train_human_boxes_list = np.stack(train_human_boxes_list)[:, np.newaxis, :, :]                # (N, 1, seq_len, 4)\n",
    "train_human_poses_list = np.stack(train_human_poses_list)[:, np.newaxis, :, :]                # (N, 1, seq_len, 17, 2)\n",
    "train_object_features_list = np.stack(train_object_features_list)[:, :, np.newaxis, :]        # (N, seq_len, 1, 1024)\n",
    "train_object_boxes_list = np.stack(train_object_boxes_list)[:, :, np.newaxis, :]              # (N, seq_len, 1, 4)\n",
    "train_gt_list = [{'Human1': [gt]*train_human_features_list.shape[2]} for gt in train_gt_list] # N\n",
    "\n",
    "# Sanity Check\n",
    "print(\"train_human_features_list:\", train_human_features_list.shape)\n",
    "print(\"train_human_boxes_list:\", train_human_boxes_list.shape)\n",
    "print(\"train_human_poses_list:\", train_human_poses_list.shape)\n",
    "print(\"train_object_features_list:\", train_object_features_list.shape)\n",
    "print(\"train_object_boxes_list:\", train_object_boxes_list.shape)\n",
    "print(\"train_xs_steps:\", len(train_xs_steps))\n",
    "print(\"train_gt_list:\", len(train_gt_list))\n",
    "\n",
    "train_data = (\n",
    "    train_human_features_list, \n",
    "    train_human_boxes_list, \n",
    "    train_human_poses_list,\n",
    "    train_object_features_list, \n",
    "    train_object_boxes_list, \n",
    "    train_gt_list, \n",
    "    train_xs_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, scalers, _ = create_data_loader(\n",
    "    *train_data, \n",
    "    model_name, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    scaling_strategy=scaling_strategy, \n",
    "    sigma=sigma,\n",
    "    downsampling=downsampling,\n",
    ")\n",
    "val_loader, _, _ = create_data_loader(\n",
    "    *val_data, \n",
    "    model_name, \n",
    "    batch_size=len(val_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "test_loader, _, _ = create_data_loader(\n",
    "    *test_data, \n",
    "    model_name, \n",
    "    batch_size=len(test_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "input_size = input_size_from_data_loader(train_loader, model_name, model_input_type)\n",
    "data_info = {'input_size': input_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_creation_args = cfg.models.parameters\n",
    "model_creation_args = {**data_info, **model_creation_args.__dict__}\n",
    "dataset_name = cfg.data.name\n",
    "model_creation_args['num_classes'] = (num_classes, None)\n",
    "device = 'cuda' if torch.cuda.is_available() and cfg.resources.use_gpu else 'cpu'\n",
    "model = TGGCN(feat_dim=1024, **model_creation_args).to(device)\n",
    "if misc_dict.get('pretrained', False) and misc_dict.get('pretrained_path') is not None:\n",
    "    state_dict = load_model_weights(misc_dict['pretrained_path'])\n",
    "    model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=cfg.models.optimization.learning_rate)\n",
    "criterion, loss_names = select_loss(model_name, model_input_type, dataset_name, cfg=cfg)\n",
    "mtll_model = None\n",
    "if misc_dict.get('multi_task_loss_learner', False):\n",
    "    loss_types = select_loss_types(model_name, dataset_name, cfg=cfg)\n",
    "    mask = select_loss_learning_mask(model_name, dataset_name, cfg=cfg)\n",
    "    mtll_model = MultiTaskLossLearner(loss_types=loss_types, mask=mask).to(device)\n",
    "    optimizer.add_param_group({'params': mtll_model.parameters()})\n",
    "# Some config + model training\n",
    "tensorboard_log_dir = cfg.models.logging.root_log_dir\n",
    "checkpoint_name = cfg.models.logging.checkpoint_name\n",
    "fetch_model_data = select_model_data_fetcher(model_name, model_input_type,\n",
    "                                             dataset_name=dataset_name, **{**misc_dict, **cfg.models.parameters.__dict__})\n",
    "feed_model_data = select_model_data_feeder(model_name, model_input_type, dataset_name=dataset_name, **misc_dict)\n",
    "num_main_losses = decide_num_main_losses(model_name, dataset_name, {**misc_dict, **cfg.models.parameters.__dict__})\n",
    "checkpoint = train(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    cfg.models.optimization.epochs, \n",
    "    device, \n",
    "    loss_names,\n",
    "    clip_gradient_at=cfg.models.optimization.clip_gradient_at,\n",
    "    fetch_model_data=fetch_model_data, feed_model_data=feed_model_data,\n",
    "    val_loader=val_loader, \n",
    "    mtll_model=mtll_model, \n",
    "    num_main_losses=num_main_losses,\n",
    "    tensorboard_log_dir=tensorboard_log_dir, \n",
    "    checkpoint_name=checkpoint_name,\n",
    ")\n",
    "# Logging\n",
    "if cfg.models.logging.log_dir is not None:\n",
    "    log_dir = cfg.models.logging.log_dir\n",
    "    checkpoint['scalers'] = scalers\n",
    "    save_checkpoint(log_dir, checkpoint, checkpoint_name=checkpoint_name, include_timestamp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from predict import match_shape, match_att_shape\n",
    "\n",
    "inspect_model = False\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets, attentions = [], [], []\n",
    "for i, dataset in enumerate(test_loader):\n",
    "    data, target = fetch_model_data(dataset, device=device)\n",
    "    with torch.no_grad():\n",
    "        output = feed_model_data(model, data)\n",
    "    if inspect_model:\n",
    "        output, attention_scores = output\n",
    "        attention_scores = [att_score[:, 0] for att_score in attention_scores]\n",
    "    if num_main_losses is not None:\n",
    "        output = output[-num_main_losses:]\n",
    "        target = target[-num_main_losses:]\n",
    "    if downsampling > 1:\n",
    "        for i, (out, tgt) in enumerate(zip(output, target)):\n",
    "            if out.ndim != 4:\n",
    "                raise RuntimeError(f'Number of dimensions for output is {out.ndim}')\n",
    "            out = torch.repeat_interleave(out, repeats=downsampling, dim=-2)\n",
    "            out = match_shape(out, tgt)\n",
    "            output[i] = out\n",
    "        if inspect_model:\n",
    "            a_target = target[0]\n",
    "            attention_scores = [torch.repeat_interleave(att_score, repeats=downsampling, dim=-2)\n",
    "                                for att_score in attention_scores]\n",
    "            attention_scores = [match_att_shape(att_score, a_target) for att_score in attention_scores]\n",
    "            attentions.append(attention_scores)\n",
    "    outputs += output\n",
    "    targets += target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(outputs[0], dim=1)\n",
    "y_pred = y_pred.squeeze(-1).mode(dim=1).values.cpu().numpy()\n",
    "y_true = targets[0].squeeze(-1).mode(dim=1).values.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(new_action_classes))))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=new_action_classes, yticklabels=new_action_classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with Last Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = outputs[0][:, :, -1, 0].argmax(dim=1).cpu().numpy()\n",
    "y_true = targets[0][:, -1, 0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(new_action_classes))))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=new_action_classes, yticklabels=new_action_classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Ground Truth\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch271",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
