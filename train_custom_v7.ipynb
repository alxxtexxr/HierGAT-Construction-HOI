{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: `py39_torch271`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dacite import from_dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pyrutils.torch.train_utils import train, save_checkpoint\n",
    "from pyrutils.torch.multi_task import MultiTaskLossLearner\n",
    "from vhoi.data_loading import (\n",
    "    input_size_from_data_loader, \n",
    "    select_model_data_feeder, \n",
    "    select_model_data_fetcher,\n",
    ")\n",
    "from vhoi.data_loading_custom import (\n",
    "    create_data,\n",
    "    create_data_loader\n",
    ")\n",
    "from vhoi.losses_custom_v2 import (\n",
    "    select_loss, \n",
    "    decide_num_main_losses, \n",
    "    select_loss_types, \n",
    "    select_loss_learning_mask,\n",
    ")\n",
    "from vhoi.models import load_model_weights\n",
    "from vhoi.models_custom_v2 import TGGCN\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)   # Python的随机性\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "np.random.seed(seed)   # numpy的随机性\n",
    "torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True   # 选择确定性算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictMixin:\n",
    "    def get(self, key, default_value=None):\n",
    "        return getattr(self, key, default_value)\n",
    "\n",
    "    def as_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "@dataclass\n",
    "class Resources(DictMixin):\n",
    "    use_gpu: bool\n",
    "    num_threads: int\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata(DictMixin):\n",
    "    model_name: str\n",
    "    input_type: str\n",
    "\n",
    "@dataclass\n",
    "class ModelParameters(DictMixin):\n",
    "    add_segment_length: int\n",
    "    add_time_position: int\n",
    "    time_position_strategy: str\n",
    "    positional_encoding_style: str\n",
    "    attention_style: str\n",
    "    bias: bool\n",
    "    cat_level_states: int\n",
    "    discrete_networks_num_layers: int\n",
    "    discrete_optimization_strategy: str\n",
    "    filter_discrete_updates: bool\n",
    "    gcn_node: int\n",
    "    hidden_size: int\n",
    "    message_humans_to_human: bool\n",
    "    message_human_to_objects: bool\n",
    "    message_objects_to_human: bool\n",
    "    message_objects_to_object: bool\n",
    "    message_geometry_to_objects: bool\n",
    "    message_geometry_to_human: bool\n",
    "    message_segment: bool\n",
    "    message_type: str\n",
    "    message_granularity: str\n",
    "    message_aggregation: str\n",
    "    object_segment_update_strategy: str\n",
    "    share_level_mlps: int\n",
    "    update_segment_threshold: float\n",
    "\n",
    "@dataclass\n",
    "class ModelOptimization(DictMixin):\n",
    "    batch_size: int\n",
    "    clip_gradient_at: float\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    val_fraction: float\n",
    "\n",
    "@dataclass\n",
    "class BudgetLoss(DictMixin):\n",
    "    add: bool\n",
    "    human_weight: float\n",
    "    object_weight: float\n",
    "\n",
    "@dataclass\n",
    "class SegmentationLoss(DictMixin):\n",
    "    add: bool\n",
    "    pretrain: bool\n",
    "    sigma: float\n",
    "    weight: float\n",
    "\n",
    "@dataclass\n",
    "class ModelMisc(DictMixin):\n",
    "    anticipation_loss_weight: float\n",
    "    budget_loss: BudgetLoss\n",
    "    first_level_loss_weight: float\n",
    "    impose_segmentation_pattern: int\n",
    "    input_human_segmentation: bool\n",
    "    input_object_segmentation: bool\n",
    "    make_attention_distance_based: bool\n",
    "    multi_task_loss_learner: bool\n",
    "    pretrained: bool\n",
    "    pretrained_path: Optional[str]\n",
    "    segmentation_loss: SegmentationLoss\n",
    "\n",
    "@dataclass\n",
    "class ModelLogging(DictMixin):\n",
    "    root_log_dir: str\n",
    "    checkpoint_name: str\n",
    "    log_dir: str\n",
    "\n",
    "@dataclass\n",
    "class Models(DictMixin):\n",
    "    metadata: ModelMetadata\n",
    "    parameters: ModelParameters\n",
    "    optimization: ModelOptimization\n",
    "    misc: ModelMisc\n",
    "    logging: ModelLogging\n",
    "\n",
    "@dataclass\n",
    "class Data(DictMixin):\n",
    "    name: str\n",
    "    path: str\n",
    "    path_zarr: str\n",
    "    path_obb_zarr: str\n",
    "    path_hbb_zarr: str\n",
    "    path_hps_zarr: str\n",
    "    cross_validation_test_subject: str\n",
    "    scaling_strategy: Optional[str]\n",
    "    downsampling: int\n",
    "\n",
    "@dataclass\n",
    "class Config(DictMixin):\n",
    "    resources: Resources\n",
    "    models: Models\n",
    "    data: Data\n",
    "    \n",
    "metadata_dict = {\n",
    "    \"model_name\": \"2G-GCN\",\n",
    "    \"input_type\": \"multiple\"\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    \"add_segment_length\": 0,  # length of the segment to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"add_time_position\": 0,  # absolute time position to the segment-level rnn. 0 is off and 1 is on.\n",
    "    \"time_position_strategy\": \"s\",  # input time position to segment [s] or discrete update [u].\n",
    "    \"positional_encoding_style\": \"e\",  # e [embedding] or p [periodic].\n",
    "    \"attention_style\": \"v3\",  # v1 [concat], v2 [dot-product], v3 [scaled_dot-product], v4 [general]\n",
    "    \"bias\": True,\n",
    "    \"cat_level_states\": 0,  # concatenate first and second level hidden states for predictors MLPs.\n",
    "    \"discrete_networks_num_layers\": 1,  # depth of the state change detector MLP.\n",
    "    \"discrete_optimization_strategy\": \"gs\",  # straight-through [st] or gumbel-sigmoid [gs]\n",
    "    \"filter_discrete_updates\": False,  # maxima filter for soft output of state change detector.\n",
    "    \"gcn_node\": 25,  # custom, original: 19 for cad120, 30 for bimanual, 26 for mphoi\n",
    "    \"hidden_size\": 512,  # 512 for cad120 & mphoi; 64 for bimanual\n",
    "    \"message_humans_to_human\": False, # custom, original: True\n",
    "    \"message_human_to_objects\": True,\n",
    "    \"message_objects_to_human\": True,\n",
    "    \"message_objects_to_object\": False, # custom, original: True\n",
    "    \"message_geometry_to_objects\": True,\n",
    "    \"message_geometry_to_human\": True,  # custom, original: False\n",
    "    \"message_segment\": True,\n",
    "    \"message_type\": \"v2\",  # v1 [relational] or v2 [non-relational]\n",
    "    \"message_granularity\": \"v1\",  # v1 [generic] or v2 [specific]\n",
    "    \"message_aggregation\": \"att\",  # mean_pooling [mp] or attention [att]\n",
    "    \"object_segment_update_strategy\": \"ind\",  # same_as_human [sah], independent [ind], or conditional_on_human [coh]\n",
    "    \"share_level_mlps\": 0,  # whether to share [1] or not [0] the prediction MLPs of the levels.\n",
    "    \"update_segment_threshold\": 0.5  # [0.0, 1.0)\n",
    "}\n",
    "\n",
    "optimization_dict = {\n",
    "    \"batch_size\": 32,  # mphoi:8; cad120:16; bimanual: 32\n",
    "    \"clip_gradient_at\": 0.0,\n",
    "    \"epochs\": 10, # custom, original: cad120 & mphoi:40; bimanual: 60\n",
    "    \"learning_rate\": 1e-4,  # mphoi:1e-4; cad120 & bimanual:1e-3\n",
    "    \"val_fraction\": 0.2,\n",
    "}\n",
    "\n",
    "data_dict = {\n",
    "    \"name\": \"mphoi\",\n",
    "    \"path\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_ground_truth_labels.json\",\n",
    "    \"path_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/faster_rcnn.zarr\",\n",
    "    \"path_obb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/object_bounding_boxes.zarr\",\n",
    "    \"path_hbb_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_bounding_boxes.zarr\",\n",
    "    \"path_hps_zarr\": f\"{os.getcwd()}/data/MPHOI/MPHOI/mphoi_derived_features/human_pose.zarr\",\n",
    "    \"cross_validation_test_subject\": \"Subject14\",  # Subject45, Subject25, Subject14\n",
    "    \"scaling_strategy\": None,  # null or \"standard\"\n",
    "    \"downsampling\": 1 # custom, original: 3, 1 = full FPS, 2 = half FPS, ...\n",
    "}\n",
    "\n",
    "# root_log_dir = f\"{os.getcwd()}/outputs_hiergat/{data_dict['name']}/{metadata_dict['model_name']}\"\n",
    "root_log_dir = f\"{os.getcwd()}/outputs_hiergat/custom\"\n",
    "checkpoint_name = (\n",
    "    f\"hs{parameters_dict['hidden_size']}_e{optimization_dict['epochs']}_bs{optimization_dict['batch_size']}_\"\n",
    "    f\"lr{optimization_dict['learning_rate']}_{parameters_dict['update_segment_threshold']}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    ")\n",
    "log_dir = f\"{root_log_dir}/{checkpoint_name}\"\n",
    "print(\"Log directory:\", log_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "cfg_dict = {\n",
    "    \"resources\": {\n",
    "        \"use_gpu\": True,\n",
    "        \"num_threads\": 32\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"metadata\": metadata_dict,\n",
    "        \"parameters\": parameters_dict,\n",
    "        \"optimization\": optimization_dict,\n",
    "        \"misc\": {\n",
    "            \"anticipation_loss_weight\": 1.0,\n",
    "            \"budget_loss\": {\n",
    "                \"add\": False,\n",
    "                \"human_weight\": 1.0,\n",
    "                \"object_weight\": 1.0\n",
    "            },\n",
    "            \"first_level_loss_weight\": 0.0,  # if positive, first level does frame-level prediction\n",
    "            \"impose_segmentation_pattern\": 1,  # 0 [no pattern], 1 [all ones]\n",
    "            \"input_human_segmentation\": False,  # (was \"flase\" in YAML, corrected here)\n",
    "            \"input_object_segmentation\": False,\n",
    "            \"make_attention_distance_based\": True,  # only meaningful if message_aggregation is attention\n",
    "            \"multi_task_loss_learner\": False,\n",
    "            \"pretrained\": False,  # unfortunately need two entries for checkpoint name\n",
    "            \"pretrained_path\": None,  # specified parameters must match pre-trained model\n",
    "            \"segmentation_loss\": {\n",
    "                \"add\": False,\n",
    "                \"pretrain\": False,\n",
    "                \"sigma\": 0.0,  # Gaussian smoothing\n",
    "                \"weight\": 1.0\n",
    "            }\n",
    "        },\n",
    "        \"logging\": {\n",
    "            \"root_log_dir\": root_log_dir,\n",
    "            \"checkpoint_name\": checkpoint_name,\n",
    "            \"log_dir\": log_dir\n",
    "        },\n",
    "    },\n",
    "    \"data\": data_dict,\n",
    "}\n",
    "\n",
    "cfg = from_dict(data_class=Config, data=cfg_dict)\n",
    "\n",
    "torch.set_num_threads(cfg.resources.num_threads)\n",
    "model_name, model_input_type = cfg.models.metadata.model_name, cfg.models.metadata.input_type\n",
    "batch_size, val_fraction = cfg.models.optimization.batch_size, cfg.models.optimization.val_fraction\n",
    "misc_dict = cfg.get('misc', default_value={})\n",
    "sigma = misc_dict.get('segmentation_loss', {}).get('sigma', 0.0)\n",
    "scaling_strategy = cfg.data.scaling_strategy\n",
    "downsampling = cfg.data.downsampling\n",
    "\n",
    "action_classes = [\n",
    "    # human\n",
    "    'supervise',        # 0\n",
    "    'collaborate with', # 1\n",
    "    'assist',           # 2\n",
    "    'lead',             # 3\n",
    "    'coordinate with',  # 4\n",
    "    'listen to',        # 5\n",
    "\n",
    "    # rebar\n",
    "    'tie',              # 6\n",
    "    'erect',            # 7\n",
    "    'prepare_rebar',    # 8\n",
    "    'transport',        # 9\n",
    "\n",
    "    # formwork\n",
    "    'install',          # 10\n",
    "    'prepare_formwork', # 11\n",
    "\n",
    "    # concrete\n",
    "    'pour',             # 12\n",
    "    'finish',           # 13\n",
    "\n",
    "    # equipment         \n",
    "    'use',              # 14\n",
    "    'carry',            # 15\n",
    "\n",
    "    # all\n",
    "    'inspect',          # 16\n",
    "    'no interaction',   # 17\n",
    "]\n",
    "new_action_classes = [\n",
    "    # rebar\n",
    "    'tie',              # 0\n",
    "    'erect',            # 1\n",
    "    'prepare_rebar',    # 2\n",
    "    'transport',        # 3\n",
    "\n",
    "    # equipment         \n",
    "    'use',              # 4\n",
    "    'carry',            # 5\n",
    "\n",
    "    # all\n",
    "    'inspect',          # 6\n",
    "    'no interaction',   # 7\n",
    "]\n",
    "# num_classes = len(action_classes)\n",
    "num_classes = len(new_action_classes)\n",
    "\n",
    "# ================================\n",
    "# Features for seq_len = 3 seconds\n",
    "# ================================\n",
    "train_features_dirs = [\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0071_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features', # {0: 130, 2: 90, 6: 105, 7: 223}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0074_full_MP4_anno_for_labelling_done_ray_full_temporal_3s/features', # {0: 124, 1: 25, 2: 11, 6: 55, 7: 133}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_3s/features', # {0: 130, 7: 65} \n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features', # {0: 240, 1: 5, 2: 56, 3: 1, 4: 6, 6: 2, 7: 34}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_3s/features', # {0: 200, 2: 181, 4: 27, 7: 84}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features', # {0: 205, 5: 8, 6: 134, 7: 236}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0090_full_MP4_anno_for_labelling_rizky_full_temporal_3s/features', # {}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_3s/features', # {0: 2, 1: 10, 5: 23, 7: 84}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_3s/features', # {0: 147, 6: 5, 7: 309}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0105_full_MP4_anno_for_labelling_full_temporal_3s/features', # {}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0106_full_MP4_anno_for_labelling_full_temporal_3s/features', # {}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0108_full_MP4_anno_for_labelling_fixed_full_temporal_3s/features', # {1: 26, 3: 21, 4: 35, 6: 16, 7: 157}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0110_full_MP4_anno_for_labelling_full_temporal_3s/features', # {2: 23, 3: 29, 4: 25, 7: 264}\n",
    "    \n",
    "    # Previously used as validation\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0100_full_MP4_anno_for_labelling_done_putu_full_temporal_3s/features', # {0: 23, 2: 10, 4: 2, 7: 191}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0109_full_MP4_anno_for_labelling_full_temporal_3s/features', # {1: 1541, 3: 38, 4: 35}       \n",
    "    \n",
    "    # Previously used as testing\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0104_full_MP4_anno_for_labelling_full_temporal_3s/features', # {}\n",
    "    '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_3s/features', # {0: 4, 2: 274, 5: 5, 6: 6, 7: 25}\n",
    "]\n",
    "\n",
    "# ================================\n",
    "# Features for seq_len = 2 seconds\n",
    "# ================================\n",
    "# train_features_dirs = [\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0071_full_MP4_anno_for_labelling_done_faridz_full_temporal_2s/features', # {0: 276, 2: 193, 6: 250}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0074_full_MP4_anno_for_labelling_done_ray_full_temporal_2s/features',    # {0: 256, 1: 40, 2: 20, 4: 1, 6: 114}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_2s/features',   # {0: 254}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_2s/features',   # {0: 451, 1: 15, 2: 118, 3: 2, 4: 12, 6: 4}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_2s/features',   # {0: 365, 2: 291, 4: 44}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_2s/features',   # {0: 429, 5: 13, 6: 292}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0090_full_MP4_anno_for_labelling_rizky_full_temporal_2s/features',       # {0: 3}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_2s/features',  # {0: 7, 1: 31, 4: 2, 5: 61}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_2s/features', # {0: 258, 2: 9, 6: 13}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0105_full_MP4_anno_for_labelling_full_temporal_2s/features',             # {}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0106_full_MP4_anno_for_labelling_full_temporal_2s/features',             # {}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0108_full_MP4_anno_for_labelling_fixed_full_temporal_2s/features',       # {1: 50, 3: 40, 4: 55, 6: 31}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0110_full_MP4_anno_for_labelling_full_temporal_2s/features',             # {1: 4, 2: 47, 3: 56, 4: 43}\n",
    "    \n",
    "#     # Previously used as validation\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0100_full_MP4_anno_for_labelling_done_putu_full_temporal_2s/features', # {0: 41, 2: 18, 3: 1, 4: 3, 5: 1}\n",
    "#     # '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0109_full_MP4_anno_for_labelling_full_temporal_2s/features', # OLD\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2601_2s/C0109_full_MP4_anno_for_labelling_full_temporal_2s/features',   # NEW: {1: 2512, 3: 78, 4: 68}\n",
    "    \n",
    "#     # Previously used as testing\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0104_full_MP4_anno_for_labelling_full_temporal_2s/features',           # {}\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_2s/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_2s/features', # {0: 13, 2: 456, 5: 12, 6: 30}\n",
    "# ]\n",
    "\n",
    "# ================================\n",
    "# Features for seq_len = 1 seconds\n",
    "# ================================\n",
    "# train_features_dirs = [\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0071_full_MP4_anno_for_labelling_done_faridz_full_temporal_1s/features',\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0074_full_MP4_anno_for_labelling_done_ray_full_temporal_1s/features',   \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0078_full_MP4_anno_for_labelling_done_anne_full_temporal_1s/features',  \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0083_full_MP4_anno_for_labelling_done_putu_full_temporal_1s/features',  \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0085_full_MP4_anno_for_labelling_done_yoga_full_temporal_1s/features',  \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0087_full_MP4_anno_for_labelling_done_arga_full_temporal_1s/features',  \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0090_full_MP4_anno_for_labelling_rizky_full_temporal_1s/features',\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0098_full_MP4_anno_for_labelling_done_akbar_full_temporal_1s/features', \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0101_full_MP4_anno_for_labelling_done_faridz_full_temporal_1s/features',\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0105_full_MP4_anno_for_labelling_full_temporal_1s/features',\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0106_full_MP4_anno_for_labelling_full_temporal_1s/features',\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0108_full_MP4_anno_for_labelling_fixed_full_temporal_1s/features',      \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0110_full_MP4_anno_for_labelling_full_temporal_1s/features',\n",
    "    \n",
    "#     # Previously used as validation\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0100_full_MP4_anno_for_labelling_done_putu_full_temporal_1s/features',  \n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0109_full_MP4_anno_for_labelling_full_temporal_1s/features',            \n",
    "    \n",
    "#     # Previously used as testing\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0104_full_MP4_anno_for_labelling_full_temporal_1s/features',\n",
    "#     '/root/vs-gats-plaster/deepsort/hiergat_data_v2512_1s/C0099_full_MP4_anno_for_labelling_done_arga_full_temporal_1s/features',  \n",
    "# ]\n",
    "\n",
    "train_features_dirs = [Path(p) for p in train_features_dirs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dirs_df(features_dirs):\n",
    "    feature_dir_dicts = []\n",
    "    for dirs in features_dirs:\n",
    "        for dir in dirs.iterdir():\n",
    "            action_label_str = str(dir).rsplit('_action_')[-1]\n",
    "            is_interpolated = 0\n",
    "            if '_interp' in action_label_str:\n",
    "                action_label_str = action_label_str.replace('_interp', '')\n",
    "                is_interpolated = 1\n",
    "            action_label = int(action_label_str)\n",
    "            action_class = action_classes[action_label]\n",
    "            if 'new_action_classes' in globals():\n",
    "                try:\n",
    "                    action_label = new_action_classes.index(action_class) # type: ignore\n",
    "                except ValueError:\n",
    "                    action_label = -1\n",
    "            feature_dir_dicts.append({\n",
    "                'base_dir': str(dirs),\n",
    "                'dir': str(dir),\n",
    "                'action_label': action_label,\n",
    "                # 'new_action_label': new_action_label,\n",
    "                'is_interpolated': is_interpolated,\n",
    "            })\n",
    "    feature_dirs_df = pd.DataFrame(feature_dir_dicts)\n",
    "    feature_dirs_df = feature_dirs_df[feature_dirs_df['action_label'] != -1] # remove rows where action_label == -1\n",
    "    return feature_dirs_df\n",
    "\n",
    "train_feature_dirs_df = get_features_dirs_df(train_features_dirs)\n",
    "\n",
    "print(\"Training set action label counts:\")\n",
    "print(train_feature_dirs_df['action_label'].value_counts().sort_index().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_data_df(feature_dirs, action_classes, new_action_classes=None, \n",
    "                   downsampling: int = 1, test_data: bool = False):\n",
    "    base_dirs = []\n",
    "    dirs = []\n",
    "    start_fids = []\n",
    "    end_fids = []\n",
    "    human_features_list = []\n",
    "    human_boxes_list = []\n",
    "    human_poses_list = []\n",
    "    object_features_list = []\n",
    "    object_boxes_list = []\n",
    "    gt_list = [] if not test_data else None\n",
    "    action_labels = [] if not test_data else None\n",
    "    xs_steps = []\n",
    "\n",
    "    for feature_dir in feature_dirs:\n",
    "        feature_dir = Path(feature_dir)\n",
    "        \n",
    "        # Store directories\n",
    "        base_dirs.append(str(feature_dir.parent.parent))\n",
    "        dirs.append(str(feature_dir))\n",
    "        \n",
    "        # Store start and end FIDs\n",
    "        match = re.search(r\"range_([0-9]+(?:\\.[0-9]+)?)_([0-9]+(?:\\.[0-9]+)?)\", str(feature_dir))\n",
    "        start_fid, end_fid = map(float, match.groups())\n",
    "        start_fids.append(start_fid)\n",
    "        end_fids.append(end_fid)\n",
    "        \n",
    "        # Load and store human (subject) features\n",
    "        subject_visual_features = np.load(feature_dir / 'subject_visual_features.npy')\n",
    "        subject_boxes = np.load(feature_dir / 'subject_boxes.npy')\n",
    "        # subject_poses = np.zeros((subject_visual_features.shape[0], 17, 2))\n",
    "        subject_poses = np.load(feature_dir / 'subject_poses.npy')\n",
    "        \n",
    "        human_features_list.append([subject_visual_features])\n",
    "        human_boxes_list.append([subject_boxes])\n",
    "        human_poses_list.append([subject_poses])\n",
    "        \n",
    "        # Load and store object features\n",
    "        object_visual_features = np.load(feature_dir / 'object_visual_features.npy')\n",
    "        object_boxes = np.load(feature_dir / 'object_boxes.npy')\n",
    "        \n",
    "        object_features_list.append(object_visual_features[:, np.newaxis, :])\n",
    "        object_boxes_list.append(object_boxes[:, np.newaxis, :])\n",
    "        \n",
    "        # Extract and store ground-truth action label\n",
    "        if not test_data:\n",
    "            action_label_str = str(feature_dir).split('_action_')[-1]\n",
    "            if '_' in action_label_str:\n",
    "                action_label_str = action_label_str.split('_')[0]\n",
    "            action_label = int(action_label_str)\n",
    "            if isinstance(new_action_classes, (list, tuple)):\n",
    "                action_class = action_classes[action_label]\n",
    "                action_label = new_action_classes.index(action_class)\n",
    "            seq_len = subject_visual_features.shape[0]\n",
    "            gt_list.append({\n",
    "                'Human1': [action_label] * seq_len,\n",
    "            })\n",
    "            action_labels.append(action_label)\n",
    "        \n",
    "        # Store number of steps\n",
    "        num_steps = len(subject_visual_features[downsampling - 1::downsampling])\n",
    "        xs_steps.append(num_steps)\n",
    "\n",
    "    xs_steps = np.array(xs_steps, dtype=np.float32)\n",
    "\n",
    "    data_df = pd.DataFrame({\n",
    "        'human_features': human_features_list,\n",
    "        'human_boxes': human_boxes_list,\n",
    "        'human_poses': human_poses_list,\n",
    "        'object_features': object_features_list,\n",
    "        'object_boxes': object_boxes_list,\n",
    "        'gt': gt_list,\n",
    "        'xs_step': xs_steps,\n",
    "        'base_dir': base_dirs,\n",
    "        'dir': dirs,\n",
    "        'start_fid': start_fids,\n",
    "        'end_fid': end_fids,\n",
    "        'action_label': action_labels,\n",
    "    })\n",
    "    data_df['xs_step'] = data_df['xs_step'].astype(int)\n",
    "    data_df['start_fid'] = data_df['start_fid'].astype(int)\n",
    "    data_df['end_fid'] = data_df['end_fid'].astype(int)\n",
    "    return data_df\n",
    "\n",
    "df = create_data_df(train_feature_dirs_df['dir'].tolist(), action_classes, new_action_classes if 'new_action_classes' in globals() else None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_dir, group_df in df.groupby('base_dir'):\n",
    "    print(\"================================================================================================================================\")\n",
    "    print(base_dir)\n",
    "    print(\"================================================================================================================================\")\n",
    "    group_df['split'] = 'train'\n",
    "    group_df = group_df.sort_values('start_fid')\n",
    "    \n",
    "    min_fid = 0\n",
    "    max_fid = group_df['end_fid'].max()\n",
    "    duration = max_fid - min_fid\n",
    "    val_duration = int(duration * val_fraction)\n",
    "    \n",
    "    # Determine validation and testing FID ranges\n",
    "    assert duration//2 >= val_duration+val_duration, f\"Duration ({duration}) should be at least twice the validation duration ({val_duration})!\"\n",
    "    # val_start_fid = random.randrange(0, duration//2) # Sample a random FID from the start to the middle\n",
    "    val_start_fid   = random.randrange(duration//2, duration-val_duration-val_duration) # Sample a random FID from the middle to the end\n",
    "    val_end_fid     = val_start_fid + val_duration - 1\n",
    "    test_start_fid  = val_start_fid + val_duration\n",
    "    test_end_fid    = test_start_fid + val_duration - 1\n",
    "    \n",
    "    print(f\"Validation FID range: {val_start_fid}-{val_end_fid}, length: {val_end_fid-val_start_fid+1}\")\n",
    "    print(f\"Testing FID range: {test_start_fid}-{test_end_fid}, length: {test_end_fid-test_start_fid+1}\")\n",
    "    print()\n",
    "    \n",
    "    # Swap validation and testing FID ranges if validation is larger than testing\n",
    "    base_dir_cond = df['base_dir'] == base_dir\n",
    "    val_len = len(df.loc[base_dir_cond & (df['start_fid'] >= val_start_fid) & (df['end_fid'] <= val_end_fid)])\n",
    "    test_len = len(df.loc[base_dir_cond & (df['start_fid'] >= test_start_fid) & (df['end_fid'] <= test_end_fid)])\n",
    "    if val_len > test_len:\n",
    "        tmp_start_fid  = test_start_fid\n",
    "        tmp_end_fid    = test_end_fid\n",
    "        test_start_fid = val_start_fid\n",
    "        test_end_fid   = val_end_fid\n",
    "        val_start_fid  = tmp_start_fid\n",
    "        val_end_fid    = tmp_end_fid\n",
    "        \n",
    "    # Assign splits\n",
    "    df.loc[base_dir_cond & (df['start_fid'] >= val_start_fid) & (df['end_fid'] <= val_end_fid), 'split'] = 'val'\n",
    "    df.loc[base_dir_cond & (df['start_fid'] >= test_start_fid) & (df['end_fid'] <= test_end_fid), 'split'] = 'test'\n",
    "    df.loc[base_dir_cond & (df['split'].isna()), 'split'] = 'train'\n",
    "    \n",
    "    train_group_df = df[base_dir_cond & (df['split'] == 'train')]\n",
    "    val_group_df = df[base_dir_cond & (df['split'] == 'val')]\n",
    "    test_group_df = df[base_dir_cond & (df['split'] == 'test')]\n",
    "    \n",
    "    try:\n",
    "        assert len(test_group_df) > len(val_group_df) and len(df[base_dir_cond]) == len(train_group_df)+len(val_group_df)+len(test_group_df)\n",
    "    except:\n",
    "        print(\"len(df[base_dir_cond]):\", len(df[base_dir_cond]))\n",
    "        print(\"len(train_group_df):\", len(train_group_df))\n",
    "        print(\"len(val_group_df):\", len(val_group_df))\n",
    "        print(\"len(test_group_df):\", len(test_group_df))\n",
    "    print(\"Training action label info:\")\n",
    "    print(train_group_df['action_label'].value_counts())\n",
    "    print(\"Total:\", len(train_group_df))\n",
    "    print()\n",
    "    print(\"Validation action label info:\")\n",
    "    print(val_group_df['action_label'].value_counts())\n",
    "    print(\"Total:\", len(val_group_df))\n",
    "    print()\n",
    "    print(\"Testing action label info:\")\n",
    "    print(test_group_df['action_label'].value_counts())\n",
    "    print(\"Total:\", len(test_group_df))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "test_df = df[df['split'] == 'test']\n",
    "\n",
    "train_size = len(train_df)\n",
    "val_size = len(val_df)\n",
    "test_size = len(test_df)\n",
    "all_size = len(df)\n",
    "\n",
    "print(f\"Total train data: {train_size} ({train_size/all_size*100:.2f}%)\")\n",
    "print(f\"Total val data: {val_size} ({val_size/all_size*100:.2f}%)\")\n",
    "print(f\"Total test data: {test_size} ({test_size/all_size*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols  = ['human_features', 'human_boxes', 'human_poses', 'object_features', 'object_boxes', 'gt', 'xs_step']\n",
    "train_data = train_df[data_cols].to_numpy().T.tolist()\n",
    "val_data   = val_df[data_cols].to_numpy().T.tolist()\n",
    "test_data  = test_df[data_cols].to_numpy().T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_features_dir = '/root/vs-gats-plaster/deepsort/hiergat_data_v3_3s/C0074_full_MP4_anno_for_labelling_done_ray_full_temporal_3s'\n",
    "\n",
    "test_video_w_seen_df = df[df['base_dir'] == test_video_features_dir]\n",
    "test_video_unseen_df = test_df[test_df['base_dir'] == test_video_features_dir]\n",
    "\n",
    "test_video_w_seen_data = test_video_w_seen_df[data_cols].to_numpy().T.tolist()\n",
    "test_video_unseen_data = test_video_unseen_df[data_cols].to_numpy().T.tolist()\n",
    "\n",
    "print(\"Test video action label counts (with seen data):\")\n",
    "print(test_video_w_seen_df['action_label'].value_counts().sort_index())\n",
    "print()\n",
    "print(\"Test video action label counts (only unseen data):\")\n",
    "print(test_video_unseen_df['action_label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, scalers, _ = create_data_loader(\n",
    "    *train_data, \n",
    "    model_name, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    scaling_strategy=scaling_strategy, \n",
    "    sigma=sigma,\n",
    "    downsampling=downsampling,\n",
    ")\n",
    "val_loader, _, _ = create_data_loader(\n",
    "    *val_data, \n",
    "    model_name, \n",
    "    batch_size=len(val_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "test_loader, _, _ = create_data_loader(\n",
    "    *test_data, \n",
    "    model_name, \n",
    "    batch_size=len(test_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "test_video_w_seen_loader, _, _ = create_data_loader(\n",
    "    *test_video_w_seen_data, \n",
    "    model_name, \n",
    "    batch_size=len(test_video_w_seen_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "test_video_unseen_loader, _, _ = create_data_loader(\n",
    "    *test_video_unseen_data, \n",
    "    model_name, \n",
    "    batch_size=len(test_video_unseen_data[0]),\n",
    "    shuffle=False, \n",
    "    scalers=scalers, \n",
    "    sigma=sigma, \n",
    "    downsampling=downsampling,\n",
    ")\n",
    "input_size = input_size_from_data_loader(train_loader, model_name, model_input_type)\n",
    "data_info = {'input_size': input_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_creation_args = cfg.models.parameters\n",
    "model_creation_args = {**data_info, **model_creation_args.__dict__}\n",
    "dataset_name = cfg.data.name\n",
    "model_creation_args['num_classes'] = (num_classes, None)\n",
    "device = 'cuda' if torch.cuda.is_available() and cfg.resources.use_gpu else 'cpu'\n",
    "model = TGGCN(feat_dim=1024, **model_creation_args).to(device)\n",
    "if misc_dict.get('pretrained', False) and misc_dict.get('pretrained_path') is not None:\n",
    "    state_dict = load_model_weights(misc_dict['pretrained_path'])\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=cfg.models.optimization.learning_rate)\n",
    "criterion, loss_names = select_loss(model_name, model_input_type, dataset_name, cfg=cfg)\n",
    "mtll_model = None\n",
    "if misc_dict.get('multi_task_loss_learner', False):\n",
    "    loss_types = select_loss_types(model_name, dataset_name, cfg=cfg)\n",
    "    mask = select_loss_learning_mask(model_name, dataset_name, cfg=cfg)\n",
    "    mtll_model = MultiTaskLossLearner(loss_types=loss_types, mask=mask).to(device)\n",
    "    optimizer.add_param_group({'params': mtll_model.parameters()})\n",
    "# Some config + model training\n",
    "tensorboard_log_dir = cfg.models.logging.root_log_dir\n",
    "checkpoint_name = cfg.models.logging.checkpoint_name\n",
    "fetch_model_data = select_model_data_fetcher(model_name, model_input_type,\n",
    "                                             dataset_name=dataset_name, **{**misc_dict, **cfg.models.parameters.__dict__})\n",
    "feed_model_data = select_model_data_feeder(model_name, model_input_type, dataset_name=dataset_name, **misc_dict)\n",
    "num_main_losses = decide_num_main_losses(model_name, dataset_name, {**misc_dict, **cfg.models.parameters.__dict__})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = train(\n",
    "#     model, \n",
    "#     train_loader, \n",
    "#     optimizer, \n",
    "#     criterion, \n",
    "#     cfg.models.optimization.epochs, \n",
    "#     device, \n",
    "#     loss_names,\n",
    "#     clip_gradient_at=cfg.models.optimization.clip_gradient_at,\n",
    "#     fetch_model_data=fetch_model_data, feed_model_data=feed_model_data,\n",
    "#     val_loader=val_loader, \n",
    "#     mtll_model=mtll_model, \n",
    "#     num_main_losses=num_main_losses,\n",
    "#     tensorboard_log_dir=tensorboard_log_dir, \n",
    "#     checkpoint_name=checkpoint_name,\n",
    "# )\n",
    "# # Logging\n",
    "# if cfg.models.logging.log_dir is not None:\n",
    "#     log_dir = cfg.models.logging.log_dir\n",
    "#     checkpoint['scalers'] = scalers\n",
    "#     save_checkpoint(log_dir, checkpoint, checkpoint_name=checkpoint_name, include_timestamp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiergat_checkpoint_path = Path('/root/workspace/HierGAT-Custom/outputs_hiergat/custom/hs512_e10_bs32_lr0.0001_0.5_20260119133043/hs512_e10_bs32_lr0.0001_0.5_20260119133043.tar')\n",
    "\n",
    "hiergat_checkpoint = torch.load(hiergat_checkpoint_path, map_location=device)\n",
    "hiergat_scalers = hiergat_checkpoint.get('scalers', None)\n",
    "model_args = {\n",
    "    # Original\n",
    "    'add_segment_length': 0,\n",
    "    'add_time_position': 0,\n",
    "    'attention_style': 'v3',\n",
    "    'bias': True,\n",
    "    'cat_level_states': 0,\n",
    "    'discrete_networks_num_layers': 1,\n",
    "    'discrete_optimization_strategy': 'gs',\n",
    "    'filter_discrete_updates': False,\n",
    "    'gcn_node': 25,\n",
    "    'hidden_size': 512,\n",
    "    'input_size': (1124, 1024),\n",
    "    'message_aggregation': 'att',\n",
    "    'message_geometry_to_human': True,\n",
    "    'message_geometry_to_objects': True,\n",
    "    'message_granularity': 'v1',\n",
    "    'message_human_to_objects': True,\n",
    "    'message_humans_to_human': False,\n",
    "    'message_objects_to_human': True,\n",
    "    'message_objects_to_object': False,\n",
    "    'message_segment': True,\n",
    "    'message_type': 'v2',\n",
    "    'num_classes': (num_classes, None),\n",
    "    'object_segment_update_strategy': 'ind',\n",
    "    'positional_encoding_style': 'e',\n",
    "    'share_level_mlps': 0,\n",
    "    'time_position_strategy': 's',\n",
    "    'update_segment_threshold': 0.5,\n",
    "    \n",
    "    # Custom\n",
    "    'feat_dim': 1024,\n",
    "}\n",
    "model = TGGCN(**model_args).to(device)\n",
    "model.load_state_dict(hiergat_checkpoint['model_state_dict'], strict=False)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from predict import match_shape, match_att_shape\n",
    "\n",
    "inspect_model = False\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Videos (Only Unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets, attentions = [], [], []\n",
    "for i, dataset in enumerate(test_loader):\n",
    "    data, target = fetch_model_data(dataset, device=device)\n",
    "    with torch.no_grad():\n",
    "        output = feed_model_data(model, data)\n",
    "    if inspect_model:\n",
    "        output, attention_scores = output\n",
    "        attention_scores = [att_score[:, 0] for att_score in attention_scores]\n",
    "    if num_main_losses is not None:\n",
    "        output = output[-num_main_losses:]\n",
    "        target = target[-num_main_losses:]\n",
    "    if downsampling > 1:\n",
    "        for i, (out, tgt) in enumerate(zip(output, target)):\n",
    "            if out.ndim != 4:\n",
    "                raise RuntimeError(f'Number of dimensions for output is {out.ndim}')\n",
    "            out = torch.repeat_interleave(out, repeats=downsampling, dim=-2)\n",
    "            out = match_shape(out, tgt)\n",
    "            output[i] = out\n",
    "        if inspect_model:\n",
    "            a_target = target[0]\n",
    "            attention_scores = [torch.repeat_interleave(att_score, repeats=downsampling, dim=-2)\n",
    "                                for att_score in attention_scores]\n",
    "            attention_scores = [match_att_shape(att_score, a_target) for att_score in attention_scores]\n",
    "            attentions.append(attention_scores)\n",
    "    outputs += output\n",
    "    targets += target\n",
    "\n",
    "y_pred = torch.argmax(outputs[0], dim=1).cpu().numpy()\n",
    "y_true = targets[0].squeeze(-1).mode(dim=1).values.cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_new_action_classes = [\n",
    "    # rebar\n",
    "    'tie',              # 0\n",
    "    'erect',            # 1\n",
    "    'prepare',          # 2\n",
    "    'transport',        # 3\n",
    "\n",
    "    # equipment         \n",
    "    'use',              # 4\n",
    "    'carry',            # 5\n",
    "\n",
    "    # all\n",
    "    'inspect',          # 6\n",
    "    'no interaction',   # 7\n",
    "]\n",
    "\n",
    "# ticklabels = action_classes\n",
    "ticklabels = new_action_classes\n",
    "if 'new_action_classes' in globals() and isinstance(new_action_classes, (list, tuple)): # type: ignore\n",
    "    y_true = [ticklabels.index(new_action_classes[y]) for y in y_true] # type: ignore\n",
    "    y_pred = [ticklabels.index(new_action_classes[y]) for y in y_pred] # type: ignore\n",
    "    \n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(ticklabels))))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=vis_new_action_classes, yticklabels=vis_new_action_classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Video (Seen + Unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected test video ID:\", Path(test_video_features_dir).stem[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets, attentions = [], [], []\n",
    "for i, dataset in enumerate(test_video_w_seen_loader):\n",
    "    data, target = fetch_model_data(dataset, device=device)\n",
    "    with torch.no_grad():\n",
    "        output = feed_model_data(model, data)\n",
    "    if inspect_model:\n",
    "        output, attention_scores = output\n",
    "        attention_scores = [att_score[:, 0] for att_score in attention_scores]\n",
    "    if num_main_losses is not None:\n",
    "        output = output[-num_main_losses:]\n",
    "        target = target[-num_main_losses:]\n",
    "    if downsampling > 1:\n",
    "        for i, (out, tgt) in enumerate(zip(output, target)):\n",
    "            if out.ndim != 4:\n",
    "                raise RuntimeError(f'Number of dimensions for output is {out.ndim}')\n",
    "            out = torch.repeat_interleave(out, repeats=downsampling, dim=-2)\n",
    "            out = match_shape(out, tgt)\n",
    "            output[i] = out\n",
    "        if inspect_model:\n",
    "            a_target = target[0]\n",
    "            attention_scores = [torch.repeat_interleave(att_score, repeats=downsampling, dim=-2)\n",
    "                                for att_score in attention_scores]\n",
    "            attention_scores = [match_att_shape(att_score, a_target) for att_score in attention_scores]\n",
    "            attentions.append(attention_scores)\n",
    "    outputs += output\n",
    "    targets += target\n",
    "    \n",
    "y_pred = torch.argmax(outputs[0], dim=1).cpu().numpy()\n",
    "y_true = targets[0].squeeze(-1).mode(dim=1).values.cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_new_action_classes = [\n",
    "    # rebar\n",
    "    'tie',              # 0\n",
    "    'erect',            # 1\n",
    "    'prepare',          # 2\n",
    "    'transport',        # 3\n",
    "\n",
    "    # equipment         \n",
    "    'use',              # 4\n",
    "    'carry',            # 5\n",
    "\n",
    "    # all\n",
    "    'inspect',          # 6\n",
    "    'no interaction',   # 7\n",
    "]\n",
    "\n",
    "# ticklabels = action_classes\n",
    "ticklabels = new_action_classes\n",
    "if 'new_action_classes' in globals() and isinstance(new_action_classes, (list, tuple)): # type: ignore\n",
    "    y_true = [ticklabels.index(new_action_classes[y]) for y in y_true] # type: ignore\n",
    "    y_pred = [ticklabels.index(new_action_classes[y]) for y in y_pred] # type: ignore\n",
    "    \n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(ticklabels))))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=vis_new_action_classes, yticklabels=vis_new_action_classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Video (Only Unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected test video ID:\", Path(test_video_features_dir).stem[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets, attentions = [], [], []\n",
    "for i, dataset in enumerate(test_video_unseen_loader):\n",
    "    data, target = fetch_model_data(dataset, device=device)\n",
    "    with torch.no_grad():\n",
    "        output = feed_model_data(model, data)\n",
    "    if inspect_model:\n",
    "        output, attention_scores = output\n",
    "        attention_scores = [att_score[:, 0] for att_score in attention_scores]\n",
    "    if num_main_losses is not None:\n",
    "        output = output[-num_main_losses:]\n",
    "        target = target[-num_main_losses:]\n",
    "    if downsampling > 1:\n",
    "        for i, (out, tgt) in enumerate(zip(output, target)):\n",
    "            if out.ndim != 4:\n",
    "                raise RuntimeError(f'Number of dimensions for output is {out.ndim}')\n",
    "            out = torch.repeat_interleave(out, repeats=downsampling, dim=-2)\n",
    "            out = match_shape(out, tgt)\n",
    "            output[i] = out\n",
    "        if inspect_model:\n",
    "            a_target = target[0]\n",
    "            attention_scores = [torch.repeat_interleave(att_score, repeats=downsampling, dim=-2)\n",
    "                                for att_score in attention_scores]\n",
    "            attention_scores = [match_att_shape(att_score, a_target) for att_score in attention_scores]\n",
    "            attentions.append(attention_scores)\n",
    "    outputs += output\n",
    "    targets += target\n",
    "    \n",
    "y_pred = torch.argmax(outputs[0], dim=1).cpu().numpy()\n",
    "y_true = targets[0].squeeze(-1).mode(dim=1).values.cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_new_action_classes = [\n",
    "    # rebar\n",
    "    'tie',              # 0\n",
    "    'erect',            # 1\n",
    "    'prepare',          # 2\n",
    "    'transport',        # 3\n",
    "\n",
    "    # equipment         \n",
    "    'use',              # 4\n",
    "    'carry',            # 5\n",
    "\n",
    "    # all\n",
    "    'inspect',          # 6\n",
    "    'no interaction',   # 7\n",
    "]\n",
    "\n",
    "# ticklabels = action_classes\n",
    "ticklabels = new_action_classes\n",
    "if 'new_action_classes' in globals() and isinstance(new_action_classes, (list, tuple)): # type: ignore\n",
    "    y_true = [ticklabels.index(new_action_classes[y]) for y in y_true] # type: ignore\n",
    "    y_pred = [ticklabels.index(new_action_classes[y]) for y in y_pred] # type: ignore\n",
    "    \n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(ticklabels))))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=vis_new_action_classes, yticklabels=vis_new_action_classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_torch271",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
